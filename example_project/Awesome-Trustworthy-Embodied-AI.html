<!DOCTYPE html>
<!-- saved from url=(0058)https://ai45lab.github.io/Awesome-Trustworthy-Embodied-AI/ -->
<html lang=""><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><link rel="stylesheet" href="./Awesome-Trustworthy-Embodied-AI_files/academicons.min.css"><script>MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        },
        svg: {
          fontCache: 'global'
        }
      };</script><script id="MathJax-script" async="" src="./Awesome-Trustworthy-Embodied-AI_files/tex-svg.js.下载"></script><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><link rel="icon" href="https://ai45lab.github.io/Awesome-Trustworthy-Embodied-AI/favicon.ico"><title>Awesome-Trustworthy-Embodied-AI</title><script defer="defer" src="./Awesome-Trustworthy-Embodied-AI_files/chunk-vendors.ff084c97.js.下载"></script><script defer="defer" src="./Awesome-Trustworthy-Embodied-AI_files/app.de47c330.js.下载"></script><link href="./Awesome-Trustworthy-Embodied-AI_files/chunk-vendors.d49ddc65.css" rel="stylesheet"><link href="./Awesome-Trustworthy-Embodied-AI_files/app.da2a2e99.css" rel="stylesheet"><style type="text/css">.CtxtMenu_InfoClose {  top:.2em; right:.2em;}
.CtxtMenu_InfoContent {  overflow:auto; text-align:left; font-size:80%;  padding:.4em .6em; border:1px inset; margin:1em 0px;  max-height:20em; max-width:30em; background-color:#EEEEEE;  white-space:normal;}
.CtxtMenu_Info.CtxtMenu_MousePost {outline:none;}
.CtxtMenu_Info {  position:fixed; left:50%; width:auto; text-align:center;  border:3px outset; padding:1em 2em; background-color:#DDDDDD;  color:black;  cursor:default; font-family:message-box; font-size:120%;  font-style:normal; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 15px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius:15px;               /* Safari and Chrome */  -moz-border-radius:15px;                  /* Firefox */  -khtml-border-radius:15px;                /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */  filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /* IE */}
</style><style type="text/css">.CtxtMenu_MenuClose {  position:absolute;  cursor:pointer;  display:inline-block;  border:2px solid #AAA;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  font-family: "Courier New", Courier;  font-size:24px;  color:#F0F0F0}
.CtxtMenu_MenuClose span {  display:block; background-color:#AAA; border:1.5px solid;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  line-height:0;  padding:8px 0 6px     /* may need to be browser-specific */}
.CtxtMenu_MenuClose:hover {  color:white!important;  border:2px solid #CCC!important}
.CtxtMenu_MenuClose:hover span {  background-color:#CCC!important}
.CtxtMenu_MenuClose:hover:focus {  outline:none}
</style><style type="text/css">.CtxtMenu_Menu {  position:absolute;  background-color:white;  color:black;  width:auto; padding:5px 0px;  border:1px solid #CCCCCC; margin:0; cursor:default;  font: menu; text-align:left; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 5px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius: 5px;             /* Safari and Chrome */  -moz-border-radius: 5px;                /* Firefox */  -khtml-border-radius: 5px;              /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */}
.CtxtMenu_MenuItem {  padding: 1px 2em;  background:transparent;}
.CtxtMenu_MenuArrow {  position:absolute; right:.5em; padding-top:.25em; color:#666666;  font-family: null; font-size: .75em}
.CtxtMenu_MenuActive .CtxtMenu_MenuArrow {color:white}
.CtxtMenu_MenuArrow.CtxtMenu_RTL {left:.5em; right:auto}
.CtxtMenu_MenuCheck {  position:absolute; left:.7em;  font-family: null}
.CtxtMenu_MenuCheck.CtxtMenu_RTL { right:.7em; left:auto }
.CtxtMenu_MenuRadioCheck {  position:absolute; left: .7em;}
.CtxtMenu_MenuRadioCheck.CtxtMenu_RTL {  right: .7em; left:auto}
.CtxtMenu_MenuInputBox {  padding-left: 1em; right:.5em; color:#666666;  font-family: null;}
.CtxtMenu_MenuInputBox.CtxtMenu_RTL {  left: .1em;}
.CtxtMenu_MenuComboBox {  left:.1em; padding-bottom:.5em;}
.CtxtMenu_MenuSlider {  left: .1em;}
.CtxtMenu_SliderValue {  position:absolute; right:.1em; padding-top:.25em; color:#333333;  font-size: .75em}
.CtxtMenu_SliderBar {  outline: none; background: #d3d3d3}
.CtxtMenu_MenuLabel {  padding: 1px 2em 3px 1.33em;  font-style:italic}
.CtxtMenu_MenuRule {  border-top: 1px solid #DDDDDD;  margin: 4px 3px;}
.CtxtMenu_MenuDisabled {  color:GrayText}
.CtxtMenu_MenuActive {  background-color: #606872;  color: white;}
.CtxtMenu_MenuDisabled:focus {  background-color: #E8E8E8}
.CtxtMenu_MenuLabel:focus {  background-color: #E8E8E8}
.CtxtMenu_ContextMenu:focus {  outline:none}
.CtxtMenu_ContextMenu .CtxtMenu_MenuItem:focus {  outline:none}
.CtxtMenu_SelectionMenu {  position:relative; float:left;  border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; }
.CtxtMenu_SelectionItem {  padding-right: 1em;}
.CtxtMenu_Selection {  right: 40%; width:50%; }
.CtxtMenu_SelectionBox {  padding: 0em; max-height:20em; max-width: none;  background-color:#FFFFFF;}
.CtxtMenu_SelectionDivider {  clear: both; border-top: 2px solid #000000;}
.CtxtMenu_Menu .CtxtMenu_MenuClose {  top:-10px; left:-10px}
</style><style id="MJX-SVG-styles">
mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
  min-height: 1px;
  min-width: 1px;
}

mjx-container[jax="SVG"] > svg a {
  fill: blue;
  stroke: blue;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line], svg[data-table] > g > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame], svg[data-table] > g > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed, svg[data-table] > g > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted, svg[data-table] > g > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > g > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

mjx-container[jax="SVG"] path[data-c], mjx-container[jax="SVG"] use[data-c] {
  stroke-width: 3;
}
</style><style id="vuetify-theme-stylesheet" type="text/css">:root {
      --v-theme-background: 255,255,255;
      --v-theme-background-overlay-multiplier: 1;
      --v-theme-surface: 255,255,255;
      --v-theme-surface-overlay-multiplier: 1;
      --v-theme-surface-bright: 255,255,255;
      --v-theme-surface-bright-overlay-multiplier: 1;
      --v-theme-surface-light: 238,238,238;
      --v-theme-surface-light-overlay-multiplier: 1;
      --v-theme-surface-variant: 66,66,66;
      --v-theme-surface-variant-overlay-multiplier: 2;
      --v-theme-on-surface-variant: 238,238,238;
      --v-theme-primary: 24,103,192;
      --v-theme-primary-overlay-multiplier: 2;
      --v-theme-primary-darken-1: 31,85,146;
      --v-theme-primary-darken-1-overlay-multiplier: 2;
      --v-theme-secondary: 72,169,166;
      --v-theme-secondary-overlay-multiplier: 1;
      --v-theme-secondary-darken-1: 1,135,134;
      --v-theme-secondary-darken-1-overlay-multiplier: 1;
      --v-theme-error: 176,0,32;
      --v-theme-error-overlay-multiplier: 2;
      --v-theme-info: 33,150,243;
      --v-theme-info-overlay-multiplier: 1;
      --v-theme-success: 76,175,80;
      --v-theme-success-overlay-multiplier: 1;
      --v-theme-warning: 251,140,0;
      --v-theme-warning-overlay-multiplier: 1;
      --v-theme-on-background: 0,0,0;
      --v-theme-on-surface: 0,0,0;
      --v-theme-on-surface-bright: 0,0,0;
      --v-theme-on-surface-light: 0,0,0;
      --v-theme-on-primary: 255,255,255;
      --v-theme-on-primary-darken-1: 255,255,255;
      --v-theme-on-secondary: 255,255,255;
      --v-theme-on-secondary-darken-1: 255,255,255;
      --v-theme-on-error: 255,255,255;
      --v-theme-on-info: 255,255,255;
      --v-theme-on-success: 255,255,255;
      --v-theme-on-warning: 255,255,255;
      --v-border-color: 0, 0, 0;
      --v-border-opacity: 0.12;
      --v-high-emphasis-opacity: 0.87;
      --v-medium-emphasis-opacity: 0.6;
      --v-disabled-opacity: 0.38;
      --v-idle-opacity: 0.04;
      --v-hover-opacity: 0.04;
      --v-focus-opacity: 0.12;
      --v-selected-opacity: 0.08;
      --v-activated-opacity: 0.12;
      --v-pressed-opacity: 0.12;
      --v-dragged-opacity: 0.08;
      --v-theme-kbd: 238, 238, 238;
      --v-theme-on-kbd: 0, 0, 0;
      --v-theme-code: 245, 245, 245;
      --v-theme-on-code: 0, 0, 0;
    }
    .v-theme--light {
      color-scheme: normal;
      --v-theme-background: 255,255,255;
      --v-theme-background-overlay-multiplier: 1;
      --v-theme-surface: 255,255,255;
      --v-theme-surface-overlay-multiplier: 1;
      --v-theme-surface-bright: 255,255,255;
      --v-theme-surface-bright-overlay-multiplier: 1;
      --v-theme-surface-light: 238,238,238;
      --v-theme-surface-light-overlay-multiplier: 1;
      --v-theme-surface-variant: 66,66,66;
      --v-theme-surface-variant-overlay-multiplier: 2;
      --v-theme-on-surface-variant: 238,238,238;
      --v-theme-primary: 24,103,192;
      --v-theme-primary-overlay-multiplier: 2;
      --v-theme-primary-darken-1: 31,85,146;
      --v-theme-primary-darken-1-overlay-multiplier: 2;
      --v-theme-secondary: 72,169,166;
      --v-theme-secondary-overlay-multiplier: 1;
      --v-theme-secondary-darken-1: 1,135,134;
      --v-theme-secondary-darken-1-overlay-multiplier: 1;
      --v-theme-error: 176,0,32;
      --v-theme-error-overlay-multiplier: 2;
      --v-theme-info: 33,150,243;
      --v-theme-info-overlay-multiplier: 1;
      --v-theme-success: 76,175,80;
      --v-theme-success-overlay-multiplier: 1;
      --v-theme-warning: 251,140,0;
      --v-theme-warning-overlay-multiplier: 1;
      --v-theme-on-background: 0,0,0;
      --v-theme-on-surface: 0,0,0;
      --v-theme-on-surface-bright: 0,0,0;
      --v-theme-on-surface-light: 0,0,0;
      --v-theme-on-primary: 255,255,255;
      --v-theme-on-primary-darken-1: 255,255,255;
      --v-theme-on-secondary: 255,255,255;
      --v-theme-on-secondary-darken-1: 255,255,255;
      --v-theme-on-error: 255,255,255;
      --v-theme-on-info: 255,255,255;
      --v-theme-on-success: 255,255,255;
      --v-theme-on-warning: 255,255,255;
      --v-border-color: 0, 0, 0;
      --v-border-opacity: 0.12;
      --v-high-emphasis-opacity: 0.87;
      --v-medium-emphasis-opacity: 0.6;
      --v-disabled-opacity: 0.38;
      --v-idle-opacity: 0.04;
      --v-hover-opacity: 0.04;
      --v-focus-opacity: 0.12;
      --v-selected-opacity: 0.08;
      --v-activated-opacity: 0.12;
      --v-pressed-opacity: 0.12;
      --v-dragged-opacity: 0.08;
      --v-theme-kbd: 238, 238, 238;
      --v-theme-on-kbd: 0, 0, 0;
      --v-theme-code: 245, 245, 245;
      --v-theme-on-code: 0, 0, 0;
    }
    .v-theme--dark {
      color-scheme: dark;
      --v-theme-background: 18,18,18;
      --v-theme-background-overlay-multiplier: 1;
      --v-theme-surface: 33,33,33;
      --v-theme-surface-overlay-multiplier: 1;
      --v-theme-surface-bright: 204,191,214;
      --v-theme-surface-bright-overlay-multiplier: 2;
      --v-theme-surface-light: 66,66,66;
      --v-theme-surface-light-overlay-multiplier: 1;
      --v-theme-surface-variant: 200,200,200;
      --v-theme-surface-variant-overlay-multiplier: 2;
      --v-theme-on-surface-variant: 0,0,0;
      --v-theme-primary: 33,150,243;
      --v-theme-primary-overlay-multiplier: 2;
      --v-theme-primary-darken-1: 39,124,193;
      --v-theme-primary-darken-1-overlay-multiplier: 2;
      --v-theme-secondary: 84,182,178;
      --v-theme-secondary-overlay-multiplier: 2;
      --v-theme-secondary-darken-1: 72,169,166;
      --v-theme-secondary-darken-1-overlay-multiplier: 2;
      --v-theme-error: 207,102,121;
      --v-theme-error-overlay-multiplier: 2;
      --v-theme-info: 33,150,243;
      --v-theme-info-overlay-multiplier: 2;
      --v-theme-success: 76,175,80;
      --v-theme-success-overlay-multiplier: 2;
      --v-theme-warning: 251,140,0;
      --v-theme-warning-overlay-multiplier: 2;
      --v-theme-on-background: 255,255,255;
      --v-theme-on-surface: 255,255,255;
      --v-theme-on-surface-bright: 0,0,0;
      --v-theme-on-surface-light: 255,255,255;
      --v-theme-on-primary: 255,255,255;
      --v-theme-on-primary-darken-1: 255,255,255;
      --v-theme-on-secondary: 255,255,255;
      --v-theme-on-secondary-darken-1: 255,255,255;
      --v-theme-on-error: 255,255,255;
      --v-theme-on-info: 255,255,255;
      --v-theme-on-success: 255,255,255;
      --v-theme-on-warning: 255,255,255;
      --v-border-color: 255, 255, 255;
      --v-border-opacity: 0.12;
      --v-high-emphasis-opacity: 1;
      --v-medium-emphasis-opacity: 0.7;
      --v-disabled-opacity: 0.5;
      --v-idle-opacity: 0.1;
      --v-hover-opacity: 0.04;
      --v-focus-opacity: 0.12;
      --v-selected-opacity: 0.08;
      --v-activated-opacity: 0.12;
      --v-pressed-opacity: 0.16;
      --v-dragged-opacity: 0.08;
      --v-theme-kbd: 66, 66, 66;
      --v-theme-on-kbd: 255, 255, 255;
      --v-theme-code: 52, 52, 52;
      --v-theme-on-code: 204, 204, 204;
    }
    .bg-background {
      --v-theme-overlay-multiplier: var(--v-theme-background-overlay-multiplier);
      background-color: rgb(var(--v-theme-background)) !important;
      color: rgb(var(--v-theme-on-background)) !important;
    }
    .bg-surface {
      --v-theme-overlay-multiplier: var(--v-theme-surface-overlay-multiplier);
      background-color: rgb(var(--v-theme-surface)) !important;
      color: rgb(var(--v-theme-on-surface)) !important;
    }
    .bg-surface-bright {
      --v-theme-overlay-multiplier: var(--v-theme-surface-bright-overlay-multiplier);
      background-color: rgb(var(--v-theme-surface-bright)) !important;
      color: rgb(var(--v-theme-on-surface-bright)) !important;
    }
    .bg-surface-light {
      --v-theme-overlay-multiplier: var(--v-theme-surface-light-overlay-multiplier);
      background-color: rgb(var(--v-theme-surface-light)) !important;
      color: rgb(var(--v-theme-on-surface-light)) !important;
    }
    .bg-surface-variant {
      --v-theme-overlay-multiplier: var(--v-theme-surface-variant-overlay-multiplier);
      background-color: rgb(var(--v-theme-surface-variant)) !important;
      color: rgb(var(--v-theme-on-surface-variant)) !important;
    }
    .bg-primary {
      --v-theme-overlay-multiplier: var(--v-theme-primary-overlay-multiplier);
      background-color: rgb(var(--v-theme-primary)) !important;
      color: rgb(var(--v-theme-on-primary)) !important;
    }
    .bg-primary-darken-1 {
      --v-theme-overlay-multiplier: var(--v-theme-primary-darken-1-overlay-multiplier);
      background-color: rgb(var(--v-theme-primary-darken-1)) !important;
      color: rgb(var(--v-theme-on-primary-darken-1)) !important;
    }
    .bg-secondary {
      --v-theme-overlay-multiplier: var(--v-theme-secondary-overlay-multiplier);
      background-color: rgb(var(--v-theme-secondary)) !important;
      color: rgb(var(--v-theme-on-secondary)) !important;
    }
    .bg-secondary-darken-1 {
      --v-theme-overlay-multiplier: var(--v-theme-secondary-darken-1-overlay-multiplier);
      background-color: rgb(var(--v-theme-secondary-darken-1)) !important;
      color: rgb(var(--v-theme-on-secondary-darken-1)) !important;
    }
    .bg-error {
      --v-theme-overlay-multiplier: var(--v-theme-error-overlay-multiplier);
      background-color: rgb(var(--v-theme-error)) !important;
      color: rgb(var(--v-theme-on-error)) !important;
    }
    .bg-info {
      --v-theme-overlay-multiplier: var(--v-theme-info-overlay-multiplier);
      background-color: rgb(var(--v-theme-info)) !important;
      color: rgb(var(--v-theme-on-info)) !important;
    }
    .bg-success {
      --v-theme-overlay-multiplier: var(--v-theme-success-overlay-multiplier);
      background-color: rgb(var(--v-theme-success)) !important;
      color: rgb(var(--v-theme-on-success)) !important;
    }
    .bg-warning {
      --v-theme-overlay-multiplier: var(--v-theme-warning-overlay-multiplier);
      background-color: rgb(var(--v-theme-warning)) !important;
      color: rgb(var(--v-theme-on-warning)) !important;
    }
    .text-background {
      color: rgb(var(--v-theme-background)) !important;
    }
    .border-background {
      --v-border-color: var(--v-theme-background);
    }
    .text-surface {
      color: rgb(var(--v-theme-surface)) !important;
    }
    .border-surface {
      --v-border-color: var(--v-theme-surface);
    }
    .text-surface-bright {
      color: rgb(var(--v-theme-surface-bright)) !important;
    }
    .border-surface-bright {
      --v-border-color: var(--v-theme-surface-bright);
    }
    .text-surface-light {
      color: rgb(var(--v-theme-surface-light)) !important;
    }
    .border-surface-light {
      --v-border-color: var(--v-theme-surface-light);
    }
    .text-surface-variant {
      color: rgb(var(--v-theme-surface-variant)) !important;
    }
    .border-surface-variant {
      --v-border-color: var(--v-theme-surface-variant);
    }
    .on-surface-variant {
      color: rgb(var(--v-theme-on-surface-variant)) !important;
    }
    .text-primary {
      color: rgb(var(--v-theme-primary)) !important;
    }
    .border-primary {
      --v-border-color: var(--v-theme-primary);
    }
    .text-primary-darken-1 {
      color: rgb(var(--v-theme-primary-darken-1)) !important;
    }
    .border-primary-darken-1 {
      --v-border-color: var(--v-theme-primary-darken-1);
    }
    .text-secondary {
      color: rgb(var(--v-theme-secondary)) !important;
    }
    .border-secondary {
      --v-border-color: var(--v-theme-secondary);
    }
    .text-secondary-darken-1 {
      color: rgb(var(--v-theme-secondary-darken-1)) !important;
    }
    .border-secondary-darken-1 {
      --v-border-color: var(--v-theme-secondary-darken-1);
    }
    .text-error {
      color: rgb(var(--v-theme-error)) !important;
    }
    .border-error {
      --v-border-color: var(--v-theme-error);
    }
    .text-info {
      color: rgb(var(--v-theme-info)) !important;
    }
    .border-info {
      --v-border-color: var(--v-theme-info);
    }
    .text-success {
      color: rgb(var(--v-theme-success)) !important;
    }
    .border-success {
      --v-border-color: var(--v-theme-success);
    }
    .text-warning {
      color: rgb(var(--v-theme-warning)) !important;
    }
    .border-warning {
      --v-border-color: var(--v-theme-warning);
    }
    .on-background {
      color: rgb(var(--v-theme-on-background)) !important;
    }
    .on-surface {
      color: rgb(var(--v-theme-on-surface)) !important;
    }
    .on-surface-bright {
      color: rgb(var(--v-theme-on-surface-bright)) !important;
    }
    .on-surface-light {
      color: rgb(var(--v-theme-on-surface-light)) !important;
    }
    .on-primary {
      color: rgb(var(--v-theme-on-primary)) !important;
    }
    .on-primary-darken-1 {
      color: rgb(var(--v-theme-on-primary-darken-1)) !important;
    }
    .on-secondary {
      color: rgb(var(--v-theme-on-secondary)) !important;
    }
    .on-secondary-darken-1 {
      color: rgb(var(--v-theme-on-secondary-darken-1)) !important;
    }
    .on-error {
      color: rgb(var(--v-theme-on-error)) !important;
    }
    .on-info {
      color: rgb(var(--v-theme-on-info)) !important;
    }
    .on-success {
      color: rgb(var(--v-theme-on-success)) !important;
    }
    .on-warning {
      color: rgb(var(--v-theme-on-warning)) !important;
    }
</style><style data-id="immersive-translate-input-injected-css">.immersive-translate-input {
  position: absolute;
  top: 0;
  right: 0;
  left: 0;
  bottom: 0;
  z-index: 2147483647;
  display: flex;
  justify-content: center;
  align-items: center;
}
.immersive-translate-attach-loading::after {
  content: " ";

  --loading-color: #f78fb6;
  width: 6px;
  height: 6px;
  border-radius: 50%;
  display: block;
  margin: 12px auto;
  position: relative;
  color: white;
  left: -100px;
  box-sizing: border-box;
  animation: immersiveTranslateShadowRolling 1.5s linear infinite;

  position: absolute;
  top: 50%;
  left: 50%;
  transform: translate(-2000%, -50%);
  z-index: 100;
}

.immersive-translate-loading-spinner {
  vertical-align: middle !important;
  width: 10px !important;
  height: 10px !important;
  display: inline-block !important;
  margin: 0 4px !important;
  border: 2px rgba(221, 244, 255, 0.6) solid !important;
  border-top: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-left: 2px rgba(0, 0, 0, 0.375) solid !important;
  border-radius: 50% !important;
  padding: 0 !important;
  -webkit-animation: immersive-translate-loading-animation 0.6s infinite linear !important;
  animation: immersive-translate-loading-animation 0.6s infinite linear !important;
}

@-webkit-keyframes immersive-translate-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes immersive-translate-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}

.immersive-translate-input-loading {
  --loading-color: #f78fb6;
  width: 6px;
  height: 6px;
  border-radius: 50%;
  display: block;
  margin: 12px auto;
  position: relative;
  color: white;
  left: -100px;
  box-sizing: border-box;
  animation: immersiveTranslateShadowRolling 1.5s linear infinite;
}

@keyframes immersiveTranslateShadowRolling {
  0% {
    box-shadow: 0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  12% {
    box-shadow: 100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  25% {
    box-shadow: 110px 0 var(--loading-color), 100px 0 var(--loading-color),
      0px 0 rgba(255, 255, 255, 0), 0px 0 rgba(255, 255, 255, 0);
  }

  36% {
    box-shadow: 120px 0 var(--loading-color), 110px 0 var(--loading-color),
      100px 0 var(--loading-color), 0px 0 rgba(255, 255, 255, 0);
  }

  50% {
    box-shadow: 130px 0 var(--loading-color), 120px 0 var(--loading-color),
      110px 0 var(--loading-color), 100px 0 var(--loading-color);
  }

  62% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color),
      120px 0 var(--loading-color), 110px 0 var(--loading-color);
  }

  75% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      130px 0 var(--loading-color), 120px 0 var(--loading-color);
  }

  87% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      200px 0 rgba(255, 255, 255, 0), 130px 0 var(--loading-color);
  }

  100% {
    box-shadow: 200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0),
      200px 0 rgba(255, 255, 255, 0), 200px 0 rgba(255, 255, 255, 0);
  }
}

.immersive-translate-modal {
  position: fixed;
  z-index: 2147483647;
  left: 0;
  top: 0;
  width: 100%;
  height: 100%;
  overflow: auto;
  background-color: rgb(0, 0, 0);
  background-color: rgba(0, 0, 0, 0.4);
  font-size: 15px;
}

.immersive-translate-modal-content {
  background-color: #fefefe;
  margin: 10% auto;
  padding: 40px 24px 24px;
  border-radius: 12px;
  width: 350px;
  font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  position: relative;
}

@media screen and (max-width: 768px) {
  .immersive-translate-modal-content {
    margin: 25% auto !important;
  }
}

@media screen and (max-width: 480px) {
  .immersive-translate-modal-content {
    width: 80vw !important;
    margin: 20vh auto !important;
    padding: 20px 12px 12px !important;
  }

  .immersive-translate-modal-title {
    font-size: 14px !important;
  }

  .immersive-translate-modal-body {
    font-size: 13px !important;
    max-height: 60vh !important;
  }

  .immersive-translate-btn {
    font-size: 13px !important;
    padding: 8px 16px !important;
    margin: 0 4px !important;
  }

  .immersive-translate-modal-footer {
    gap: 6px !important;
    margin-top: 16px !important;
  }
}

.immersive-translate-modal .immersive-translate-modal-content-in-input {
  max-width: 500px;
}
.immersive-translate-modal-content-in-input .immersive-translate-modal-body {
  text-align: left;
  max-height: unset;
}

.immersive-translate-modal-title {
  text-align: center;
  font-size: 16px;
  font-weight: 700;
  color: #333333;
}

.immersive-translate-modal-body {
  text-align: center;
  font-size: 14px;
  font-weight: 400;
  color: #333333;
  margin-top: 24px;
}

@media screen and (max-width: 768px) {
  .immersive-translate-modal-body {
    max-height: 250px;
    overflow-y: auto;
  }
}

.immersive-translate-close {
  color: #666666;
  position: absolute;
  right: 16px;
  top: 16px;
  font-size: 20px;
  font-weight: bold;
}

.immersive-translate-close:hover,
.immersive-translate-close:focus {
  text-decoration: none;
  cursor: pointer;
}

.immersive-translate-modal-footer {
  display: flex;
  justify-content: center;
  flex-wrap: wrap;
  margin-top: 24px;
}

.immersive-translate-btn {
  width: fit-content;
  color: #fff;
  background-color: #ea4c89;
  border: none;
  font-size: 14px;
  margin: 0 8px;
  padding: 9px 30px;
  border-radius: 5px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  transition: background-color 0.3s ease;
}

.immersive-translate-btn-container {
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  gap: 8px;
}

.immersive-translate-btn:hover {
  background-color: #f082ac;
}
.immersive-translate-btn:disabled {
  opacity: 0.6;
  cursor: not-allowed;
}
.immersive-translate-btn:disabled:hover {
  background-color: #ea4c89;
}

.immersive-translate-link-btn {
  background-color: transparent;
  color: #ea4c89;
  border: none;
  cursor: pointer;
  height: 30px;
  line-height: 30px;
}

.immersive-translate-cancel-btn {
  /* gray color */
  background-color: rgb(89, 107, 120);
}

.immersive-translate-cancel-btn:hover {
  background-color: hsl(205, 20%, 32%);
}

.immersive-translate-action-btn {
  background-color: transparent;
  color: #ea4c89;
  border: 1px solid #ea4c89;
}

.immersive-translate-btn svg {
  margin-right: 5px;
}

.immersive-translate-link {
  cursor: pointer;
  user-select: none;
  -webkit-user-drag: none;
  text-decoration: none;
  color: #ea4c89;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1);
}

.immersive-translate-primary-link {
  cursor: pointer;
  user-select: none;
  -webkit-user-drag: none;
  text-decoration: none;
  color: #ea4c89;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0.1);
}

.immersive-translate-modal input[type="radio"] {
  margin: 0 6px;
  cursor: pointer;
}

.immersive-translate-modal label {
  cursor: pointer;
}

.immersive-translate-close-action {
  position: absolute;
  top: 2px;
  right: 0px;
  cursor: pointer;
}

.imt-image-status {
  background-color: rgba(0, 0, 0, 0.5) !important;
  display: flex !important;
  flex-direction: column !important;
  align-items: center !important;
  justify-content: center !important;
  border-radius: 16px !important;
}
.imt-image-status img,
.imt-image-status svg,
.imt-img-loading {
  width: 28px !important;
  height: 28px !important;
  margin: 0 0 8px 0 !important;
  min-height: 28px !important;
  min-width: 28px !important;
  position: relative !important;
}
.imt-img-loading {
  background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADgAAAA4CAMAAACfWMssAAAAtFBMVEUAAAD////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////////oK74hAAAAPHRSTlMABBMIDyQXHwyBfFdDMSw+OjXCb+5RG51IvV/k0rOqlGRM6KKMhdvNyZBz9MaupmxpWyj437iYd/yJVNZeuUC7AAACt0lEQVRIx53T2XKiUBCA4QYOiyCbiAsuuGBcYtxiYtT3f6/pbqoYHVFO5r+iivpo6DpAWYpqeoFfr9f90DsYAuRSWkFnPO50OgR9PwiCUFcl2GEcx+N/YBh6pvKaefHlUgZd1zVe0NbYcQjGBfzrPE8Xz8aF+71D8gG6DHFPpc4a7xFiCDuhaWgKgGIJQ3d5IMGDrpS4S5KgpIm+en9f6PlAhKby4JwEIxlYJV9h5k5nee9GoxHJ2IDSNB0dwdad1NAxDJ/uXDHYmebdk4PdbkS58CIVHdYSUHTYYRWOJblWSyu2lmy3KNFVJNBhxcuGW4YBVCbYGRZwIooipHsNqjM4FbgOQqQqSKQQU9V8xmi1QlgHqQQ6DDBvRUVCDirs+EzGDGOQTCATgtYTnbCVLgsVgRE0T1QE0qHCFAht2z6dLvJQs3Lo2FQoDxWNUiBhaP4eRgwNkI+dAjVOA/kUrIDwf3CG8NfNOE0eiFotSuo+rBiq8tD9oY4Qzc6YJw99hl1wzpQvD7ef2M8QgnOGJfJw+EltQc+oX2yn907QB22WZcvlUpd143dqQu+8pCJZuGE4xCuPXJqqcs5sNpsI93Rmzym1k4Npk+oD1SH3/a3LOK/JpUBpWfqNySxWzCfNCUITuDG5dtuphrUJ1myeIE9bIsPiKrfqTai5WZxbhtNphYx6GEIHihyGFTI69lje/rxajdh0s0msZ0zYxyPLhYCb1CyHm9Qsd2H37Y3lugVwL9kNh8Ot8cha6fUNQ8nuXi5z9/ExsAO4zQrb/ev1yrCB7lGyQzgYDGuxq1toDN/JGvN+HyWNHKB7zEoK+PX11e12G431erGYzwmytAWU56fkMHY5JJnDRR2eZji3AwtIcrEV8Cojat/BdQ7XOwGV1e1hDjGGjXbdArm8uJZtCH5MbcctVX8A1WpqumJHwckAAAAASUVORK5CYII=");
  background-size: 28px 28px;
  animation: image-loading-rotate 1s linear infinite !important;
}

.imt-image-status span {
  color: var(--bg-2, #fff) !important;
  font-size: 14px !important;
  line-height: 14px !important;
  font-weight: 500 !important;
  font-family: "PingFang SC", Arial, sans-serif !important;
}

.imt-primary-button {
  display: flex;
  padding: 12px 80px;
  justify-content: center;
  align-items: center;
  gap: 8px;
  border-radius: 8px;
  background: #ea4c89;
  color: #fff;
  font-size: 16px;
  font-style: normal;
  font-weight: 700;
  line-height: 24px;
  border: none;
  cursor: pointer;
}

.imt-retry-text {
  color: #999;
  text-align: center;
  font-size: 14px;
  font-style: normal;
  font-weight: 400;
  line-height: 21px;
  cursor: pointer;
}

.imt-action-container {
  display: flex;
  flex-direction: column;
  gap: 12px;
}

.imt-modal-content-text {
  text-align: left;
  color: #333;
  font-size: 16px;
  font-weight: 400;
  line-height: 24px;
}

@keyframes image-loading-rotate {
  from {
    transform: rotate(360deg);
  }
  to {
    transform: rotate(0deg);
  }
}

.imt-linear-gradient-text {
  background: linear-gradient(90deg, #00a6ff 0%, #c369ff 52.4%, #ff4590 100%);
  background-clip: text;
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
}

.imt-flex-center {
  display: flex;
  align-items: center;
  justify-content: center;
}

.imt-linear-black-btn {
  border-radius: 50px;
  background: linear-gradient(66deg, #222 19%, #696969 94.25%);
  height: 48px;
  width: 100%;
  color: #fff;
  font-size: 16px;
  font-weight: 700;
  display: flex;
  align-items: center;
  cursor: pointer;
  justify-content: center;
}
</style></head><body data-new-gr-c-s-check-loaded="14.1270.0" data-gr-ext-installed=""><noscript><strong>We're sorry but Trustworthy Embodied-AI doesn't work properly without JavaScript enabled. Please enable it to continue.</strong></noscript><div id="app" data-v-app=""><div class="v-application v-theme--light v-layout v-layout--full-height v-locale--is-ltr"><div class="v-application__wrap"><div data-v-0172fc6e="" class="v-container v-container--fluid v-locale--is-ltr project-page"><section data-v-8450487a="" data-v-0172fc6e="" class="intro mb-6"><h1 data-v-8450487a="" class="title">Towards Safe and Trustworthy Embodied AI: Foundations, Status, and Prospects</h1><div class="authors" data-v-8450487a=""><a href="https://tanxincs.github.io/" target="_blank" class="author" data-v-8450487a="">Xin&nbsp;Tan<sup data-v-8450487a="">*</sup></a><span class="sep" data-v-8450487a="">, </span><a href="https://ai45lab.github.io/Awesome-Trustworthy-Embodied-AI/#" target="_blank" class="author" data-v-8450487a="">Bangwei&nbsp;Liu<sup data-v-8450487a="">*</sup></a><span class="sep" data-v-8450487a="">, </span><a href="https://ai45lab.github.io/Awesome-Trustworthy-Embodied-AI/#" target="_blank" class="author" data-v-8450487a="">Yicheng&nbsp;Bao</a><span class="sep" data-v-8450487a="">, </span><a href="https://fangzhou2000.github.io/" target="_blank" class="author" data-v-8450487a="">Qijian&nbsp;Tian</a><span class="sep" data-v-8450487a="">, </span><a href="https://ai45lab.github.io/Awesome-Trustworthy-Embodied-AI/#" target="_blank" class="author" data-v-8450487a="">Zhenkun&nbsp;Gao</a><span class="sep" data-v-8450487a="">, </span><a href="https://ai45lab.github.io/Awesome-Trustworthy-Embodied-AI/#" target="_blank" class="author" data-v-8450487a="">Xiongbin&nbsp;Wu</a><span class="sep" data-v-8450487a="">, </span><a href="https://ai45lab.github.io/Awesome-Trustworthy-Embodied-AI/#" target="_blank" class="author" data-v-8450487a="">Zhihao&nbsp;Luo</a><span class="sep" data-v-8450487a="">, </span><br data-v-8450487a=""><a href="https://ai45lab.github.io/Awesome-Trustworthy-Embodied-AI/#" target="_blank" class="author" data-v-8450487a="">Sen&nbsp;Wang</a><span class="sep" data-v-8450487a="">, </span><a href="https://ai45lab.github.io/Awesome-Trustworthy-Embodied-AI/#" target="_blank" class="author" data-v-8450487a="">Yuqi&nbsp;Zhang</a><span class="sep" data-v-8450487a="">, </span><a href="https://wangxuhongcn.github.io/" target="_blank" class="author" data-v-8450487a="">Xuhong&nbsp;Wang<sup data-v-8450487a="">§</sup></a><span class="sep" data-v-8450487a="">, </span><a href="https://causallu.com/" target="_blank" class="author" data-v-8450487a="">Chaochao&nbsp;Lu<sup data-v-8450487a="">§†</sup></a><span class="sep" data-v-8450487a="">, </span><a href="https://scholar.google.com/citations?user=h3Nsz6YAAAAJ&amp;hl=zh-CN&amp;oi=ao" target="_blank" class="author" data-v-8450487a="">Bowen&nbsp;Zhou<sup data-v-8450487a="">§‡</sup></a></div><div data-v-8450487a="" class="aff-row"><div data-v-8450487a="" class="aff-item"><a data-v-8450487a="" href="https://www.shlab.org.cn/" target="_blank" rel="noopener" class="aff-logo-link" aria-label="Shanghai AI Lab"><img data-v-8450487a="" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAGIAAAAlCAIAAAA2mquNAAAXcElEQVRoBe1ad1RbV5r3ObM7u2f37J6z5UwmkzLJJJNkJ5nsZNM9iePESVziEmInxI47NtjGBmPcwPQOpvfeRRFF9CpUEJJAQkK9ARKSUEUIFSTU2b3vCZmA48Qzs3P2j9X5ncdD7717v3t1v+/7fb/7dqz//+cnzMCO9fV1D/isu92eR8HjcXs8Lrf7/yzcbmgY62Agf1kj3W7PDo/Ho1oyzMq0QqmGL1FzxUr2nIIpWqQLZDS+lMJdILPFROY8gTGLp4vw9FksTYiZFo5RBaNT/JFJ/vAkb4jMHSRxB4icARKnb4LdS2D3jLO68cwuPAOFm0HhGCgcoxM704EBaB+jI0dpSDQMesvI9AZoLSO0jXPvl62jNOQorX0MPNWBmenEgta6cMyecXbfBOirj8AeIHJhEJlivdHi8Xj4EvUgidszzuoZZ3XhmSgcA3SNhQAaAcbATUHnjE4ssLB7nNVDYPVNsPuJnEESd3iSNzrFx0wLcTSRUKoB02QwWzV6s3rZpNIZFVqDQmtY1K7INStSlX5BuSxW6ObkWpFUI5SqoalU8cQqrljJmQcTyppdZM7KGSL5jFBOF8hpfNk0T0blgfmd4ixMsiWTbAmZJSazJSSWmMQSTzDmJxhzE4x5wszcOH0WBp4+64PvS/iEMANuBs8yAcgs8RRngcqT0vigu80QLKjNFtv6+rpCa5gRyqf5Mgp3gcIFNsBdbz6S2aApMgtcgo2kcEGz0zwpXSBjiBZZswrOvJInVgkW1CqdATid3eG0rNkta/ZVK4DZajNZ1kyWNePqmsFsXTFZ9UYLgMmybFxdNlp0hlUYSytmGFq9GYZGb9LoTeplIwSTetkLlc74o9i4GX7WCDel1Xu7gHtchixZMQGrDGZgnmXNDscWh9MFG2wwW2GzgeWQzT6DdYbV7Tb7jIcv+ToCfZksKybQBVhNEoWOLpBNQ0sAdrFx+hx2WjRGFQ6ReH0T7G48E4UFS7cNTUei6a2jtObh6aZhKmKI2jhIqR+YrO0j1/SSq3tIVT2kym5SOWqiDEUo7SSUdHhR3D5e1LYVBa24AiRAYRu+qA1f3D5e2jlRBj1bjpqo6CJW9ZDq+qcaBqYaBylNw8AT28boKCyjZxx4xwCRO0jizQjlcHhVaA0jIAIADBC5fRPA97vwzDbI4JYRYDNiiOK1uX+ytm+ypheYDaOuf6re2xG1ZWQaiaZ3YICD9xE4TNEimKZFzQpPrOLMKdlzSqZIMSMEvkPlSac4CySWmMCYw9FE2GkRFJKEaMrmqMSDo1I/kdNP5PRNcDYCE6t7HKAL7wUKx9wCKC4wO7E+MFA4ZheeCT/YjQeRpZfAHiCBwAcFCxAKx6hCHA14KAE47zyRKRYsqOFp0urN8G9MZIJLBMYciKTQj+0Lo8OT/CEyZDPJazAU4zj9RG4/kTNA5A6RecOToCM0RQAFplnCzLxIpvVmOveDROZxutwOp8vucNodrjW7Y83usNocG15pA15psZmtALBj+pY68AUzWOc+QE66+ugjfDPkRMBZDGarcRX4u9kK+oL7tdqAGQ6nC6TkjY/L7Xa6Hg6H0wUPweZw2hxOeAibR2G2eAMLHF7gI9wj3KnVBka9ZnfY7E6ny7XD7fZgqMLKblJpB6GobTyvBZuNGMuoH0mpGUqsGowq6b1T0HUzr/NGbsf17PaQzLarGcjgjNYr6a2X01supbUEpTZfTG66kIQISEQEJDWdT0ScS0CcjW88m9AIH8/ENzwC8G3nEhrPg8cRF6CmLiYjAlOarqS3XstEhma1hWW338zrvFPQhcIxjKtrG7O0jqYIcpowuc3YvBZsfgsORl4LNqcZk9mIzqgfTakZjqvojyrpjSjqvlPYfTMPdSO3IzSr7dp9ZHAGEjY+KLU5KLX5UlrLpdSWK+nIq/fbQjLbr2d3hOeibhd0RRb3xpUPdOGYwOnECh3sZb68ABjAzNz4jDdCwYt2ZApO/8D5vQyAyO2f4PQRvL4G8QBWF46FwkIuBh83uVsnCHBwAt7wQSwThWV24ZjdeCbsZb3jIM33T4CsDLkAb2SSPzoF3E2woLbZnb5pEit0IGGxvakKTlggsbJBSiUyxYQZb7gYo8KxQrDBYGD7gfH9E+AIU4pBEhjaMGA5/NEpAZoixEyLcDSRYEENnO7P/EDs1Htwgw+gqS6X+0fhY7MeD0xxvY38mfb8bzwOnI7Gl3XjWe1jMy0j0w0DU1XdxNIOQn4LLqN+NL6i/15x9+2CznDI6a7dR15OA6v0YnLTeeBW9adj607G1J6Iqvn2XvU3EVVfR1QevVPx1Z3yH8LROxXH7nrxTUSVf2TVieiaU7F1Z+IazsY3BCQhLiY38SUq31CXVswNA1P369GFSHx1N7lxgNIyQmtD0zsxDBDm8Szv6iNw+ic4vePsbjyrE8toG6MjhihV3aSitvGcJkxG/Why9VB85UBUKYghYdntwRmtQanNgSlNMIJSmy+nNV/LRF7PbgvPBQ4eVdIbXzGQVjec14IdowjBanI4XdY1x6rVbly1rpgsuhWzZtmkXDJIVfo5uVawoOZJVFASXGR42bmMCtijBKZnE4z58RmQfbDTIvSUYHSSP0Lm/RBGJ/mjk3z0lGCMIhyjCLHTIjxdRGTOb1BHCZUrNa5afdPkcLogS5aVS4alFdOKyWJctZotNssayCpWm33N5tgMqw0MxGyxGcyWpRWzSmdc1K5IVctixdKcfEko1XDFStasAq4xaHyZDzCrZIoW2XMK7rxSsKCelWkXlMsKrcFoXvtJTrfZrf6y577p+GuePGIIP2QGCOFavUmi1MFFyawcLu5U3HklQyif5knJLAm0XuawNEA4R6Zg6sHrJ4LI3Y1nQSUbo31sBjlKb/0BINH0tjEIaHr7GCjuUDgGiSV2OF3bLZuVaalc6YxAzpoFPyxfohLJtPOLOolyeUG5LFPrZWq9XKNf1K4salfg6mp7jTW/uAQXqoIFNXceeMOMUE7lgRJ1fGYWQxWMUQRoCGNUAQbQMdE4HRRGZJaYwl2g8aVM0SK8rFQ64w6X2908PB1R1H0zr/N6dvvV+8hLac0XkkDcORFV8/XdSr9b5Udulh26UXrgevH+0OK9IUWfXyv69GrhnuCCT67kf3w5f/elvI+Ccj8KytsVmPvhxZyHYldgrg8fBYH7d1/KvV3QZbI8SPC++cpGjB27U/ntvZqTMSBmBSQ2XklvDcvuuJ3fda+4N6asL6FyMLlmKLV2OK1u+H4DGkZG/Whq7Uhi1WBseX9kcc+NnI6rGcig1OaApMYzcfUnY+qOR9V8HVHld7vi0I2yA9dL9oUU7b1WtDekaF9I8f7Qki/CSg+GlR4OL/e7VXnsbtU3EVUnomrPxNVfTG6q6Z18uNNtW5brUDLaevSlqp94spkc+iZly8nmrrdc+tP+3dzgxvnWgWyMbuP6pr9wpyDTccUqEH0pgFbABRHsTZ1YRjtmpm0MFHGto3S4LGoengYF3RC1aYiKGHw8zAjlj54prd7cT+R0YkEW691gTyOTPMy0EE+fnWACqWCSI/FV8zS+jC6QwcoEXF2R2SBE4GhgOMMQv+sjAG0HzoAdUBJsHYVFmwdHOFZAkWGmHRJbUBtyzSCJy5pVAKfrn+DkNmPvN4ym1AzFA9rac6eg63p22yWwYhGnY+u+i645fq8GyvdVR+9AbhgO3PBgWMmBUMgTrxV+Flyw50r+FnwanP9ZcMFnwQWfXy3ce62oqpvkdj+oNravDr5EfTOv83JaS2hW+808VERRd2x5X2rtcHbTWGEbvrSTUNlNrOkl1w9MIYaozcNQjQrUK6BbIQYp9f1T1T3k0g5CXgs2o340CThgX2QxiCehWW2X01ouJDWdjgXBxD+yyj+y2ofj92q+i649FVN/Jq7hfCIiKLUlOAMZltNxt7A7trwfsHCfrR4P4IROl8tmd1ptDtPqmh7STDRADDEqlwyLGq8IJVHo5hdBfhVJAV0AYt68ijW7yBDKt4AJCTesWQV7DuhT6mWTr7uHnjicLli7MFlAQbdmA3WcywV0yYfe/4gv4eE4nK41QBFABbdisuoMqzDX2Rz4FVqDcgnoPGqdSbNs0hlW9Uag0pgtNqvNYXc4gXq5vr5uWl3T6gFRkqn1YsWSUKpmzympXOk4fRZNEQwQgRLYgZlBoulNw9SGAUptH9BMKromyjonSqBKsACJz2/F5bXgvBVWKy6/FVeIxJPZYpfb7RuM1eao6iZWdZNq+8iNg5SWkelhMk+5ZPDdAJ+YLTahVCNeXJJrVtQ647LBYjBbV602q81ud4CaFpo6IOr64HKDct1mB8KZcXVNZ1hV6YxyjV6s0AmlGtbs4jRfOsGYR08JBogcn6oJK6vdeCDLABGCzMNQRYSZuUn2Al0g584r5+RLUtWy3mgBTteJYcRXDNwp6ArJbAtKbT4TV+8fWX3oRumeK/kfXMx55+z9P3yX+tq3yf/xTdJLRxNe8It77kjMrw/HPHMw+qkD93514N4v90U+sTfiib0Rv/g84hfQ8Ym9kU/sjXz6YFRa3bDN8aAK0+rNL/jFPnso+rkjMS9+Ff/ysYRDN0pxtNkt00QXyE7H1QcmN4Vmtt8r7k2vGy1oxVV2ERsGppBoGgrH6CWwBoicIRJ3BBI9BomcPgIbhWW2jEzX9JALWvHJ1UMRRT2hWe2BKU2n4+q+iag6HF76WXDBhxez3z6T/sbJ1NePJ//+eNLvjye/fiLlD9+lvnk6452z998/n7UrMHfPlfz9ocVf3io/fq/mfELj1QwkYogKeJNMrWfPKRmiRRokjAJVlAnU2HFQOs5ioOiOpghHp0CMB9UjmT9E4gFAMRLSg3iDpK0YIvNEMs3mYGSzO0GWIPGGyfyRSdAamS1ZWjFvmSa90TLJBnGaxpcB8gIprbOAOi1JlDqpClAnwJs0XtL0IBoodXPyJcGCmjP//eFA4jJUycNCPqiENwMD1DQRlibaELNgAiWZ4ixM86QLquUHsWmLrbBv2x0uq81httrgIkar9+rlsFIuBhFKK5JphFKNQKoWSAHBX7M7fPwAjikez/rSihkamF65ZAD+v7K6YrKYLTan64FLbjHgL/vvxnCcUFlmM1vXYEC+DEQlu8PpcLrg7ZntoXCHw+m6kdPx+vGUV/2BW718LPG3R+Nf/Cr++S9jnzsSCzvX0wejnz4Y/dQXUb86APDkgXtPHrj3y/2RT+yLBF62N+IX+yJhvPJNEntOsWWEa3bHyZjap77wPvirA1FPfRH19MHoP17M7iWwt9z8iH8HSdyz8fVf3iw7eqfi67uV30Ak8NidCr9bZYdulOwLKfzkct4fL2S9dTr9D9+l/v54yu/8k14+lvDSsYQXv4r7jV/sc4djnz0U88wX0c8c3IpnD0X/+nDMc0din/8y9vkvY17wi3vxq7iXjiW88nXSq9+m3C3sBk6nM6zK1HqJYlkk1XDnlXSBjMQSY6iiASK3C8dsQ88gBqm1vZMVXcTi9vG8ZmxW41ha7Uhi5WBsWd+94t7b+aDmDsvuCMvuiCrpVWi3hmSH09U8PJ1SM5RRP5qNwBS3EypQxJqeSRSWKVUtP2JetlyyO1zGVa+2aVq1+QDLp768rFwyytUrC0owHM6cki6QE5nzmGnhAJGLwjHaIPYASN8QtQmmFGgaKJ6wjB48a5DIRU8JxulzU5wFhlDOl6jFCp3OsAqcDmTN7ym5XhkXkj5tsOTqO8I67Pbj5oy2ZXjwv3AO8rVjXF0zAaUVpGqb3bl9nT+0ke1fAm+CNll92rTN7lV1N4Tp7w3HZPneiDZs8Gq+QLyGuMhmqdfhdIFM1z5GjyrpDclEBiQ2Ho+q+fJW+d6Qwl1B2e+ey3jjJMhxr3yd+NujCb/5Mva5wzHPQiv26S+ADz65H0pz+yKe3B/JFC1uH4PvG49n/UIS4kW/+Bf84n/7VfwrXye+6p/0nydS3zyVvjMgqxCJh7fYfPf/xBOPx7NsWKVwFyYYc2iKAIVl1PaSc5uxceUDYTkdF5KajkfVHLlZtjek8KNLue+ey3zjZPrrJ1Je+zb5Vf+kV/2TX/sWZLo3Tqa/eTr9nbP3dwZk7Q7K+yy44GBYydHbFadi6y6lNd/KR3ViGWCaEEPU8FzAfc8lNJ6Iqj12t+pweNn+0OJPrxbsvpz3wcWcnQHZ75/Peu985jtn7799JuPtMxlvnfbizVPpb55Kf+t0Bk/8QEvbPkiPZ/1mXufOgOz3zme9H5C1MyD7g4s5uwJzP76c//m1orLOiT95mhRaQzee1YamNw5SylET2YixhMrB2/ldwRnI84mNJ6K9w9kXUrTnSsHuS3m7gnI/uJgD48PA3F1BuR9dyv34Sv6e4ILPrxUeuF5yKLzU71aFf2T1qdi6C0mIkMy25uFp4HRuN9hNgbkZTL61evOiZkWi0M3KtHyJmj2nAHRBIJviSEgssG2Lp89OcSQLymV4u1i9bHqoJOLxeDTLJqFUI5Jp4Z1hoI1sJHiJUifX6FU6o94Ish5EeV3OP4lzb/9h4HGZLGvLEPNe1BokgC5487JQCvQikVQzJ18SK3QSpU6m1iu0K+pl09KKecVkNa2uQWwWbKt4PJ4dbo8HTREAVbcVl4UYS6sbSagcjCrpu5mHugaxzfOJiFOx9cfBKqv0u1VxOLzsi7CS/aHFN3I64T2yh5oIf+l0uap7SJDyW+kfWf1dTO3puPrziYjAlObgDOT17I7bQE4Fwkha3UgWYqwAiS9DTWxPAo/o4hGXllbMOU2YOK+u0nkts+1yemtgSvPFlKaLKUCnvpTWciW99ep9ZEhmW3ge6k5Bd1RpX1z5QHL1UEbDaE4TpqhtvBw1gafN7nC63OUoYmBK87mExpMxgLD63ao4eKN0b0jRnmCwSj8MzN0ZkP3uucy3Tme8cTLt9ROpr32b8jv/5O+i6x4dj+DkkIUYe+fs/f86lfb22Yz3zmftDMj6MBCIU59cyf80uGBfaPHBG6V+tyv8I4G6dC6hMSi1RbCgecTgf/ql/yF35xIajtwsO3C9+PNrhXuuFHx8OR/oYkGQrwXl7b4E9DJgydXCz68V7Q8tOXij9MjNsmN3K49HgV80IAlxKbWlrn8KEAKD2Qo0Y41hQbk8v+hjsZB0yZbA21B4+iyGKhyd4kPqBK9/gktkzq+YHojWm633eDw2uxNOajS+FN7LHp7kjVGAHkJkgi0jKnipQe4j2fOLOqkKCJIqndG+qb7Z3OzjntsdTrFCBwcNeCvb+wIIRzIJvQZC4UrB6xV8KU0gmxFCYqlYxZcAkixWwG5oUC+bTJY1EMIHidzcZkxGw2hS9VBMWV9kcc+tfFQYpP5dTgOrtLSD8FgmWtbsHZiZE9GQOhFbdya+/lxC44XkpsAUsHEYnAEWeVhOx618VERRT0xZf2LVUHo9WOTF7YSqHnLjIBWJpnfhmBiqUG+0PFbXj3uzx+Mpahu/kdsRntt5uwBIN9GlffEVAym1w/cb0XktuNKOCRxNBJwuqXpwd1DuW6czXvVPfsEv7plD0U/si/iXPbf/affNf9gV/vOd109E1z5W9wazNbFq8GfvhfzsvZC/eR/gb3eG/nzn9b/7Y9jffxD2jx+F//PHN//10ztP7APl8W/84n7nn/zm6YxdgbkHQkv8I6sDEkF+uVvYndOEeSz++VhGwje73Z7D4aX/9tntf//s9pP7I585FP38kdiXjyW8fiL13XOZn1zJPxxeVtCKA06n0Br4Em+5OM2XkdmSceBiopFJPpAd8CwKd+GxLLA7nAzRYm3fZMMApWmI2jpK68AyuvBMIEgSOUNkLpoiwE6LCIx5MuR9M0I5e07Bk6iEG6lHqtLLNbADPmRP4bGM+dGb6QIZlgbecAP7BVDJTReAkpsDdivUIplWvWzcWvpCKrD3BUMfu/1Rhr3dFJcbMAy7A7yyAb/4AL8W4X3dz+XdHIb3e7c//tf8xleoQ3Wvd/95iwH/Df5WfCWTe6ZgAAAAAElFTkSuQmCC" alt="Shanghai Artificial Intelligence Laboratory" class="aff-logo"></a><div data-v-8450487a="" class="aff-names" title="" style="width: 217px; flex: 0 0 auto;"><div data-v-8450487a="" class="aff-cn" style="width: 217px; letter-spacing: 9.125px; word-spacing: 0px; padding-right: 0px; white-space: nowrap; display: inline-block;">上海人工智能实验室</div><div data-v-8450487a="" class="aff-en" style="width: 217px; letter-spacing: 0px; word-spacing: 0px; padding-right: 0px; white-space: nowrap; display: inline-block;">Shanghai Artificial Intelligence Laboratory</div></div></div><div data-v-8450487a="" class="aff-item"><a data-v-8450487a="" href="https://www.ecnu.edu.cn/" target="_blank" rel="noopener" class="aff-logo-link" aria-label="East China Normal University"><img data-v-8450487a="" src="./Awesome-Trustworthy-Embodied-AI_files/ecnu.4992016f.png" alt="East China Normal University" class="aff-logo"></a><div data-v-8450487a="" class="aff-names" title="" style="width: 149px; flex: 0 0 auto;"><div data-v-8450487a="" class="aff-cn" style="width: 149px; letter-spacing: 10px; word-spacing: 0px; padding-right: 3px; white-space: nowrap; display: inline-block;">华东师范大学</div><div data-v-8450487a="" class="aff-en" style="width: 149px; letter-spacing: 0px; word-spacing: 0px; padding-right: 0px; white-space: nowrap; display: inline-block;">East China Normal University</div></div></div><div data-v-8450487a="" class="aff-item"><a data-v-8450487a="" href="https://www.tsinghua.edu.cn/" target="_blank" rel="noopener" class="aff-logo-link" aria-label="Tsinghua University"><img data-v-8450487a="" src="./Awesome-Trustworthy-Embodied-AI_files/thu.54d19a61.png" alt="Tsinghua University" class="aff-logo"></a><div data-v-8450487a="" class="aff-names" title="" style="width: 100px; flex: 0 0 auto;"><div data-v-8450487a="" class="aff-cn" style="width: 100px; letter-spacing: 10px; word-spacing: 0px; padding-right: 6px; white-space: nowrap; display: inline-block;">清华大学</div><div data-v-8450487a="" class="aff-en" style="width: 100px; letter-spacing: 0px; word-spacing: 0px; padding-right: 0px; white-space: nowrap; display: inline-block;">Tsinghua University</div></div></div></div><div data-v-8450487a="" class="actions"><a data-v-8450487a="" href="https://openreview.net/pdf?id=Eu6Yt21Alv" target="_blank" class="action-btn"><svg data-v-8450487a="" class="btn-icon" width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path data-v-8450487a="" d="M14,2H6A2,2 0 0,0 4,4V20A2,2 0 0,0 6,22H18A2,2 0 0,0 20,20V8L14,2M18,20H6V4H13V9H18V20Z"></path></svg> Paper </a><a data-v-8450487a="" href="https://github.com/ai45lab/Awesome-Trustworthy-Embodied-AI" target="_blank" class="action-btn"><svg data-v-8450487a="" class="btn-icon" width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path data-v-8450487a="" d="M12,2A10,10 0 0,0 2,12C2,16.42 4.87,20.17 8.84,21.5C9.34,21.58 9.5,21.27 9.5,21C9.5,20.77 9.5,20.14 9.5,19.31C6.73,19.91 6.14,17.97 6.14,17.97C5.68,16.81 5.03,16.5 5.03,16.5C4.12,15.88 5.1,15.9 5.1,15.9C6.1,15.97 6.63,16.93 6.63,16.93C7.5,18.45 8.97,18 9.54,17.76C9.63,17.11 9.89,16.67 10.17,16.42C7.95,16.17 5.62,15.31 5.62,11.5C5.62,10.39 6,9.5 6.65,8.79C6.55,8.54 6.2,7.5 6.75,6.15C6.75,6.15 7.59,5.88 9.5,7.17C10.29,6.95 11.15,6.84 12,6.84C12.85,6.84 13.71,6.95 14.5,7.17C16.41,5.88 17.25,6.15 17.25,6.15C17.8,7.5 17.45,8.54 17.35,8.79C18,9.5 18.38,10.39 18.38,11.5C18.38,15.32 16.04,16.16 13.81,16.41C14.17,16.72 14.5,17.33 14.5,18.26C14.5,19.6 14.5,20.68 14.5,21C14.5,21.27 14.66,21.59 15.17,21.5C19.14,20.16 22,16.42 22,12A10,10 0 0,0 12,2Z"></path></svg> GitHub </a></div><section class="abs-band" data-v-8450487a=""><div class="abs-decorations" data-v-8450487a=""><div class="abs-line abs-line-1" data-v-8450487a=""></div><div class="abs-line abs-line-2" data-v-8450487a=""></div><div class="abs-dot abs-dot-1" data-v-8450487a=""></div><div class="abs-dot abs-dot-2" data-v-8450487a=""></div><div class="abs-dot abs-dot-3" data-v-8450487a=""></div><div class="abs-gradient-orb abs-orb-1" data-v-8450487a=""></div><div class="abs-gradient-orb abs-orb-2" data-v-8450487a=""></div></div><div class="abs-inner" data-v-8450487a=""><div class="abs-title-wrapper" data-v-8450487a=""><div class="abs-title-line" data-v-8450487a=""></div><h2 class="abs-title" data-v-8450487a="">Abstract</h2><div class="abs-title-line" data-v-8450487a=""></div></div><p class="abs-text" lang="en" data-v-8450487a="">The increasing autonomy and physical capability of Embodied Artificial Intelligence (EAI) introduce critical challenges to safety and trustworthiness. Unlike purely digital AI, failures in perception, planning, or interaction can lead to direct physical harm, property damage, or the violation of human safety and social norms. However, current EAI foundation models disregard the risks of misalignment between the model capabilities and the safety and trustworthiness competencies. Some works attempt to address these issues, however, they lack a unified framework capable of balancing the developmental trajectories between safety and capability. In this paper, we first comprehensively define a new term <em data-v-8450487a="">safe and trustworthy EAI</em> by establishing an L1-L5 levels framework and proposing ten core principles of trustworthiness and safety. To unify fragmented research efforts, we propose a novel, agent-centric framework that analyzes risks across the four operational stages of an EAI system. We systematically review state-of-the-art but fragmented solutions, benchmarks, and evaluation metrics, identifying key gaps and challenges. Finally, we identify the need for a paradigm shift away from optimizing isolated components towards a holistic, cybernetic approach. We argue that future progress hinges on engineering the closed-loop system of the agent (Self), its environment (World), and their dynamic coupling (Interaction), paving the way for the next generation of truly safe and trustworthy EAI.</p></div></section><figure data-v-8450487a="" class="hero-image"><div data-v-8450487a="" class="v-responsive v-img full-img" contain=""><div class="v-responsive__sizer" style="padding-bottom: 62.0476%;"></div><img class="v-img__img v-img__img--contain" src="./Awesome-Trustworthy-Embodied-AI_files/figure1.png" style=""><!----><!----><!----><!----><!----></div><figcaption data-v-8450487a="" class="caption"> Figure 1. Capability-Safety divergence in the Embodied AI landscape </figcaption></figure><div class="motivation-block" data-v-8450487a=""><h2 class="section-title" data-v-8450487a="">Research Motivation</h2><p class="section-text" style="margin-bottom:8px;" data-v-8450487a=""> While embodied AI (EAI) products are rapidly improving in capability, they often lack reliable safety mechanisms. In contrast, academic safety research remains fragmented and lags behind in capability. This divergence has raised significant concerns about the trustworthiness of physical AI systems.</p><p class="section-text" style="margin-top:0;" data-v-8450487a=""> To address this gap, we propose a unified research framework that integrates capability advancement and trustworthy safety, paving the way for the development of safe and aligned embodied agents. </p><div class="tags" data-v-8450487a=""><span class="tag" data-v-8450487a="">EAI Safety</span><span class="tag" data-v-8450487a="">Capability-Safety Gap</span><span class="tag" data-v-8450487a="">Trustworthy AI</span><span class="tag" data-v-8450487a="">Embodied Intelligence</span></div></div><section data-v-8450487a="" class="figure2-section f2-compact" id="figure2"><h2 data-v-8450487a="" class="f2-title">The Five Levels of “Make Safe EAI”</h2><p data-v-8450487a="" class="f2-lead"> We chart the progression from <strong data-v-8450487a="">Resistance</strong> (L1–L2) to <strong data-v-8450487a="">Resilience</strong> (L3–L5), moving from refusal and oversight to adaptive learning and verifiable guarantees. </p><figure data-v-8450487a="" class="f2-figure"><img data-v-8450487a="" src="./Awesome-Trustworthy-Embodied-AI_files/figure2.png" alt="Figure 2: The five levels of Make Safe EAI"><figcaption data-v-8450487a="">Figure 2. From Resistance (L1–L2) to Resilience (L3–L5).</figcaption></figure><div class="f2-row f2-row-top" data-v-8450487a=""><article class="f2-card" data-v-8450487a=""><h3 data-v-8450487a="">L1 — Alignment <span data-v-8450487a="">(Foundational Resistance)</span></h3><ul class="f2-bullets" data-v-8450487a=""><li data-v-8450487a=""><span class="dot" data-v-8450487a="">•</span><span class="label" data-v-8450487a="">Goal</span><span class="text" data-v-8450487a="">Refuse harmful instructions; follow basic safety norms.</span></li><li data-v-8450487a=""><span class="dot" data-v-8450487a="">•</span><span class="label" data-v-8450487a="">How</span><span class="text" data-v-8450487a="">Instruction tuning, RLHF, safety filters, red-teaming data.</span></li><li data-v-8450487a=""><span class="dot" data-v-8450487a="">•</span><span class="label" data-v-8450487a="">Limit</span><span class="text" data-v-8450487a="">Correlation-based; vulnerable to jailbreaks and shifts.</span></li></ul></article><article class="f2-card" data-v-8450487a=""><h3 data-v-8450487a="">L2 — Intervention <span data-v-8450487a="">(Oversight-based Resistance)</span></h3><ul class="f2-bullets" data-v-8450487a=""><li data-v-8450487a=""><span class="dot" data-v-8450487a="">•</span><span class="label" data-v-8450487a="">Goal</span><span class="text" data-v-8450487a="">Let humans halt/redirect <i data-v-8450487a="">before</i> risky actions.</span></li><li data-v-8450487a=""><span class="dot" data-v-8450487a="">•</span><span class="label" data-v-8450487a="">How</span><span class="text" data-v-8450487a="">Interrupt channels, intent/plan display, trajectory visualization.</span></li><li data-v-8450487a=""><span class="dot" data-v-8450487a="">•</span><span class="label" data-v-8450487a="">Limit</span><span class="text" data-v-8450487a="">Needs constant oversight; weak scalability to high autonomy.</span></li></ul></article></div><div class="f2-row f2-row-bottom" data-v-8450487a=""><article class="f2-card" data-v-8450487a=""><h3 data-v-8450487a="">L3 — Mimetic Reflection <span data-v-8450487a="">(Foundational Resilience)</span></h3><ul class="f2-bullets" data-v-8450487a=""><li data-v-8450487a=""><span class="dot" data-v-8450487a="">•</span><span class="label" data-v-8450487a="">Goal</span><span class="text" data-v-8450487a="">Internalize validated safe behaviors.</span></li><li data-v-8450487a=""><span class="dot" data-v-8450487a="">•</span><span class="label" data-v-8450487a="">How</span><span class="text" data-v-8450487a="">Imitation learning, behavior cloning, curated safety playbooks.</span></li><li data-v-8450487a=""><span class="dot" data-v-8450487a="">•</span><span class="label" data-v-8450487a="">Limit</span><span class="text" data-v-8450487a="">Limited generalization to novel tasks and combinations.</span></li></ul></article><article class="f2-card" data-v-8450487a=""><h3 data-v-8450487a="">L4 — Evolutionary Reflection <span data-v-8450487a="">(Adaptive Resilience)</span></h3><ul class="f2-bullets" data-v-8450487a=""><li data-v-8450487a=""><span class="dot" data-v-8450487a="">•</span><span class="label" data-v-8450487a="">Goal</span><span class="text" data-v-8450487a="">Continual self-improvement; proactive patching.</span></li><li data-v-8450487a=""><span class="dot" data-v-8450487a="">•</span><span class="label" data-v-8450487a="">How</span><span class="text" data-v-8450487a="">Continual learning, self red-teaming, safety-aware exploration.</span></li><li data-v-8450487a=""><span class="dot" data-v-8450487a="">•</span><span class="label" data-v-8450487a="">Limit</span><span class="text" data-v-8450487a="">Empirical assurance only — no prior formal guarantees.</span></li></ul></article><article class="f2-card" data-v-8450487a=""><h3 data-v-8450487a="">L5 — Verifiable Reflection <span data-v-8450487a="">(Guaranteed Resilience)</span></h3><ul class="f2-bullets" data-v-8450487a=""><li data-v-8450487a=""><span class="dot" data-v-8450487a="">•</span><span class="label" data-v-8450487a="">Goal</span><span class="text" data-v-8450487a="">Provable safety/stability of closed-loop behavior.</span></li><li data-v-8450487a=""><span class="dot" data-v-8450487a="">•</span><span class="label" data-v-8450487a="">How</span><span class="text" data-v-8450487a="">Reachability/invariance analysis, control-theoretic synthesis, neuro-symbolic proofs.</span></li><li data-v-8450487a=""><span class="dot" data-v-8450487a="">•</span><span class="label" data-v-8450487a="">Limit</span><span class="text" data-v-8450487a="">Model/compute-intensive; engineering maturity evolving.</span></li></ul></article></div></section></section><div data-v-0172fc6e="" class="header"><h1 data-v-0172fc6e="" class="title"><span data-v-0172fc6e="" class="title-decor" data-text="Awesome">Awesome</span> <span data-v-0172fc6e="" class="title-chip">Trustworthy</span> <span data-v-0172fc6e="" class="title-decor" data-text="Embodied-AI">Embodied-AI</span></h1></div><div data-v-0172fc6e="" class="search-container"><!----><div data-v-0172fc6e="" class="tag-matrix-container"><section data-v-5da9c21a="" data-v-0172fc6e="" class="tmx tmx--compact" style="--tmx-col-min: 112px; --tmx-row-label-w: 168px;"><div data-v-5da9c21a="" class="tmx__toolbar"><div data-v-5da9c21a="" class="tmx__title"><span data-v-5da9c21a="" class="bar" aria-hidden="true"></span><h2 data-v-5da9c21a="">Filter by Categories</h2></div><div data-v-5da9c21a="" class="tmx__left-actions"><div data-v-0172fc6e="" data-v-5da9c21a-s="" class="search-bar"><div data-v-0172fc6e="" data-v-5da9c21a-s="" class="v-input v-input--horizontal v-input--center-affix v-input--density-default v-theme--light v-locale--is-ltr v-text-field search-input"><!----><div class="v-input__control"><div class="v-field v-field--appended v-field--center-affix v-field--prepended v-field--no-label v-field--variant-filled v-theme--light v-locale--is-ltr"><div class="v-field__overlay"></div><div class="v-field__loader"><div class="v-progress-linear v-theme--light v-locale--is-ltr" role="progressbar" aria-hidden="true" aria-valuemin="0" aria-valuemax="100" style="top: 0px; height: 0px; --v-progress-linear-height: 2px;"><!----><div class="v-progress-linear__background"></div><div class="v-progress-linear__buffer" style="width: 0%;"></div><div class="v-progress-linear__indeterminate"><div class="v-progress-linear__indeterminate long"></div><div class="v-progress-linear__indeterminate short"></div></div><!----></div></div><div class="v-field__prepend-inner"><i class="mdi-magnify mdi v-icon notranslate v-theme--light v-icon--size-default" aria-hidden="true"></i><!----></div><div class="v-field__field" data-no-activator=""><!----><!----><!----><input placeholder="Search papers by title" size="1" type="text" id="input-v-1" dense="" outlined="" class="v-field__input" value=""><!----></div><div class="v-field__clearable" style="display: none;"><i class="mdi-close-circle mdi v-icon notranslate v-theme--light v-icon--size-default v-icon--clickable" role="button" aria-hidden="false" tabindex="-1" aria-label="Clear "></i></div><div class="v-field__append-inner"><!----><!----></div><div class="v-field__outline"><!----><!----></div></div></div><!----><!----></div><button data-v-0172fc6e="" data-v-5da9c21a-s="" type="button" class="v-btn v-btn--elevated v-theme--light v-btn--density-default elevation-0 v-btn--size-default v-btn--variant-elevated search-btn" small=""><span class="v-btn__overlay"></span><span class="v-btn__underlay"></span><!----><span class="v-btn__content" data-no-activator=""> Search </span><!----><!----></button></div></div><div data-v-5da9c21a="" class="tmx__spacer"></div><div data-v-5da9c21a="" class="tmx__cta-right"><!----><!----><!----><!----></div></div><div data-v-5da9c21a="" class="tmx__legend"><span data-v-5da9c21a="" class="dot safety"></span><span data-v-5da9c21a="">Safety</span><span data-v-5da9c21a="" class="separator">•</span><span data-v-5da9c21a="" class="dot trust"></span><span data-v-5da9c21a="">Trustworthiness</span></div><div data-v-5da9c21a="" class="tmx__matrixWrap"><div data-v-5da9c21a="" class="tmx__matrix full"><div data-v-5da9c21a="" class="corner"></div><div data-v-5da9c21a="" class="colhead safety"><span data-v-5da9c21a="" class="colhead__text" title="Safety - Abuse Prevention">Abuse Prevention</span></div><div data-v-5da9c21a="" class="colhead safety"><span data-v-5da9c21a="" class="colhead__text" title="Safety - Value Alignment">Value Alignment</span></div><div data-v-5da9c21a="" class="colhead safety"><span data-v-5da9c21a="" class="colhead__text" title="Safety - Attack Resistance">Attack Resistance</span></div><div data-v-5da9c21a="" class="colhead safety"><span data-v-5da9c21a="" class="colhead__text" title="Safety - Privacy Protection">Privacy Protection</span></div><div data-v-5da9c21a="" class="colhead safety"><span data-v-5da9c21a="" class="colhead__text" title="Safety - Identifiability">Identifiability</span></div><div data-v-5da9c21a="" class="colhead trust"><span data-v-5da9c21a="" class="colhead__text" title="Trustworthiness - Explainability">Explainability</span></div><div data-v-5da9c21a="" class="colhead trust"><span data-v-5da9c21a="" class="colhead__text" title="Trustworthiness - Reliability">Reliability</span></div><div data-v-5da9c21a="" class="colhead trust"><span data-v-5da9c21a="" class="colhead__text" title="Trustworthiness - Controllability">Controllability</span></div><div data-v-5da9c21a="" class="colhead trust"><span data-v-5da9c21a="" class="colhead__text" title="Trustworthiness - Auditability">Auditability</span></div><div data-v-5da9c21a="" class="colhead trust"><span data-v-5da9c21a="" class="colhead__text" title="Trustworthiness - Accuracy">Accuracy</span></div><div data-v-5da9c21a="" class="rowhead"><span data-v-5da9c21a="" class="rowhead__text">Instruction Understanding</span></div><button data-v-5da9c21a="" class="cell safety" title="Instruction Understanding × Safety - Abuse Prevention — 1 paper"><span data-v-5da9c21a="" class="count">1</span></button><button data-v-5da9c21a="" class="cell safety" title="Instruction Understanding × Safety - Value Alignment — 2 papers"><span data-v-5da9c21a="" class="count">2</span></button><button data-v-5da9c21a="" class="cell safety" title="Instruction Understanding × Safety - Attack Resistance — 11 papers"><span data-v-5da9c21a="" class="count">11</span></button><button data-v-5da9c21a="" class="cell safety" title="Instruction Understanding × Safety - Privacy Protection — 1 paper"><span data-v-5da9c21a="" class="count">1</span></button><button data-v-5da9c21a="" class="cell safety" title="Instruction Understanding × Safety - Identifiability — 0 papers"><span data-v-5da9c21a="" class="count">0</span></button><button data-v-5da9c21a="" class="cell trust" title="Instruction Understanding × Trustworthiness - Explainability — 0 papers"><span data-v-5da9c21a="" class="count">0</span></button><button data-v-5da9c21a="" class="cell trust" title="Instruction Understanding × Trustworthiness - Reliability — 4 papers"><span data-v-5da9c21a="" class="count">4</span></button><button data-v-5da9c21a="" class="cell trust" title="Instruction Understanding × Trustworthiness - Controllability — 1 paper"><span data-v-5da9c21a="" class="count">1</span></button><button data-v-5da9c21a="" class="cell trust" title="Instruction Understanding × Trustworthiness - Auditability — 0 papers"><span data-v-5da9c21a="" class="count">0</span></button><button data-v-5da9c21a="" class="cell trust" title="Instruction Understanding × Trustworthiness - Accuracy — 12 papers"><span data-v-5da9c21a="" class="count">12</span></button><div data-v-5da9c21a="" class="rowhead"><span data-v-5da9c21a="" class="rowhead__text">Environment Perception</span></div><button data-v-5da9c21a="" class="cell safety" title="Environment Perception × Safety - Abuse Prevention — 9 papers"><span data-v-5da9c21a="" class="count">9</span></button><button data-v-5da9c21a="" class="cell safety" title="Environment Perception × Safety - Value Alignment — 4 papers"><span data-v-5da9c21a="" class="count">4</span></button><button data-v-5da9c21a="" class="cell safety" title="Environment Perception × Safety - Attack Resistance — 9 papers"><span data-v-5da9c21a="" class="count">9</span></button><button data-v-5da9c21a="" class="cell safety" title="Environment Perception × Safety - Privacy Protection — 6 papers"><span data-v-5da9c21a="" class="count">6</span></button><button data-v-5da9c21a="" class="cell safety" title="Environment Perception × Safety - Identifiability — 0 papers"><span data-v-5da9c21a="" class="count">0</span></button><button data-v-5da9c21a="" class="cell trust" title="Environment Perception × Trustworthiness - Explainability — 7 papers"><span data-v-5da9c21a="" class="count">7</span></button><button data-v-5da9c21a="" class="cell trust" title="Environment Perception × Trustworthiness - Reliability — 8 papers"><span data-v-5da9c21a="" class="count">8</span></button><button data-v-5da9c21a="" class="cell trust" title="Environment Perception × Trustworthiness - Controllability — 0 papers"><span data-v-5da9c21a="" class="count">0</span></button><button data-v-5da9c21a="" class="cell trust" title="Environment Perception × Trustworthiness - Auditability — 3 papers"><span data-v-5da9c21a="" class="count">3</span></button><button data-v-5da9c21a="" class="cell trust" title="Environment Perception × Trustworthiness - Accuracy — 25 papers"><span data-v-5da9c21a="" class="count">25</span></button><div data-v-5da9c21a="" class="rowhead"><span data-v-5da9c21a="" class="rowhead__text">Physical Interaction</span></div><button data-v-5da9c21a="" class="cell safety" title="Physical Interaction × Safety - Abuse Prevention — 2 papers"><span data-v-5da9c21a="" class="count">2</span></button><button data-v-5da9c21a="" class="cell safety" title="Physical Interaction × Safety - Value Alignment — 3 papers"><span data-v-5da9c21a="" class="count">3</span></button><button data-v-5da9c21a="" class="cell safety" title="Physical Interaction × Safety - Attack Resistance — 3 papers"><span data-v-5da9c21a="" class="count">3</span></button><button data-v-5da9c21a="" class="cell safety" title="Physical Interaction × Safety - Privacy Protection — 3 papers"><span data-v-5da9c21a="" class="count">3</span></button><button data-v-5da9c21a="" class="cell safety" title="Physical Interaction × Safety - Identifiability — 0 papers"><span data-v-5da9c21a="" class="count">0</span></button><button data-v-5da9c21a="" class="cell trust" title="Physical Interaction × Trustworthiness - Explainability — 0 papers"><span data-v-5da9c21a="" class="count">0</span></button><button data-v-5da9c21a="" class="cell trust" title="Physical Interaction × Trustworthiness - Reliability — 16 papers"><span data-v-5da9c21a="" class="count">16</span></button><button data-v-5da9c21a="" class="cell trust" title="Physical Interaction × Trustworthiness - Controllability — 2 papers"><span data-v-5da9c21a="" class="count">2</span></button><button data-v-5da9c21a="" class="cell trust" title="Physical Interaction × Trustworthiness - Auditability — 0 papers"><span data-v-5da9c21a="" class="count">0</span></button><button data-v-5da9c21a="" class="cell trust" title="Physical Interaction × Trustworthiness - Accuracy — 0 papers"><span data-v-5da9c21a="" class="count">0</span></button><div data-v-5da9c21a="" class="rowhead"><span data-v-5da9c21a="" class="rowhead__text">Action Planning</span></div><button data-v-5da9c21a="" class="cell safety" title="Action Planning × Safety - Abuse Prevention — 0 papers"><span data-v-5da9c21a="" class="count">0</span></button><button data-v-5da9c21a="" class="cell safety" title="Action Planning × Safety - Value Alignment — 4 papers"><span data-v-5da9c21a="" class="count">4</span></button><button data-v-5da9c21a="" class="cell safety" title="Action Planning × Safety - Attack Resistance — 4 papers"><span data-v-5da9c21a="" class="count">4</span></button><button data-v-5da9c21a="" class="cell safety" title="Action Planning × Safety - Privacy Protection — 2 papers"><span data-v-5da9c21a="" class="count">2</span></button><button data-v-5da9c21a="" class="cell safety" title="Action Planning × Safety - Identifiability — 0 papers"><span data-v-5da9c21a="" class="count">0</span></button><button data-v-5da9c21a="" class="cell trust" title="Action Planning × Trustworthiness - Explainability — 1 paper"><span data-v-5da9c21a="" class="count">1</span></button><button data-v-5da9c21a="" class="cell trust" title="Action Planning × Trustworthiness - Reliability — 5 papers"><span data-v-5da9c21a="" class="count">5</span></button><button data-v-5da9c21a="" class="cell trust" title="Action Planning × Trustworthiness - Controllability — 6 papers"><span data-v-5da9c21a="" class="count">6</span></button><button data-v-5da9c21a="" class="cell trust" title="Action Planning × Trustworthiness - Auditability — 2 papers"><span data-v-5da9c21a="" class="count">2</span></button><button data-v-5da9c21a="" class="cell trust" title="Action Planning × Trustworthiness - Accuracy — 15 papers"><span data-v-5da9c21a="" class="count">15</span></button></div></div><!----><div data-v-5da9c21a="" class="tmx__summary-row"><div data-v-5da9c21a="" class="tmx__summary"><strong data-v-5da9c21a="">219</strong> paper(s) match current filters</div><div data-v-5da9c21a="" class="tmx__summary-right"><button data-v-0172fc6e="" data-v-5da9c21a-s="" class="tmx-toggle">▲ HIDE PAPERS</button></div></div></section><!----></div></div><div data-v-0172fc6e="" class="content-frame"><div data-v-0172fc6e="" class="v-row justify-center"><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/1912.03628" target="_blank" rel="noopener" title="6-DOF Grasping for Target-driven Object Manipulation in Clutter">6-DOF Grasping for Target-driven Object Manipulation in Clutter</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Adithyavairavan Murali" aria-label="First author: Adithyavairavan Murali">Adithyavairavan Murali</span><span data-v-72037286="" class="date-pill" title="2019.12">2019.12</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/1912.03628" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2504.14650" target="_blank" rel="noopener" title="A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents">A Framework for Benchmarking and Aligning Task-Planning Safety in LLM-Based Embodied Agents</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Yuting Huang" aria-label="First author: Yuting Huang">Yuting Huang</span><span data-v-72037286="" class="date-pill" title="2025.04">2025.04</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2504.14650" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2409.17004" target="_blank" rel="noopener" title="A Model-Agnostic Approach for Semantically Driven Disambiguation in Human-Robot Interaction">A Model-Agnostic Approach for Semantically Driven Disambiguation in Human-Robot Interaction</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Fethiye Irmak Dogan" aria-label="First author: Fethiye Irmak Dogan">Fethiye Irmak Dogan</span><span data-v-72037286="" class="date-pill" title="2025.04">2025.04</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2409.17004" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2017.00075/full" target="_blank" rel="noopener" title="A Review of Future and Ethical Perspectives of Robotics and AI">A Review of Future and Ethical Perspectives of Robotics and AI</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Safety - Value Alignment">...</span><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Privacy Protection">Safety - Privacy Protection</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Jim Torresen" aria-label="First author: Jim Torresen">Jim Torresen</span><span data-v-72037286="" class="date-pill" title="2018.01">2018.01</span><a data-v-72037286="" class="link-icon" href="https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2017.00075/full" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ieeexplore.ieee.org/document/10144090" target="_blank" rel="noopener" title="A Secure Robot Learning Framework for Cyber Attack Scheduling and Countermeasure">A Secure Robot Learning Framework for Cyber Attack Scheduling and Countermeasure</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Abuse Prevention">Safety - Abuse Prevention</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Chengwei Wu" aria-label="First author: Chengwei Wu">Chengwei Wu</span><span data-v-72037286="" class="date-pill" title="2023.06">2023.06</span><a data-v-72037286="" class="link-icon" href="https://ieeexplore.ieee.org/document/10144090" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2408.11537" target="_blank" rel="noopener" title="A Survey of Embodied Learning for Object-Centric Robotic Manipulation ">A Survey of Embodied Learning for Object-Centric Robotic Manipulation </a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><!----><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2408.11537" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2505.01458" target="_blank" rel="noopener" title="A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI">A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><!----><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2505.01458" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2505.17342" target="_blank" rel="noopener" title="A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety">A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><!----><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2505.17342" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2411.13778v1" target="_blank" rel="noopener" title="A Survey on Adversarial Robustness of LiDAR-based Machine Learning Perception in Autonomous Vehicles">A Survey on Adversarial Robustness of LiDAR-based Machine Learning Perception in Autonomous Vehicles</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Safety - Abuse Prevention">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Junae Kim" aria-label="First author: Junae Kim">Junae Kim</span><span data-v-72037286="" class="date-pill" title="2024.11">2024.11</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2411.13778v1" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/1908.09635" target="_blank" rel="noopener" title="A Survey on Bias and Fairness in Machine Learning">A Survey on Bias and Fairness in Machine Learning</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><!----><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/1908.09635" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2506.14697" target="_blank" rel="noopener" title="AGENTSAFE: Benchmarking the Safety of Embodied Agents on Hazardous Instructions">AGENTSAFE: Benchmarking the Safety of Embodied Agents on Hazardous Instructions</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Aishan Liu" aria-label="First author: Aishan Liu">Aishan Liu</span><span data-v-72037286="" class="date-pill" title="2025.06">2025.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2506.14697" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ieeexplore.ieee.org/abstract/document/11037221" target="_blank" rel="noopener" title="Active SLAM With Dynamic Viewpoint Optimization for Robust Visual Navigation">Active SLAM With Dynamic Viewpoint Optimization for Robust Visual Navigation</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Trustworthiness - Reliability">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Peng Li" aria-label="First author: Peng Li">Peng Li</span><span data-v-72037286="" class="date-pill" title="2025.06">2025.06</span><a data-v-72037286="" class="link-icon" href="https://ieeexplore.ieee.org/abstract/document/11037221" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2507.09857" target="_blank" rel="noopener" title="AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective">AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Xiaofei Wang" aria-label="First author: Xiaofei Wang">Xiaofei Wang</span><span data-v-72037286="" class="date-pill" title="2025.07">2025.07</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2507.09857" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2504.15699" target="_blank" rel="noopener" title="Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation">Advancing Embodied Agent Security: From Safety Benchmarks to Input Moderation</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Ning Wang" aria-label="First author: Ning Wang">Ning Wang</span><span data-v-72037286="" class="date-pill" title="2025.04">2025.04</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2504.15699" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2506.15988" target="_blank" rel="noopener" title="Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation">Adversarial Attacks and Detection in Visual Place Recognition for Safer Robot Navigation</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Safety - Abuse Prevention">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Connor Malone" aria-label="First author: Connor Malone">Connor Malone</span><span data-v-72037286="" class="date-pill" title="2025.01">2025.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2506.15988" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2506.03350" target="_blank" rel="noopener" title="Adversarial Attacks on Robotic Vision Language Action Models">Adversarial Attacks on Robotic Vision Language Action Models</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Eliot Krzysztof Jones" aria-label="First author: Eliot Krzysztof Jones">Eliot Krzysztof Jones</span><span data-v-72037286="" class="date-pill" title="2025.06">2025.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2506.03350" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2503.04833" target="_blank" rel="noopener" title="Adversarial Training for Multimodal Large Language Models against Jailbreak Attacks">Adversarial Training for Multimodal Large Language Models against Jailbreak Attacks</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Liming Lu" aria-label="First author: Liming Lu">Liming Lu</span><span data-v-72037286="" class="date-pill" title="2025.03">2025.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2503.04833" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2505.23450" target="_blank" rel="noopener" title="Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents">Agentic Robot: A Brain-Inspired Framework for Vision-Language-Action Models in Embodied Agents</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Zhejian Yang" aria-label="First author: Zhejian Yang">Zhejian Yang</span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2505.23450" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2503.06669" target="_blank" rel="noopener" title="AgiBot World Colosseo: Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems">AgiBot World Colosseo: Large-scale Manipulation Platform for Scalable and Intelligent Embodied Systems</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author:  Team AgiBot-World" aria-label="First author:  Team AgiBot-World"> Team AgiBot-World</span><span data-v-72037286="" class="date-pill" title="2025.03">2025.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2503.06669" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://philpapers.org/archive/CANAEA-5.pdf" target="_blank" rel="noopener" title="An Enactive Approach to Value Alignment in Artificial Intelligence: A Matter of Relevance">An Enactive Approach to Value Alignment in Artificial Intelligence: A Matter of Relevance</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Value Alignment">Safety - Value Alignment</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2025.11">2025.11</span><a data-v-72037286="" class="link-icon" href="https://philpapers.org/archive/CANAEA-5.pdf" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2211.09960" target="_blank" rel="noopener" title="Ask4Help: Learning to Leverage an Expert for Embodied Tasks">Ask4Help: Learning to Leverage an Expert for Embodied Tasks</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Kunal Pratap Singh " aria-label="First author: Kunal Pratap Singh ">Kunal Pratap Singh </span><span data-v-72037286="" class="date-pill" title="2022.11">2022.11</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2211.09960" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2406.14243" target="_blank" rel="noopener" title="AuditMAI: Towards An Infrastructure for Continuous AI Auditing">AuditMAI: Towards An Infrastructure for Continuous AI Auditing</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Auditability">Trustworthiness - Auditability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2024.06">2024.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2406.14243" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2505.16154" target="_blank" rel="noopener" title="BadDepth: Backdoor Attacks Against Monocular Depth Estimation in the Physical World">BadDepth: Backdoor Attacks Against Monocular Depth Estimation in the Physical World</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Safety - Abuse Prevention">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Ji Guo" aria-label="First author: Ji Guo">Ji Guo</span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2505.16154" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2505.12443" target="_blank" rel="noopener" title="BadNAVer: Exploring Jailbreak Attacks On Vision-and-Language Navigation">BadNAVer: Exploring Jailbreak Attacks On Vision-and-Language Navigation</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Wenqi Lyu" aria-label="First author: Wenqi Lyu">Wenqi Lyu</span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2505.12443" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2407.20242" target="_blank" rel="noopener" title="BadRobot: Jailbreaking Embodied LLMs in the Physical World">BadRobot: Jailbreaking Embodied LLMs in the Physical World</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Instruction Understanding">...</span><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Hangtao Zhang" aria-label="First author: Hangtao Zhang">Hangtao Zhang</span><span data-v-72037286="" class="date-pill" title="2024.07">2024.07</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2407.20242" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2505.16640" target="_blank" rel="noopener" title="BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization">BadVLA: Towards Backdoor Attacks on Vision-Language-Action Models via Objective-Decoupled Optimization</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Xueyang Zhou" aria-label="First author: Xueyang Zhou">Xueyang Zhou</span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2505.16640" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/html/2411.08469v1" target="_blank" rel="noopener" title="Building Trustworthy AI: Transparent AI Systems via Language Models, Ontologies, and Logical Reasoning (TranspNet)">Building Trustworthy AI: Transparent AI Systems via Language Models, Ontologies, and Logical Reasoning (TranspNet)</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Fadi Al Machot" aria-label="First author: Fadi Al Machot">Fadi Al Machot</span><span data-v-72037286="" class="date-pill" title="2024.12">2024.12</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/html/2411.08469v1" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2405.20774" target="_blank" rel="noopener" title="CAN WE TRUST EMBODIED AGENTS? EXPLORING BACKDOOR ATTACKS AGAINST EMBODIED LLM-BASED DECISION-MAKING SYSTEMS">CAN WE TRUST EMBODIED AGENTS? EXPLORING BACKDOOR ATTACKS AGAINST EMBODIED LLM-BASED DECISION-MAKING SYSTEMS</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Ruochen Jiao; Shaoyuan Xie" aria-label="First author: Ruochen Jiao; Shaoyuan Xie">Ruochen Jiao; Shaoyuan Xie</span><span data-v-72037286="" class="date-pill" title="2024.05">2024.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2405.20774" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2506.17629" target="_blank" rel="noopener" title="CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning ">CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning </a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Kailing Li" aria-label="First author: Kailing Li">Kailing Li</span><span data-v-72037286="" class="date-pill" title="2025.06">2025.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2506.17629" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://kclpure.kcl.ac.uk/ws/portalfiles/portal/248844190/icra2024_motion_planning_attacks.pdf" target="_blank" rel="noopener" title="Characterizing Physical Adversarial Attacks on Robot Motion Planners">Characterizing Physical Adversarial Attacks on Robot Motion Planners</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Wenxi Wu" aria-label="First author: Wenxi Wu">Wenxi Wu</span><span data-v-72037286="" class="date-pill" title="2024.01">2024.01</span><a data-v-72037286="" class="link-icon" href="https://kclpure.kcl.ac.uk/ws/portalfiles/portal/248844190/icra2024_motion_planning_attacks.pdf" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2504.13201" target="_blank" rel="noopener" title="Concept Enhancement Engineering: A Lightweight and Efficient Robust Defense Against Jailbreak Attacks in Embodied AI">Concept Enhancement Engineering: A Lightweight and Efficient Robust Defense Against Jailbreak Attacks in Embodied AI</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Jirui Yang; Zheyu Lin" aria-label="First author: Jirui Yang; Zheyu Lin">Jirui Yang; Zheyu Lin</span><span data-v-72037286="" class="date-pill" title="2025.04">2025.04</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2504.13201" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2103.14127" target="_blank" rel="noopener" title="Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes">Contact-GraspNet: Efficient 6-DoF Grasp Generation in Cluttered Scenes</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Martin Sundermeyer" aria-label="First author: Martin Sundermeyer">Martin Sundermeyer</span><span data-v-72037286="" class="date-pill" title="2021.05">2021.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2103.14127" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/0901.3764" target="_blank" rel="noopener" title="Controllability, Observability, Realizability, and Stability of Dynamic Linear Systems">Controllability, Observability, Realizability, and Stability of Dynamic Linear Systems</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Controllability">Trustworthiness - Controllability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: John M. Davis" aria-label="First author: John M. Davis">John M. Davis</span><span data-v-72037286="" class="date-pill" title="2009.01">2009.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/0901.3764" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2307.00165" target="_blank" rel="noopener" title="Counterfactual collaborative reasoning">Counterfactual collaborative reasoning</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Jianchao Ji" aria-label="First author: Jianchao Ji">Jianchao Ji</span><span data-v-72037286="" class="date-pill" title="2023.06">2023.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2307.00165" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://link.springer.com/chapter/10.1007/978-3-031-68256-8_2" target="_blank" rel="noopener" title="Demystifying Embodied AI ">Demystifying Embodied AI </a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Sunil Sable" aria-label="First author: Sunil Sable">Sunil Sable</span><span data-v-72037286="" class="date-pill" title="2025.01">2025.01</span><a data-v-72037286="" class="link-icon" href="https://link.springer.com/chapter/10.1007/978-3-031-68256-8_2" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2006.11239" target="_blank" rel="noopener" title="Denoising diffusion probabilistic models">Denoising diffusion probabilistic models</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Jonathan Ho" aria-label="First author: Jonathan Ho">Jonathan Ho</span><span data-v-72037286="" class="date-pill" title="2020.06">2020.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2006.11239" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2110.14217" target="_blank" rel="noopener" title="Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects">Dex-NeRF: Using a Neural Radiance Field to Grasp Transparent Objects</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Jeffrey Ichnowski" aria-label="First author: Jeffrey Ichnowski">Jeffrey Ichnowski</span><span data-v-72037286="" class="date-pill" title="2021.01">2021.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2110.14217" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2202.13330" target="_blank" rel="noopener" title="DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following">DialFRED: Dialogue-Enabled Agents for Embodied Instruction Following</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Xiaofeng Gao" aria-label="First author: Xiaofeng Gao">Xiaofeng Gao</span><span data-v-72037286="" class="date-pill" title="2022.07">2022.07</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2202.13330" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2504.08438" target="_blank" rel="noopener" title=" Diffusion Models for Robotic Manipulation: A Survey"> Diffusion Models for Robotic Manipulation: A Survey</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><!----><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2504.08438" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2303.04137" target="_blank" rel="noopener" title="Diffusion Policy: Visuomotor Policy Learning via Action Diffusion">Diffusion Policy: Visuomotor Policy Learning via Action Diffusion</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Cheng Chi" aria-label="First author: Cheng Chi">Cheng Chi</span><span data-v-72037286="" class="date-pill" title="2023.03">2023.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2303.04137" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.frontiersin.org/journals/medicine/articles/10.3389/fmed.2024.1437280/full" target="_blank" rel="noopener" title="Disability 4.0: bioethical considerations on the use of embodied artificial intelligence">Disability 4.0: bioethical considerations on the use of embodied artificial intelligence</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Value Alignment">Safety - Value Alignment</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Francesco De Micco" aria-label="First author: Francesco De Micco">Francesco De Micco</span><span data-v-72037286="" class="date-pill" title="2024.08">2024.08</span><a data-v-72037286="" class="link-icon" href="https://www.frontiersin.org/journals/medicine/articles/10.3389/fmed.2024.1437280/full" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.frontiersin.org/journals/medicine/articles/10.3389/fmed.2024.1437280/full" target="_blank" rel="noopener" title="Disability 4.0: bioethical considerations on the use of embodied artificial intelligence">Disability 4.0: bioethical considerations on the use of embodied artificial intelligence</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Privacy Protection">Safety - Privacy Protection</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2024.08">2024.08</span><a data-v-72037286="" class="link-icon" href="https://www.frontiersin.org/journals/medicine/articles/10.3389/fmed.2024.1437280/full" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ieeexplore.ieee.org/abstract/document/9846930" target="_blank" rel="noopener" title="DoRO: Disambiguation of Referred Object for Embodied Agents">DoRO: Disambiguation of Referred Object for Embodied Agents</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Pradip Pramanick" aria-label="First author: Pradip Pramanick">Pradip Pramanick</span><span data-v-72037286="" class="date-pill" title="2022.01">2022.01</span><a data-v-72037286="" class="link-icon" href="https://ieeexplore.ieee.org/abstract/document/9846930" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2411.04999" target="_blank" rel="noopener" title="DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation">DynaMem: Online Dynamic Spatio-Semantic Memory for Open World Mobile Manipulation</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Peiqi Liu" aria-label="First author: Peiqi Liu">Peiqi Liu</span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2411.04999" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2409.03256" target="_blank" rel="noopener" title="E2CL: exploration-based error correction learning for embodied agents">E2CL: exploration-based error correction learning for embodied agents</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Explainability">Trustworthiness - Explainability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2024.09">2024.09</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2409.03256" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2406.09239" target="_blank" rel="noopener" title="EHAZOP: A Proof of Concept Ethical Hazard Analysis of an Assistive Robot">EHAZOP: A Proof of Concept Ethical Hazard Analysis of an Assistive Robot</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Controllability">Trustworthiness - Controllability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2024.06">2024.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2406.09239" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2408.04449" target="_blank" rel="noopener" title="Earbench: Towards evaluating physical risk awareness for task planning of foundation model-based embodied ai agents">Earbench: Towards evaluating physical risk awareness for task planning of foundation model-based embodied ai agents</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Zihao Zhu" aria-label="First author: Zihao Zhu">Zihao Zhu</span><span data-v-72037286="" class="date-pill" title="2024.08">2024.08</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2408.04449" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2506.24019" target="_blank" rel="noopener" title="Ella: Embodied Social Agents with Lifelong Memory">Ella: Embodied Social Agents with Lifelong Memory</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Hongxin Zhang" aria-label="First author: Hongxin Zhang">Hongxin Zhang</span><span data-v-72037286="" class="date-pill" title="2025.06">2025.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2506.24019" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ieeexplore.ieee.org/abstract/document/10802181" target="_blank" rel="noopener" title="Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity">Embodied AI with Two Arms: Zero-shot Learning, Safety and Modularity</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><!----><a data-v-72037286="" class="link-icon" href="https://ieeexplore.ieee.org/abstract/document/10802181" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2404.00540" target="_blank" rel="noopener" title="Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches">Embodied Active Defense: Leveraging Recurrent Feedback to Counter Adversarial Patches</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Safety - Abuse Prevention">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2024.03">2024.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2404.00540" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2410.07166" target="_blank" rel="noopener" title="Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making">Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Manling Li,Shiyu Zhao,Qineng Wang,Kangrui Wang,Yu Zhou" aria-label="First author: Manling Li,Shiyu Zhao,Qineng Wang,Kangrui Wang,Yu Zhou">Manling Li,Shiyu Zhao,Qineng Wang,Kangrui Wang,Yu Zhou</span><span data-v-72037286="" class="date-pill" title="2024.01">2024.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2410.07166" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2503.03208" target="_blank" rel="noopener" title="Embodied Escaping: End-to-End Reinforcement Learning for Robot Navigation in Narrow Environment">Embodied Escaping: End-to-End Reinforcement Learning for Robot Navigation in Narrow Environment</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Han Zheng" aria-label="First author: Han Zheng">Han Zheng</span><span data-v-72037286="" class="date-pill" title="2025.03">2025.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2503.03208" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.arxiv.org/pdf/2406.11818" target="_blank" rel="noopener" title="Embodied Instruction Following in Unknown Environments">Embodied Instruction Following in Unknown Environments</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2025.07">2025.07</span><a data-v-72037286="" class="link-icon" href="https://www.arxiv.org/pdf/2406.11818" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2312.09554" target="_blank" rel="noopener" title="Embodied Laser Attack:Leveraging Scene Priors to Achieve Agent-based Robust Non-contact Attacks">Embodied Laser Attack:Leveraging Scene Priors to Achieve Agent-based Robust Non-contact Attacks</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Safety - Abuse Prevention">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2024.07">2024.07</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2312.09554" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://web.archive.org/web/20220704170254id_/http://www.roboticsproceedings.org/rss18/p032.pdf" target="_blank" rel="noopener" title="Embodied Multi-Agent Task Planning from Ambiguous Instruction">Embodied Multi-Agent Task Planning from Ambiguous Instruction</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Xinzhu Liu" aria-label="First author: Xinzhu Liu">Xinzhu Liu</span><span data-v-72037286="" class="date-pill" title="2022.06">2022.06</span><a data-v-72037286="" class="link-icon" href="https://web.archive.org/web/20220704170254id_/http://www.roboticsproceedings.org/rss18/p032.pdf" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ieeexplore.ieee.org/abstract/document/10801562" target="_blank" rel="noopener" title="Embodied Uncertainty-Aware Object Segmentation">Embodied Uncertainty-Aware Object Segmentation</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: fang2024embodied" aria-label="First author: fang2024embodied">fang2024embodied</span><span data-v-72037286="" class="date-pill" title="2024.01">2024.01</span><a data-v-72037286="" class="link-icon" href="https://ieeexplore.ieee.org/abstract/document/10801562" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ieeexplore.ieee.org/abstract/document/9816133" target="_blank" rel="noopener" title="Embodied active domain adaptation for semantic segmentation via informative path planning">Embodied active domain adaptation for semantic segmentation via informative path planning</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: René Zurbrügg" aria-label="First author: René Zurbrügg">René Zurbrügg</span><span data-v-72037286="" class="date-pill" title="2022.01">2022.01</span><a data-v-72037286="" class="link-icon" href="https://ieeexplore.ieee.org/abstract/document/9816133" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.nature.com/articles/s41746-025-01754-4.pdf" target="_blank" rel="noopener" title="Embodied artificial intelligence in ophthalmology">Embodied artificial intelligence in ophthalmology</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Yao Qiu" aria-label="First author: Yao Qiu">Yao Qiu</span><span data-v-72037286="" class="date-pill" title="2025.06">2025.06</span><a data-v-72037286="" class="link-icon" href="https://www.nature.com/articles/s41746-025-01754-4.pdf" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2501.00358" target="_blank" rel="noopener" title="Embodied videoagent: Persistent memory from egocentric videos and embodied sensors enables dynamic scene understanding">Embodied videoagent: Persistent memory from egocentric videos and embodied sensors enables dynamic scene understanding</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Explainability">Trustworthiness - Explainability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2025.01">2025.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2501.00358" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ojs.aaai.org/index.php/AAAI/article/view/16338" target="_blank" rel="noopener" title="Embodied visual active learning for semantic segmentation">Embodied visual active learning for semantic segmentation</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: David Nilsson" aria-label="First author: David Nilsson">David Nilsson</span><span data-v-72037286="" class="date-pill" title="2021.12">2021.12</span><a data-v-72037286="" class="link-icon" href="https://ojs.aaai.org/index.php/AAAI/article/view/16338" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2409.18313" target="_blank" rel="noopener" title="Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation">Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Quanting Xie" aria-label="First author: Quanting Xie">Quanting Xie</span><span data-v-72037286="" class="date-pill" title="2025.01">2025.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2409.18313" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2503.21696" target="_blank" rel="noopener" title="Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks">Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Wenqi Zhang" aria-label="First author: Wenqi Zhang">Wenqi Zhang</span><span data-v-72037286="" class="date-pill" title="2025.03">2025.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2503.21696" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/4ec43957eda1126ad4887995d05fae3b-Paper-Conference.pdf" target="_blank" rel="noopener" title="Embodiedgpt: Vision-language pre-training via embodied chain of thought">Embodiedgpt: Vision-language pre-training via embodied chain of thought</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Yao Mu" aria-label="First author: Yao Mu">Yao Mu</span><span data-v-72037286="" class="date-pill" title="2023.05">2023.05</span><a data-v-72037286="" class="link-icon" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/4ec43957eda1126ad4887995d05fae3b-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_EmbodiedScan_A_Holistic_Multi-Modal_3D_Perception_Suite_Towards_Embodied_AI_CVPR_2024_paper.html" target="_blank" rel="noopener" title="Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai">Embodiedscan: A holistic multi-modal 3d perception suite towards embodied ai</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Tai Wang" aria-label="First author: Tai Wang">Tai Wang</span><span data-v-72037286="" class="date-pill" title="2024.03">2024.03</span><a data-v-72037286="" class="link-icon" href="https://openaccess.thecvf.com/content/CVPR2024/html/Wang_EmbodiedScan_A_Holistic_Multi-Modal_3D_Perception_Suite_Towards_Embodied_AI_CVPR_2024_paper.html" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://link.springer.com/chapter/10.1007/978-3-031-68256-8_13" target="_blank" rel="noopener" title="Embracing the Future: Navigating the Challenges and Solutions in Embodied Artificial Intelligence ">Embracing the Future: Navigating the Challenges and Solutions in Embodied Artificial Intelligence </a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Wasim Khan" aria-label="First author: Wasim Khan">Wasim Khan</span><span data-v-72037286="" class="date-pill" title="2025.01">2025.01</span><a data-v-72037286="" class="link-icon" href="https://link.springer.com/chapter/10.1007/978-3-031-68256-8_13" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2312.00812" target="_blank" rel="noopener" title="Empowering Autonomous Driving with Large Language Models: A Safety Perspective">Empowering Autonomous Driving with Large Language Models: A Safety Perspective</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2023.12">2023.12</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2312.00812" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2402.03721" target="_blank" rel="noopener" title="Enhancing embodied object detection through language-image pre-training and implicit object memory">Enhancing embodied object detection through language-image pre-training and implicit object memory</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Nicolas Harvey Chapman" aria-label="First author: Nicolas Harvey Chapman">Nicolas Harvey Chapman</span><span data-v-72037286="" class="date-pill" title="2024.02">2024.02</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2402.03721" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.cell.com/heliyon/fulltext/S2405-8440(24)02328-4" target="_blank" rel="noopener" title="Ethical and regulatory challenges of AI technologies in healthcare: A narrative review">Ethical and regulatory challenges of AI technologies in healthcare: A narrative review</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Ciro Mennella" aria-label="First author: Ciro Mennella">Ciro Mennella</span><span data-v-72037286="" class="date-pill" title="2024.02">2024.02</span><a data-v-72037286="" class="link-icon" href="https://www.cell.com/heliyon/fulltext/S2405-8440(24)02328-4" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://openreview.net/forum?id=rXGxbDJadh&amp;referrer=%5Bthe%20profile%20of%20Keji%20He%5D(%2Fprofile%3Fid%3D~Keji_He1)" target="_blank" rel="noopener" title="Everyday Object Meets Vision-and-Language Navigation Agent via Backdoor">Everyday Object Meets Vision-and-Language Navigation Agent via Backdoor</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Keji He" aria-label="First author: Keji He">Keji He</span><span data-v-72037286="" class="date-pill" title="2024.11">2024.11</span><a data-v-72037286="" class="link-icon" href="https://openreview.net/forum?id=rXGxbDJadh&amp;referrer=%5Bthe%20profile%20of%20Keji%20He%5D(%2Fprofile%3Fid%3D~Keji_He1)" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://dl.acm.org/doi/abs/10.1145/3457188" target="_blank" rel="noopener" title="Explainable Embodied Agents Through Social Cues: A Review">Explainable Embodied Agents Through Social Cues: A Review</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: SEBASTIAN WALLKÖTTER" aria-label="First author: SEBASTIAN WALLKÖTTER">SEBASTIAN WALLKÖTTER</span><span data-v-72037286="" class="date-pill" title="2021.07">2021.07</span><a data-v-72037286="" class="link-icon" href="https://dl.acm.org/doi/abs/10.1145/3457188" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2504.06154" target="_blank" rel="noopener" title="Exploring Adversarial Obstacle Attacks in Search-based Path Planning for Autonomous Mobile Robots">Exploring Adversarial Obstacle Attacks in Search-based Path Planning for Autonomous Mobile Robots</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Adrian Szvoren;" aria-label="First author: Adrian Szvoren;">Adrian Szvoren;</span><span data-v-72037286="" class="date-pill" title="2025.04">2025.04</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2504.06154" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2411.13587" target="_blank" rel="noopener" title="Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics">Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Taowen Wang" aria-label="First author: Taowen Wang">Taowen Wang</span><span data-v-72037286="" class="date-pill" title="2024.11">2024.11</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2411.13587" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2405.19802" target="_blank" rel="noopener" title="Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models">Exploring the Robustness of Decision-Level Through Adversarial Attacks on LLM-Based Embodied Models</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Shuyuan Liu" aria-label="First author: Shuyuan Liu">Shuyuan Liu</span><span data-v-72037286="" class="date-pill" title="2024.05">2024.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2405.19802" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2503.08950" target="_blank" rel="noopener" title="FP3: A 3D Foundation Policy for Robotic Manipulation">FP3: A 3D Foundation Policy for Robotic Manipulation</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author:  Rujia Yang, Geng Chen" aria-label="First author:  Rujia Yang, Geng Chen"> Rujia Yang, Geng Chen</span><span data-v-72037286="" class="date-pill" title="2025.03">2025.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2503.08950" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/1910.13675" target="_blank" rel="noopener" title="Form2Fit: Learning Shape Priors for Generalizable Assembly from Disassembly">Form2Fit: Learning Shape Priors for Generalizable Assembly from Disassembly</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Kevin Zakka" aria-label="First author: Kevin Zakka">Kevin Zakka</span><span data-v-72037286="" class="date-pill" title="2019.01">2019.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/1910.13675" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2405.04798" target="_blank" rel="noopener" title="From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot Control">From LLMs to Actions: Latent Codes as Bridges in Hierarchical Robot Control</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2024.05">2024.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2405.04798" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2501.07468" target="_blank" rel="noopener" title="From Screens to Scenes: A Survey of Embodied AI in Healthcare">From Screens to Scenes: A Survey of Embodied AI in Healthcare</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Auditability">Trustworthiness - Auditability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Yihao Liu" aria-label="First author: Yihao Liu">Yihao Liu</span><span data-v-72037286="" class="date-pill" title="2025.03">2025.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2501.07468" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2505.22503" target="_blank" rel="noopener" title="From Strangers to Assistants: Fast Desire Alignment for Embodied Agent-User Adaptation">From Strangers to Assistants: Fast Desire Alignment for Embodied Agent-User Adaptation</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Value Alignment">Safety - Value Alignment</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2505.22503" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://dl.acm.org/doi/10.1145/3581783.3612351" target="_blank" rel="noopener" title="Generating Explanations for Embodied Action Decision from Visual Observation">Generating Explanations for Embodied Action Decision from Visual Observation</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Explainability">Trustworthiness - Explainability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Xiaohan Wang" aria-label="First author: Xiaohan Wang">Xiaohan Wang</span><span data-v-72037286="" class="date-pill" title="2023.01">2023.01</span><a data-v-72037286="" class="link-icon" href="https://dl.acm.org/doi/10.1145/3581783.3612351" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2304.03442" target="_blank" rel="noopener" title="Generative Agents: Interactive Simulacra of Human Behavior">Generative Agents: Interactive Simulacra of Human Behavior</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Joon Sung Park" aria-label="First author: Joon Sung Park">Joon Sung Park</span><span data-v-72037286="" class="date-pill" title="2023.08">2023.08</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2304.03442" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2206.10606" target="_blank" rel="noopener" title="Good time to ask: A learning framework for asking for help in embodied visual navigation">Good time to ask: A learning framework for asking for help in embodied visual navigation</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Explainability">Trustworthiness - Explainability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2023.06">2023.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2206.10606" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2504.17784" target="_blank" rel="noopener" title="Gripper Keypose and Object Pointflow as Interfaces for Bimanual Robotic Manipulation">Gripper Keypose and Object Pointflow as Interfaces for Bimanual Robotic Manipulation</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Yuyin Yang; Zetao Cai" aria-label="First author: Yuyin Yang; Zetao Cai">Yuyin Yang; Zetao Cai</span><span data-v-72037286="" class="date-pill" title="2025.04">2025.04</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2504.17784" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2504.00907" target="_blank" rel="noopener" title="Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning">Grounding Multimodal LLMs to Embodied Agents that Ask for Help with Reinforcement Learning</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Ram Ramrakhya" aria-label="First author: Ram Ramrakhya">Ram Ramrakhya</span><span data-v-72037286="" class="date-pill" title="2025.04">2025.04</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2504.00907" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2502.05485" target="_blank" rel="noopener" title="HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation">HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Yi Li, Yuquan Deng, Jesse Zhang" aria-label="First author: Yi Li, Yuquan Deng, Jesse Zhang">Yi Li, Yuquan Deng, Jesse Zhang</span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2502.05485" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2503.08241" target="_blank" rel="noopener" title="HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in Embodied Agents">HASARD: A Benchmark for Vision-Based Safe Reinforcement Learning in Embodied Agents</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Tristan Tomilin" aria-label="First author: Tristan Tomilin">Tristan Tomilin</span><span data-v-72037286="" class="date-pill" title="2025.03">2025.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2503.08241" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2401.12975" target="_blank" rel="noopener" title="HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments">HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><!----><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2401.12975" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2204.03514" target="_blank" rel="noopener" title="Habitat-web: Learning embodied object-search strategies from human demonstrations at scale">Habitat-web: Learning embodied object-search strategies from human demonstrations at scale</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2022.04">2022.04</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2204.03514" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2106.02523" target="_blank" rel="noopener" title="Hallucination In Object Detection -- A Study In Visual Part Verification">Hallucination In Object Detection -- A Study In Visual Part Verification</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2021.06">2021.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2106.02523" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2403.03890" target="_blank" rel="noopener" title="Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation">Hierarchical Diffusion Policy for Kinematics-Aware Multi-Task Robotic Manipulation</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Xiao Ma" aria-label="First author: Xiao Ma">Xiao Ma</span><span data-v-72037286="" class="date-pill" title="2024.03">2024.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2403.03890" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2306.11565" target="_blank" rel="noopener" title="HomeRobot: Open-Vocabulary Mobile Manipulation">HomeRobot: Open-Vocabulary Mobile Manipulation</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Sriram Yenamandra" aria-label="First author: Sriram Yenamandra">Sriram Yenamandra</span><span data-v-72037286="" class="date-pill" title="2023.06">2023.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2306.11565" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2023.1189914/full" target="_blank" rel="noopener" title="Humanizing AI in medical training: ethical framework for responsible design">Humanizing AI in medical training: ethical framework for responsible design</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Value Alignment">Safety - Value Alignment</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Mohammed Tahri Sqalli" aria-label="First author: Mohammed Tahri Sqalli">Mohammed Tahri Sqalli</span><span data-v-72037286="" class="date-pill" title="2023.05">2023.05</span><a data-v-72037286="" class="link-icon" href="https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2023.1189914/full" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.mdpi.com/2076-3417/14/24/11823" target="_blank" rel="noopener" title="Humanoid Robots in Tourism and Hospitality—Exploring Managerial, Ethical, and Societal Challenges">Humanoid Robots in Tourism and Hospitality—Exploring Managerial, Ethical, and Societal Challenges</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Safety - Value Alignment">...</span><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Privacy Protection">Safety - Privacy Protection</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Ida Skubis" aria-label="First author: Ida Skubis">Ida Skubis</span><span data-v-72037286="" class="date-pill" title="2024.12">2024.12</span><a data-v-72037286="" class="link-icon" href="https://www.mdpi.com/2076-3417/14/24/11823" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.multidisciplinaryfrontiers.com/uploads/archives/20250312183510_FMR-2025-1-004.1.pdf" target="_blank" rel="noopener" title="INTRODUCING THE Robot Security Framework (RSF), A STANDARDIZED METHODOLOGY TO PERFORM SECURITY ASSESSMENTS IN ROBOTICS">INTRODUCING THE Robot Security Framework (RSF), A STANDARDIZED METHODOLOGY TO PERFORM SECURITY ASSESSMENTS IN ROBOTICS</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Auditability">Trustworthiness - Auditability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Abiodun Sunday Adebayo" aria-label="First author: Abiodun Sunday Adebayo">Abiodun Sunday Adebayo</span><span data-v-72037286="" class="date-pill" title="2023.12">2023.12</span><a data-v-72037286="" class="link-icon" href="https://www.multidisciplinaryfrontiers.com/uploads/archives/20250312183510_FMR-2025-1-004.1.pdf" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2506.16402" target="_blank" rel="noopener" title="IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks">IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Xiaoya Lu" aria-label="First author: Xiaoya Lu">Xiaoya Lu</span><span data-v-72037286="" class="date-pill" title="2025.06">2025.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2506.16402" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2507.16034" target="_blank" rel="noopener" title="Improved Semantic Segmentation from Ultra-Low-Resolution RGB Images Applied to Privacy-Preserving Object-Goal Navigation">Improved Semantic Segmentation from Ultra-Low-Resolution RGB Images Applied to Privacy-Preserving Object-Goal Navigation</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Privacy Protection">Safety - Privacy Protection</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2025.07">2025.07</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2507.16034" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ieeexplore.ieee.org/abstract/document/8794287" target="_blank" rel="noopener" title="Improving Grounded Natural Language Understanding through Human-Robot Dialog">Improving Grounded Natural Language Understanding through Human-Robot Dialog</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Jesse Thomason" aria-label="First author: Jesse Thomason">Jesse Thomason</span><span data-v-72037286="" class="date-pill" title="2019.05">2019.05</span><a data-v-72037286="" class="link-icon" href="https://ieeexplore.ieee.org/abstract/document/8794287" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2505.20640" target="_blank" rel="noopener" title="IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios">IndustryEQA: Pushing the Frontiers of Embodied Question Answering in Industrial Scenarios</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Yifan Li" aria-label="First author: Yifan Li">Yifan Li</span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2505.20640" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2207.05608" target="_blank" rel="noopener" title="Inner Monologue: Embodied Reasoning through Planning with Language Models">Inner Monologue: Embodied Reasoning through Planning with Language Models</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2022.07">2022.07</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2207.05608" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2404.14547" target="_blank" rel="noopener" title="Integrating Disambiguation and User Preferences into Large Language Models for Robot Motion Planning">Integrating Disambiguation and User Preferences into Large Language Models for Robot Motion Planning</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Mohammed Abugurain" aria-label="First author: Mohammed Abugurain">Mohammed Abugurain</span><span data-v-72037286="" class="date-pill" title="2024.04">2024.04</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2404.14547" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://link.springer.com/article/10.1007/s10458-020-09481-8" target="_blank" rel="noopener" title="Interactive task learning via embodied corrective feedback">Interactive task learning via embodied corrective feedback</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Explainability">Trustworthiness - Explainability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2020.09">2020.09</span><a data-v-72037286="" class="link-icon" href="https://link.springer.com/article/10.1007/s10458-020-09481-8" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kotar_Interactron_Embodied_Adaptive_Object_Detection_CVPR_2022_paper.pdf" target="_blank" rel="noopener" title="Interactron: Embodied adaptive object detection">Interactron: Embodied adaptive object detection</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Klemen Kotar" aria-label="First author: Klemen Kotar">Klemen Kotar</span><span data-v-72037286="" class="date-pill" title="2022.03">2022.03</span><a data-v-72037286="" class="link-icon" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Kotar_Interactron_Embodied_Adaptive_Object_Detection_CVPR_2022_paper.pdf" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://link.springer.com/article/10.1007/s12369-024-01153-x" target="_blank" rel="noopener" title="Is the robot spying on me? a study on perceived privacy in telepresence scenarios in a care setting with mobile and humanoid robots">Is the robot spying on me? a study on perceived privacy in telepresence scenarios in a care setting with mobile and humanoid robots</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Privacy Protection">Safety - Privacy Protection</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2024.08">2024.08</span><a data-v-72037286="" class="link-icon" href="https://link.springer.com/article/10.1007/s12369-024-01153-x" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2410.13691" target="_blank" rel="noopener" title="Jailbreaking LLM-Controlled Robots">Jailbreaking LLM-Controlled Robots</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Alexander Robey" aria-label="First author: Alexander Robey">Alexander Robey</span><span data-v-72037286="" class="date-pill" title="2024.01">2024.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2410.13691" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2409.14908" target="_blank" rel="noopener" title="KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems">KARMA: Augmenting Embodied AI Agents with Long-and-short Term Memory Systems</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Zixuan Wang" aria-label="First author: Zixuan Wang">Zixuan Wang</span><span data-v-72037286="" class="date-pill" title="2024.09">2024.09</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2409.14908" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2310.12344" target="_blank" rel="noopener" title="LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following">LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2023.01">2023.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2310.12344" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2406.08824" target="_blank" rel="noopener" title="LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions">LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Value Alignment">Safety - Value Alignment</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2024.06">2024.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2406.08824" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2507.08496" target="_blank" rel="noopener" title="LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning">LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Shibo Sun" aria-label="First author: Shibo Sun">Shibo Sun</span><span data-v-72037286="" class="date-pill" title="2025.07">2025.07</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2507.08496" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ojs.aaai.org/index.php/AAAI/article/view/28281" target="_blank" rel="noopener" title="Learn how to see: collaborative embodied learning for object detection and camera adjusting">Learn how to see: collaborative embodied learning for object detection and camera adjusting</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Lingdong Shen" aria-label="First author: Lingdong Shen">Lingdong Shen</span><span data-v-72037286="" class="date-pill" title="2024.03">2024.03</span><a data-v-72037286="" class="link-icon" href="https://ojs.aaai.org/index.php/AAAI/article/view/28281" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2101.05436" target="_blank" rel="noopener" title="Learning Safe Multi-Agent Control with Decentralized Neural Barrier Certificates">Learning Safe Multi-Agent Control with Decentralized Neural Barrier Certificates</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><!----><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2101.05436" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ojs.aaai.org/index.php/AAAI/article/view/33573" target="_blank" rel="noopener" title="Learning Visually Grounded Domain Ontologies via Embodied Conversation and Explanation">Learning Visually Grounded Domain Ontologies via Embodied Conversation and Explanation</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Jonghyuk Park" aria-label="First author: Jonghyuk Park">Jonghyuk Park</span><span data-v-72037286="" class="date-pill" title="2025.04">2025.04</span><a data-v-72037286="" class="link-icon" href="https://ojs.aaai.org/index.php/AAAI/article/view/33573" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2201.08117" target="_blank" rel="noopener" title="Learning robust perceptive locomotion for quadrupedal robots in the wild">Learning robust perceptive locomotion for quadrupedal robots in the wild</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Trustworthiness - Reliability">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: TAKAHIRO MIKI" aria-label="First author: TAKAHIRO MIKI">TAKAHIRO MIKI</span><span data-v-72037286="" class="date-pill" title="2022.01">2022.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2201.08117" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2004.05155" target="_blank" rel="noopener" title="Learning to Explore using Active Neural SLAM">Learning to Explore using Active Neural SLAM</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Devendra Singh Chaplot" aria-label="First author: Devendra Singh Chaplot">Devendra Singh Chaplot</span><span data-v-72037286="" class="date-pill" title="2020.04">2020.04</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2004.05155" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2209.09233" target="_blank" rel="noopener" title="Learning to Walk by Steering: Perceptive Quadrupedal Locomotion in Dynamic Environments">Learning to Walk by Steering: Perceptive Quadrupedal Locomotion in Dynamic Environments</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Trustworthiness - Reliability">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Mingyo Seo" aria-label="First author: Mingyo Seo">Mingyo Seo</span><span data-v-72037286="" class="date-pill" title="2022.09">2022.09</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2209.09233" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.frontiersin.org/journals/surgery/articles/10.3389/fsurg.2022.862322/full?gclid=Cj0KCQi" target="_blank" rel="noopener" title="Legal and Ethical Consideration in Artificial Intelligence in Healthcare: Who Takes Responsibility?">Legal and Ethical Consideration in Artificial Intelligence in Healthcare: Who Takes Responsibility?</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Nithesh Naik" aria-label="First author: Nithesh Naik">Nithesh Naik</span><span data-v-72037286="" class="date-pill" title="2022.03">2022.03</span><a data-v-72037286="" class="link-icon" href="https://www.frontiersin.org/journals/surgery/articles/10.3389/fsurg.2022.862322/full?gclid=Cj0KCQi" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://proceedings.mlr.press/v205/agarwal23a.html" target="_blank" rel="noopener" title="Legged locomotion in challenging terrains using egocentric vision">Legged locomotion in challenging terrains using egocentric vision</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Trustworthiness - Reliability">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Ananye Agarwal" aria-label="First author: Ananye Agarwal">Ananye Agarwal</span><span data-v-72037286="" class="date-pill" title="2022.11">2022.11</span><a data-v-72037286="" class="link-icon" href="https://proceedings.mlr.press/v205/agarwal23a.html" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2311.17842" target="_blank" rel="noopener" title="Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning">Look before you leap: Unveiling the power of gpt-4v in robotic vision-language planning</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2023.11">2023.11</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2311.17842" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2311.17600" target="_blank" rel="noopener" title="MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models">MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Xin Liu" aria-label="First author: Xin Liu">Xin Liu</span><span data-v-72037286="" class="date-pill" title="2023.11">2023.11</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2311.17600" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_ManipLLM_Embodied_Multimodal_Large_Language_Model_for_Object-Centric_Robotic_Manipulation_CVPR_2024_paper.pdf" target="_blank" rel="noopener" title="ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation ">ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation </a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Xiaoqi Li" aria-label="First author: Xiaoqi Li">Xiaoqi Li</span><span data-v-72037286="" class="date-pill" title="2024.06">2024.06</span><a data-v-72037286="" class="link-icon" href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_ManipLLM_Embodied_Multimodal_Large_Language_Model_for_Object-Centric_Robotic_Manipulation_CVPR_2024_paper.pdf" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2403.18256" target="_blank" rel="noopener" title="Manipulating Neural Path Planners via Slight Perturbations">Manipulating Neural Path Planners via Slight Perturbations</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Privacy Protection">Safety - Privacy Protection</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Zikang Xiong" aria-label="First author: Zikang Xiong">Zikang Xiong</span><span data-v-72037286="" class="date-pill" title="2024.03">2024.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2403.18256" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2503.13446" target="_blank" rel="noopener" title="MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation">MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Zhenyu Wu" aria-label="First author: Zhenyu Wu">Zhenyu Wu</span><span data-v-72037286="" class="date-pill" title="2025.03">2025.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2503.13446" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2305.17537" target="_blank" rel="noopener" title="Modeling Dynamic Environments with Scene Graph Memory">Modeling Dynamic Environments with Scene Graph Memory</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Andrey Kurenkov" aria-label="First author: Andrey Kurenkov">Andrey Kurenkov</span><span data-v-72037286="" class="date-pill" title="2023.06">2023.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2305.17537" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2005.11816" target="_blank" rel="noopener" title="Monitoring and Diagnosability of Perception Systems">Monitoring and Diagnosability of Perception Systems</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Auditability">Trustworthiness - Auditability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2020.11">2020.11</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2005.11816" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.bmvc2021-virtualconference.com/assets/papers/0615.pdf" target="_blank" rel="noopener" title="Move to see better: Self-improving embodied object detection">Move to see better: Self-improving embodied object detection</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Zhaoyuan Fang" aria-label="First author: Zhaoyuan Fang">Zhaoyuan Fang</span><span data-v-72037286="" class="date-pill" title="2020.12">2020.12</span><a data-v-72037286="" class="link-icon" href="https://www.bmvc2021-virtualconference.com/assets/papers/0615.pdf" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2505.11191" target="_blank" rel="noopener" title="Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration">Multi-Modal Multi-Task (M3T) Federated Foundation Models for Embodied AI: Potentials and Challenges for Edge Integration</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Privacy Protection">Safety - Privacy Protection</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Kasra Borazjani" aria-label="First author: Kasra Borazjani">Kasra Borazjani</span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2505.11191" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2505.05108?" target="_blank" rel="noopener" title="Multi-agent Embodied AI: Advances and Future Directions">Multi-agent Embodied AI: Advances and Future Directions</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Zhaohan Feng" aria-label="First author: Zhaohan Feng">Zhaohan Feng</span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2505.05108?" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.arxiv.org/abs/2508.01235" target="_blank" rel="noopener" title="NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place Exploration">NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place Exploration</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2025.08">2025.08</span><a data-v-72037286="" class="link-icon" href="https://www.arxiv.org/abs/2508.01235" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2211.14769" target="_blank" rel="noopener" title="Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning">Navigation as Attackers Wish? Towards Building Robust Embodied Agents under Federated Learning</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Yunchao Zhang" aria-label="First author: Yunchao Zhang">Yunchao Zhang</span><span data-v-72037286="" class="date-pill" title="2022.11">2022.11</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2211.14769" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2412.10726" target="_blank" rel="noopener" title="NoisyEQA: Benchmarking Embodied Question Answering Against Noisy Queries">NoisyEQA: Benchmarking Embodied Question Answering Against Noisy Queries</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Tao Wu" aria-label="First author: Tao Wu">Tao Wu</span><span data-v-72037286="" class="date-pill" title="2024.12">2024.12</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2412.10726" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2409.19709" target="_blank" rel="noopener" title="Obstacle-Aware Quadrupedal Locomotion With Resilient Multi-Modal Reinforcement Learning">Obstacle-Aware Quadrupedal Locomotion With Resilient Multi-Modal Reinforcement Learning</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Trustworthiness - Reliability">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: I Made Aswin Nahrendra" aria-label="First author: I Made Aswin Nahrendra">I Made Aswin Nahrendra</span><span data-v-72037286="" class="date-pill" title="2024.09">2024.09</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2409.19709" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2002.05630" target="_blank" rel="noopener" title="On the Sensory Commutativity of Action Sequences for Embodied Agents">On the Sensory Commutativity of Action Sequences for Embodied Agents</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Value Alignment">Safety - Value Alignment</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2021.01">2021.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2002.05630" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.sciencedirect.com/science/article/pii/S1474667017700948" target="_blank" rel="noopener" title="On the general theory of control systems">On the general theory of control systems</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Controllability">Trustworthiness - Controllability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="1959.12">1959.12</span><a data-v-72037286="" class="link-icon" href="https://www.sciencedirect.com/science/article/pii/S1474667017700948" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2310.15127" target="_blank" rel="noopener" title="Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models">Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2023.01">2023.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2310.15127" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2406.09246" target="_blank" rel="noopener" title="Openvla: An open-source vision-language-action model">Openvla: An open-source vision-language-action model</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Moo Jin Kim" aria-label="First author: Moo Jin Kim">Moo Jin Kim</span><span data-v-72037286="" class="date-pill" title="2024.06">2024.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2406.09246" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2502.07839" target="_blank" rel="noopener" title="Optimal Actuator Attacks on Autonomous Vehicles Using Reinforcement Learning ">Optimal Actuator Attacks on Autonomous Vehicles Using Reinforcement Learning </a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Abuse Prevention">Safety - Abuse Prevention</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Pengyu Wang" aria-label="First author: Pengyu Wang">Pengyu Wang</span><span data-v-72037286="" class="date-pill" title="2025.02">2025.02</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2502.07839" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2412.16633" target="_blank" rel="noopener" title="POEX: Understanding and Mitigating Policy Executable Jailbreak Attacks against Embodied AI">POEX: Understanding and Mitigating Policy Executable Jailbreak Attacks against Embodied AI</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Xuancun Lu" aria-label="First author: Xuancun Lu">Xuancun Lu</span><span data-v-72037286="" class="date-pill" title="2024.12">2024.12</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2412.16633" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://dl.acm.org/doi/abs/10.5555/3618408.3618748" target="_blank" rel="noopener" title="Palm-e: An embodied multimodal language model">Palm-e: An embodied multimodal language model</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Danny Driess" aria-label="First author: Danny Driess">Danny Driess</span><span data-v-72037286="" class="date-pill" title="2023.07">2023.07</span><a data-v-72037286="" class="link-icon" href="https://dl.acm.org/doi/abs/10.5555/3618408.3618748" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2303.16958" target="_blank" rel="noopener" title="Partmanip: Learning cross-category generalizable part manipulation policy from point cloud observations">Partmanip: Learning cross-category generalizable part manipulation policy from point cloud observations</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Haoran Geng" aria-label="First author: Haoran Geng">Haoran Geng</span><span data-v-72037286="" class="date-pill" title="2023.03">2023.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2303.16958" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2408.02297" target="_blank" rel="noopener" title="Perception Matters: Enhancing Embodied AI with Uncertainty-Aware Semantic Segmentation">Perception Matters: Enhancing Embodied AI with Uncertainty-Aware Semantic Segmentation</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Sai Prasanna" aria-label="First author: Sai Prasanna">Sai Prasanna</span><span data-v-72037286="" class="date-pill" title="2024.08">2024.08</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2408.02297" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2205.09991" target="_blank" rel="noopener" title="Planning with Diffusion for Flexible Behavior Synthesis">Planning with Diffusion for Flexible Behavior Synthesis</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2022.05">2022.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2205.09991" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2309.09919" target="_blank" rel="noopener" title="Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents">Plug in the Safety Chip: Enforcing Constraints for LLM-driven Robot Agents</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Abuse Prevention">Safety - Abuse Prevention</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2023.09">2023.09</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2309.09919" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://openreview.net/forum?id=e5admkWKgV" target="_blank" rel="noopener" title="Position: a call for embodied AI">Position: a call for embodied AI</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><!----><a data-v-72037286="" class="link-icon" href="https://openreview.net/forum?id=e5admkWKgV" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2505.07766" target="_blank" rel="noopener" title="Privacy Risks of Robot Vision: A User Study on Image Modalities and Resolution">Privacy Risks of Robot Vision: A User Study on Image Modalities and Resolution</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Privacy Protection">Safety - Privacy Protection</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2505.07766" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://dl.acm.org/doi/10.1145/3689216" target="_blank" rel="noopener" title="Privacy beyond Data: Assessment and Mitigation of Privacy Risks in Robotic Technology for Elderly Care">Privacy beyond Data: Assessment and Mitigation of Privacy Risks in Robotic Technology for Elderly Care</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Privacy Protection">Safety - Privacy Protection</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2024.11">2024.11</span><a data-v-72037286="" class="link-icon" href="https://dl.acm.org/doi/10.1145/3689216" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ieeexplore.ieee.org/document/8967681" target="_blank" rel="noopener" title="Privacy-preserving robot vision with anonymized faces by extreme low resolution">Privacy-preserving robot vision with anonymized faces by extreme low resolution</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Privacy Protection">Safety - Privacy Protection</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2019.11">2019.11</span><a data-v-72037286="" class="link-icon" href="https://ieeexplore.ieee.org/document/8967681" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2205.06311" target="_blank" rel="noopener" title="Provably Safe Deep Reinforcement Learning for Robotic Manipulation in Human Environments">Provably Safe Deep Reinforcement Learning for Robotic Manipulation in Human Environments</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><!----><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2205.06311" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2410.07864" target="_blank" rel="noopener" title="RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation">RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author:  Songming Liu; Lingxuan Wu" aria-label="First author:  Songming Liu; Lingxuan Wu"> Songming Liu; Lingxuan Wu</span><span data-v-72037286="" class="date-pill" title="2025.03">2025.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2410.07864" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2210.03629" target="_blank" rel="noopener" title="REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS">REACT: SYNERGIZING REASONING AND ACTING IN LANGUAGE MODELS</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Shunyu Yao" aria-label="First author: Shunyu Yao">Shunyu Yao</span><span data-v-72037286="" class="date-pill" title="2023.05">2023.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2210.03629" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2307.11932" target="_blank" rel="noopener" title="RIC: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction">RIC: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Isaac Kasahara" aria-label="First author: Isaac Kasahara">Isaac Kasahara</span><span data-v-72037286="" class="date-pill" title="2023.07">2023.07</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2307.11932" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://proceedings.mlr.press/v229/zitkovich23a.html" target="_blank" rel="noopener" title="RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control">RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2023.07">2023.07</span><a data-v-72037286="" class="link-icon" href="https://proceedings.mlr.press/v229/zitkovich23a.html" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.ndss-symposium.org/ndss-paper/auto-draft-476/" target="_blank" rel="noopener" title="Random Spoofing Attack against LiDAR-Based Scan Matching SLAM">Random Spoofing Attack against LiDAR-Based Scan Matching SLAM</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Masashi Fukunaga" aria-label="First author: Masashi Fukunaga">Masashi Fukunaga</span><span data-v-72037286="" class="date-pill" title="2024.02">2024.02</span><a data-v-72037286="" class="link-icon" href="https://www.ndss-symposium.org/ndss-paper/auto-draft-476/" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.ndss-symposium.org/ndss-paper/auto-draft-476/" target="_blank" rel="noopener" title="Random Spoofing Attack against Scan Matching Algorithm SLAM (Long)">Random Spoofing Attack against Scan Matching Algorithm SLAM (Long)</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Safety - Abuse Prevention">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2024.02">2024.02</span><a data-v-72037286="" class="link-icon" href="https://www.ndss-symposium.org/ndss-paper/auto-draft-476/" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2409.13682" target="_blank" rel="noopener" title="ReMEmbR: Building and Reasoning Over Long-Horizon Spatio-Temporal Memory for Robot Navigation">ReMEmbR: Building and Reasoning Over Long-Horizon Spatio-Temporal Memory for Robot Navigation</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Abrar Anwar" aria-label="First author: Abrar Anwar">Abrar Anwar</span><span data-v-72037286="" class="date-pill" title="2024.09">2024.09</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2409.13682" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2505.05519" target="_blank" rel="noopener" title="Real-time privacy preservation for robot visual perception">Real-time privacy preservation for robot visual perception</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Privacy Protection">Safety - Privacy Protection</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2505.05519" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2505.22050" target="_blank" rel="noopener" title="Reinforced Reasoning for Embodied Planning">Reinforced Reasoning for Embodied Planning</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Di Wu" aria-label="First author: Di Wu">Di Wu</span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2505.22050" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ieeexplore.ieee.org/abstract/document/10611254" target="_blank" rel="noopener" title="Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-to-End">Resilient Legged Local Navigation: Learning to Traverse with Compromised Perception End-to-End</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Trustworthiness - Reliability">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Jin Jin" aria-label="First author: Jin Jin">Jin Jin</span><span data-v-72037286="" class="date-pill" title="2023.01">2023.01</span><a data-v-72037286="" class="link-icon" href="https://ieeexplore.ieee.org/abstract/document/10611254" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2408.04449v1" target="_blank" rel="noopener" title="RiskAwareBench: Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based Embodied Agents">RiskAwareBench: Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based Embodied Agents</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2024.08">2024.08</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2408.04449v1" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2504.17070" target="_blank" rel="noopener" title="Robo-Troj: Attacking LLM-based Task Planners">Robo-Troj: Attacking LLM-based Task Planners</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Mohaiminul Al Nahian; Zainab Altaweel" aria-label="First author: Mohaiminul Al Nahian; Zainab Altaweel">Mohaiminul Al Nahian; Zainab Altaweel</span><span data-v-72037286="" class="date-pill" title="2025.04">2025.04</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2504.17070" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2502.07837" target="_blank" rel="noopener" title="RoboBERT: An End-to-end Multimodal Robotic Manipulation Model">RoboBERT: An End-to-end Multimodal Robotic Manipulation Model</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author:  Sicheng Wang, Sheng Liu, Weiheng Wang" aria-label="First author:  Sicheng Wang, Sheng Liu, Weiheng Wang"> Sicheng Wang, Sheng Liu, Weiheng Wang</span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2502.07837" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2503.19510" target="_blank" rel="noopener" title="RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation">RoboFlamingo-Plus: Fusion of Depth and RGB Perception with Vision-Language Models for Enhanced Robotic Manipulation</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Xiaojian Li" aria-label="First author: Xiaojian Li">Xiaojian Li</span><span data-v-72037286="" class="date-pill" title="2025.03">2025.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2503.19510" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2504.21530" target="_blank" rel="noopener" title="RoboGround: Robotic Manipulation with Grounded Vision-Language Priors">RoboGround: Robotic Manipulation with Grounded Vision-Language Priors</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Haifeng Huang; Xinyi Chen" aria-label="First author: Haifeng Huang; Xinyi Chen">Haifeng Huang; Xinyi Chen</span><span data-v-72037286="" class="date-pill" title="2025.04">2025.04</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2504.21530" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/cit2.70022" target="_blank" rel="noopener" title="Robot Manipulation Based on Embodied Visual Perception: A Survey">Robot Manipulation Based on Embodied Visual Perception: A Survey</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Sicheng Wang" aria-label="First author: Sicheng Wang">Sicheng Wang</span><span data-v-72037286="" class="date-pill" title="2025.06">2025.06</span><a data-v-72037286="" class="link-icon" href="https://ietresearch.onlinelibrary.wiley.com/doi/full/10.1049/cit2.70022" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/1907.04616" target="_blank" rel="noopener" title="Robust Humanoid Locomotion Using Trajectory Optimization and Sample-Efficient Learning">Robust Humanoid Locomotion Using Trajectory Optimization and Sample-Efficient Learning</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2019.07">2019.07</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/1907.04616" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2504.18698" target="_blank" rel="noopener" title="Robust Push Recovery on Bipedal Robots: Leveraging Multi-Domain Hybrid Systems with Reduced-Order Model Predictive Control">Robust Push Recovery on Bipedal Robots: Leveraging Multi-Domain Hybrid Systems with Reduced-Order Model Predictive Control</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Min Dai" aria-label="First author: Min Dai">Min Dai</span><span data-v-72037286="" class="date-pill" title="2025.04">2025.04</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2504.18698" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://openaccess.thecvf.com/content/ICCV2021/html/Chattopadhyay_RobustNav_Towards_Benchmarking_Robustness_in_Embodied_Navigation_ICCV_2021_paper.html" target="_blank" rel="noopener" title="Robustnav: Towards benchmarking robustness in embodied navigation">Robustnav: Towards benchmarking robustness in embodied navigation</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Trustworthiness - Reliability">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Prithvijit Chattopadhyay" aria-label="First author: Prithvijit Chattopadhyay">Prithvijit Chattopadhyay</span><span data-v-72037286="" class="date-pill" title="2021.06">2021.06</span><a data-v-72037286="" class="link-icon" href="https://openaccess.thecvf.com/content/ICCV2021/html/Chattopadhyay_RobustNav_Towards_Benchmarking_Robustness_in_Embodied_Navigation_ICCV_2021_paper.html" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://link.springer.com/chapter/10.1007/978-3-031-25075-0_15" target="_blank" rel="noopener" title="Robustness of embodied point navigation agents">Robustness of embodied point navigation agents</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Trustworthiness - Reliability">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Frano Rajiˇc" aria-label="First author: Frano Rajiˇc">Frano Rajiˇc</span><span data-v-72037286="" class="date-pill" title="2023.02">2023.02</span><a data-v-72037286="" class="link-icon" href="https://link.springer.com/chapter/10.1007/978-3-031-25075-0_15" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/1803.00653" target="_blank" rel="noopener" title="SEMI-PARAMETRIC TOPOLOGICAL MEMORY FOR NAVIGATION">SEMI-PARAMETRIC TOPOLOGICAL MEMORY FOR NAVIGATION</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Nikolay Savinov" aria-label="First author: Nikolay Savinov">Nikolay Savinov</span><span data-v-72037286="" class="date-pill" title="2018.05">2018.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/1803.00653" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/html/2502.13641v1" target="_blank" rel="noopener" title="SLAMSpoof: Practical LiDAR Spoofing Attacks on Localization Systems Guided by Scan Matching Vulnerability Analysis">SLAMSpoof: Practical LiDAR Spoofing Attacks on Localization Systems Guided by Scan Matching Vulnerability Analysis</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Safety - Abuse Prevention">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Rokuto Nagata" aria-label="First author: Rokuto Nagata">Rokuto Nagata</span><span data-v-72037286="" class="date-pill" title="2025.02">2025.02</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/html/2502.13641v1" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2412.20350" target="_blank" rel="noopener" title="Safe Bayesian Optimization for the Control of High-Dimensional Embodied Systems">Safe Bayesian Optimization for the Control of High-Dimensional Embodied Systems</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Yunyue Wei" aria-label="First author: Yunyue Wei">Yunyue Wei</span><span data-v-72037286="" class="date-pill" title="2024.12">2024.12</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2412.20350" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.sciencedirect.com/science/article/abs/pii/S0004370223000516?via%3Dihub" target="_blank" rel="noopener" title="Safe multi-agent reinforcement learning for multi-robot control">Safe multi-agent reinforcement learning for multi-robot control</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><!----><a data-v-72037286="" class="link-icon" href="https://www.sciencedirect.com/science/article/abs/pii/S0004370223000516?via%3Dihub" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/1610.03295" target="_blank" rel="noopener" title="Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving">Safe, Multi-Agent, Reinforcement Learning for Autonomous Driving</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><!----><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/1610.03295" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2412.13178" target="_blank" rel="noopener" title="SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents">SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Sheng Yin" aria-label="First author: Sheng Yin">Sheng Yin</span><span data-v-72037286="" class="date-pill" title="2024.12">2024.12</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2412.13178" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2206.09682" target="_blank" rel="noopener" title="SafeBench: A Benchmarking Platform for Safety Evaluation of Autonomous Vehicles">SafeBench: A Benchmarking Platform for Safety Evaluation of Autonomous Vehicles</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Chejian Xu" aria-label="First author: Chejian Xu">Chejian Xu</span><span data-v-72037286="" class="date-pill" title="2022.06">2022.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2206.09682" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2409.01630" target="_blank" rel="noopener" title="SafeEmbodAI: a Safety Framework for Mobile Robots in Embodied AI Systems">SafeEmbodAI: a Safety Framework for Mobile Robots in Embodied AI Systems</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><!----><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2409.01630" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2503.03480" target="_blank" rel="noopener" title="SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning">SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Action Planning, Physical Interaction">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Value Alignment">Safety - Value Alignment</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Borong Zhang" aria-label="First author: Borong Zhang">Borong Zhang</span><span data-v-72037286="" class="date-pill" title="2025.03">2025.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2503.03480" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2208.08237" target="_blank" rel="noopener" title="Safety Assessment for Autonomous Systems&#39; Perception Capabilities">Safety Assessment for Autonomous Systems' Perception Capabilities</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Auditability">Trustworthiness - Auditability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2022.08">2022.08</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2208.08237" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2503.15707" target="_blank" rel="noopener" title="Safety Aware Task Planning via Large Language Models in Robotics">Safety Aware Task Planning via Large Language Models in Robotics</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Controllability">Trustworthiness - Controllability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Azal Ahmad Khan" aria-label="First author: Azal Ahmad Khan">Azal Ahmad Khan</span><span data-v-72037286="" class="date-pill" title="2025.03">2025.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2503.15707" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2405.17846" target="_blank" rel="noopener" title="Safety Control of Service Robots with LLMs and Embodied Knowledge Graphs">Safety Control of Service Robots with LLMs and Embodied Knowledge Graphs</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Yong Qi" aria-label="First author: Yong Qi">Yong Qi</span><span data-v-72037286="" class="date-pill" title="2024.05">2024.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2405.17846" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2109.14700" target="_blank" rel="noopener" title="Safety assurances for human-robot interaction via confidence-aware game-theoretic human models">Safety assurances for human-robot interaction via confidence-aware game-theoretic human models</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Controllability">Trustworthiness - Controllability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2021.01">2021.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2109.14700" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10161569" target="_blank" rel="noopener" title="Se (3)-diffusionfields: Learning smooth cost functions for joint grasp and motion optimization through diffusion">Se (3)-diffusionfields: Learning smooth cost functions for joint grasp and motion optimization through diffusion</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Julen Urain;Niklas Funk" aria-label="First author: Julen Urain;Niklas Funk">Julen Urain;Niklas Funk</span><span data-v-72037286="" class="date-pill" title="2023.07">2023.07</span><a data-v-72037286="" class="link-icon" href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10161569" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://link.springer.com/chapter/10.1007/978-3-031-68256-8_21" target="_blank" rel="noopener" title="Securing Embodied AI: Addressing Cybersecurity Challenges in Physical Systems">Securing Embodied AI: Addressing Cybersecurity Challenges in Physical Systems</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><!----><a data-v-72037286="" class="link-icon" href="https://link.springer.com/chapter/10.1007/978-3-031-68256-8_21" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2409.09972" target="_blank" rel="noopener" title="Securing the Future: Exploring Privacy Risks and Security Questions in Robotic Systems">Securing the Future: Exploring Privacy Risks and Security Questions in Robotic Systems</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><!----><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2409.09972" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2310.08565" target="_blank" rel="noopener" title="Security Considerations in AI-Robotics: A Survey of Current Methods, Challenges, and Opportunities">Security Considerations in AI-Robotics: A Survey of Current Methods, Challenges, and Opportunities</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Privacy Protection">Safety - Privacy Protection</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2023.01">2023.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2310.08565" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2310.08565" target="_blank" rel="noopener" title="Security Considerations in AI-Robotics:A Survey of Current Methods,Challenges, and Opportunities">Security Considerations in AI-Robotics:A Survey of Current Methods,Challenges, and Opportunities</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: SUBASH NEUPANE" aria-label="First author: SUBASH NEUPANE">SUBASH NEUPANE</span><span data-v-72037286="" class="date-pill" title="2024.01">2024.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2310.08565" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2404.05603" target="_blank" rel="noopener" title="Self-Explainable Affordance Learning with Embodied Caption">Self-Explainable Affordance Learning with Embodied Caption</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Explainability">Trustworthiness - Explainability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Zhipeng Zhang" aria-label="First author: Zhipeng Zhang">Zhipeng Zhang</span><span data-v-72037286="" class="date-pill" title="2024.04">2024.04</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2404.05603" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2408.01024" target="_blank" rel="noopener" title="Semantic Skill Grounding for Embodied Instruction-Following in Cross-Domain Environments">Semantic Skill Grounding for Embodied Instruction-Following in Cross-Domain Environments</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2024.08">2024.08</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2408.01024" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2501.06919" target="_blank" rel="noopener" title="Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing">Shake-VLA: Vision-Language-Action Model-Based System for Bimanual Robotic Manipulations and Liquid Mixing</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Muhamamd Haris Khan; Selamawit Asfaw" aria-label="First author: Muhamamd Haris Khan; Selamawit Asfaw">Muhamamd Haris Khan; Selamawit Asfaw</span><span data-v-72037286="" class="date-pill" title="2025.01">2025.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2501.06919" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2407.12061" target="_blank" rel="noopener" title="Situated Instruction Following">Situated Instruction Following</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: So Yeon Min" aria-label="First author: So Yeon Min">So Yeon Min</span><span data-v-72037286="" class="date-pill" title="2024.07">2024.07</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2407.12061" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2312.11598" target="_blank" rel="noopener" title="SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution">SkillDiffuser: Interpretable Hierarchical Planning via Skill Abstractions in Diffusion-Based Task Execution</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2023.12">2023.12</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2312.11598" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/html/2411.17735v1" target="_blank" rel="noopener" title="SnapMem: Snapshot-based 3D Scene Memory for Embodied Exploration and Reasoning">SnapMem: Snapshot-based 3D Scene Memory for Embodied Exploration and Reasoning</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Yuncong Yang" aria-label="First author: Yuncong Yang">Yuncong Yang</span><span data-v-72037286="" class="date-pill" title="2024.11">2024.11</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/html/2411.17735v1" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2205.04662" target="_blank" rel="noopener" title="SoK: Rethinking Sensor Spoofing Attacks against Robotic Vehicles from a Systematic View">SoK: Rethinking Sensor Spoofing Attacks against Robotic Vehicles from a Systematic View</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Safety - Abuse Prevention">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2023.07">2023.07</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2205.04662" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2504.13276" target="_blank" rel="noopener" title="Strategic Planning of Stealthy Backdoor Attacks in Markov Decision Processes">Strategic Planning of Stealthy Backdoor Attacks in Markov Decision Processes</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Xinyi Wei" aria-label="First author: Xinyi Wei">Xinyi Wei</span><span data-v-72037286="" class="date-pill" title="2025.04">2025.04</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2504.13276" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2505.19933" target="_blank" rel="noopener" title="Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety of LLMs for Embodied Decision Making">Subtle Risks, Critical Failures: A Framework for Diagnosing Physical Safety of LLMs for Embodied Decision Making</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Yejin Son" aria-label="First author: Yejin Son">Yejin Son</span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2505.19933" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/1509.06825" target="_blank" rel="noopener" title="Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours">Supersizing Self-supervision: Learning to Grasp from 50K Tries and 700 Robot Hours</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2015.09">2015.09</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/1509.06825" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2503.08548" target="_blank" rel="noopener" title="TLA: Tactile-Language-Action Model for Contact-Rich Manipulation">TLA: Tactile-Language-Action Model for Contact-Rich Manipulation</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author:  Peng Hao, Chaofan Zhang" aria-label="First author:  Peng Hao, Chaofan Zhang"> Peng Hao, Chaofan Zhang</span><span data-v-72037286="" class="date-pill" title="2025.03">2025.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2503.08548" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2504.00420" target="_blank" rel="noopener" title="Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation">Think Small, Act Big: Primitive Prompt Learning for Lifelong Robot Manipulation</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author:  Yuanqi Yao, Yan Ding, Siao Liu, Bin Zhao, Haoming Song" aria-label="First author:  Yuanqi Yao, Yan Ding, Siao Liu, Bin Zhao, Haoming Song"> Yuanqi Yao, Yan Ding, Siao Liu, Bin Zhao, Haoming Song</span><span data-v-72037286="" class="date-pill" title="2025.06">2025.06</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2504.00420" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2312.07062" target="_blank" rel="noopener" title="ThinkBot: Embodied Instruction Following with Thought Chain Reasoning">ThinkBot: Embodied Instruction Following with Thought Chain Reasoning</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2023.12">2023.12</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2312.07062" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2412.14171" target="_blank" rel="noopener" title="Thinking in Space:How Multimodal Large Language Models See, Remember, and Recall Spaces">Thinking in Space:How Multimodal Large Language Models See, Remember, and Recall Spaces</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Jihan Yang" aria-label="First author: Jihan Yang">Jihan Yang</span><span data-v-72037286="" class="date-pill" title="2024.12">2024.12</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2412.14171" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://openreview.net/forum?id=gPqkW8V6Je" target="_blank" rel="noopener" title="Towards Embodied Agent Intent Explanation in Human-Robot Collaboration: ACT Error Analysis and Solution Conceptualization">Towards Embodied Agent Intent Explanation in Human-Robot Collaboration: ACT Error Analysis and Solution Conceptualization</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Explainability">Trustworthiness - Explainability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://openreview.net/forum?id=gPqkW8V6Je" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://www.ri.cmu.edu/app/uploads/2021/08/MSRoboticsThesis-2.pdf" target="_blank" rel="noopener" title="Towards Explainable Embodied AI">Towards Explainable Embodied AI</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Vidhi Jain " aria-label="First author: Vidhi Jain ">Vidhi Jain </span><span data-v-72037286="" class="date-pill" title="2021.08">2021.08</span><a data-v-72037286="" class="link-icon" href="https://www.ri.cmu.edu/app/uploads/2021/08/MSRoboticsThesis-2.pdf" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ora.ox.ac.uk/objects/uuid:4d8079cb-7633-417d-ba9b-d580aaebff64" target="_blank" rel="noopener" title="Towards Explainable and Trustworthy Collaborative Robots through Embodied Question Answering">Towards Explainable and Trustworthy Collaborative Robots through Embodied Question Answering</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Lars Kunze" aria-label="First author: Lars Kunze">Lars Kunze</span><span data-v-72037286="" class="date-pill" title="2022.05">2022.05</span><a data-v-72037286="" class="link-icon" href="https://ora.ox.ac.uk/objects/uuid:4d8079cb-7633-417d-ba9b-d580aaebff64" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2502.00653" target="_blank" rel="noopener" title="Towards Robust Multimodal Large Language Models Against Jailbreak Attacks">Towards Robust Multimodal Large Language Models Against Jailbreak Attacks</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Ziyi Yin" aria-label="First author: Ziyi Yin">Ziyi Yin</span><span data-v-72037286="" class="date-pill" title="2025.02">2025.02</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2502.00653" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2502.13175" target="_blank" rel="noopener" title="Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks">Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Safety - Abuse Prevention">...</span><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Attack Resistance">Safety - Attack Resistance</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: WENPENG XING" aria-label="First author: WENPENG XING">WENPENG XING</span><span data-v-72037286="" class="date-pill" title="2025.02">2025.02</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2502.13175" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2202.08471" target="_blank" rel="noopener" title="TransCG: A Large-Scale Real-World Dataset for Transparent Object Depth Completion and a Grasping Baseline">TransCG: A Large-Scale Real-World Dataset for Transparent Object Depth Completion and a Grasping Baseline</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Hongjie Fang" aria-label="First author: Hongjie Fang">Hongjie Fang</span><span data-v-72037286="" class="date-pill" title="2022.02">2022.02</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2202.08471" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2310.01163" target="_blank" rel="noopener" title="Trust-aware motion planning for human-robot collaboration under distribution temporal logic specifications">Trust-aware motion planning for human-robot collaboration under distribution temporal logic specifications</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Controllability">Trustworthiness - Controllability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2023.01">2023.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2310.01163" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2503.10628" target="_blank" rel="noopener" title="Uncertainty in Action: Confidence Elicitation in Embodied Agents">Uncertainty in Action: Confidence Elicitation in Embodied Agents</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Tianjiao Yu" aria-label="First author: Tianjiao Yu">Tianjiao Yu</span><span data-v-72037286="" class="date-pill" title="2025.03">2025.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2503.10628" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ieeexplore.ieee.org/document/8825881" target="_blank" rel="noopener" title="Unintended Consequences of Biased Robotic and Artificial Intelligence Systems [Ethical, Legal, and Societal Issues]">Unintended Consequences of Biased Robotic and Artificial Intelligence Systems [Ethical, Legal, and Societal Issues]</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Ludovic Righetti" aria-label="First author: Ludovic Righetti">Ludovic Righetti</span><span data-v-72037286="" class="date-pill" title="2019.09">2019.09</span><a data-v-72037286="" class="link-icon" href="https://ieeexplore.ieee.org/document/8825881" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2502.02175" target="_blank" rel="noopener" title="VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation">VLA-Cache: Towards Efficient Vision-Language-Action Model via Adaptive Token Caching in Robotic Manipulation</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Siyu Xu" aria-label="First author: Siyu Xu">Siyu Xu</span><span data-v-72037286="" class="date-pill" title="2025.02">2025.02</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2502.02175" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2505.18719" target="_blank" rel="noopener" title="VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning">VLA-RL: Towards Masterful and General Robotic Manipulation with Scalable Reinforcement Learning</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Guanxing Lu" aria-label="First author: Guanxing Lu">Guanxing Lu</span><span data-v-72037286="" class="date-pill" title="2025.05">2025.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2505.18719" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2404.00210" target="_blank" rel="noopener" title="VLM-Social-Nav: Socially Aware Robot Navigation through Scoring using Vision-Language Models">VLM-Social-Nav: Socially Aware Robot Navigation through Scoring using Vision-Language Models</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Controllability">Trustworthiness - Controllability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2024.11">2024.11</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2404.00210" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2305.16291" target="_blank" rel="noopener" title="VOYAGER: An Open-Ended Embodied Agent with Large Language Models ">VOYAGER: An Open-Ended Embodied Agent with Large Language Models </a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Guanzhi Wang" aria-label="First author: Guanzhi Wang">Guanzhi Wang</span><span data-v-72037286="" class="date-pill" title="2023.05">2023.05</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2305.16291" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2402.11498" target="_blank" rel="noopener" title="Verifiably Following Complex Robot Instructions with Foundation Models">Verifiably Following Complex Robot Instructions with Foundation Models</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2024.02">2024.02</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2402.11498" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ieeexplore.ieee.org/abstract/document/10592798" target="_blank" rel="noopener" title="Viewinfer3d: 3d visual grounding based on embodied viewpoint inference">Viewinfer3d: 3d visual grounding based on embodied viewpoint inference</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Liang Geng" aria-label="First author: Liang Geng">Liang Geng</span><span data-v-72037286="" class="date-pill" title="2024.07">2024.07</span><a data-v-72037286="" class="link-icon" href="https://ieeexplore.ieee.org/abstract/document/10592798" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2210.03094" target="_blank" rel="noopener" title="Vima: General robot manipulation with multimodal prompts">Vima: General robot manipulation with multimodal prompts</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Yunfan Jiang" aria-label="First author: Yunfan Jiang">Yunfan Jiang</span><span data-v-72037286="" class="date-pill" title="2022.01">2022.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2210.03094" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2410.14022" target="_blank" rel="noopener" title="Vision-Language-Action Model and Diffusion Policy Switching Enables Dexterous Control of an Anthropomorphic Hand">Vision-Language-Action Model and Diffusion Policy Switching Enables Dexterous Control of an Anthropomorphic Hand</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Physical Interaction">Physical Interaction</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Reliability">Trustworthiness - Reliability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Cheng Pan" aria-label="First author: Cheng Pan">Cheng Pan</span><span data-v-72037286="" class="date-pill" title="2024.01">2024.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2410.14022" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://openaccess.thecvf.com/content/CVPR2022/html/Dwivedi_What_Do_Navigation_Agents_Learn_About_Their_Environment_CVPR_2022_paper.html" target="_blank" rel="noopener" title="What do navigation agents learn about their environment?">What do navigation agents learn about their environment?</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Environment Perception">Environment Perception</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Explainability">Trustworthiness - Explainability</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Kshitij Dwivedi" aria-label="First author: Kshitij Dwivedi">Kshitij Dwivedi</span><span data-v-72037286="" class="date-pill" title="2022.06">2022.06</span><a data-v-72037286="" class="link-icon" href="https://openaccess.thecvf.com/content/CVPR2022/html/Dwivedi_What_Do_Navigation_Agents_Learn_About_Their_Environment_CVPR_2022_paper.html" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://dl.acm.org/doi/pdf/10.1145/3645090" target="_blank" rel="noopener" title="Who’s in Charge Here? A Survey on Trustworthy AI in Variable Autonomy Robotic Systems">Who’s in Charge Here? A Survey on Trustworthy AI in Variable Autonomy Robotic Systems</a><div data-v-72037286="" class="tags-row"><span data-v-72037286="" class="more-tags-indicator" aria-label="Instruction Understanding, Trustworthiness - Controllability">...</span><span data-v-72037286="" class="pill pill-stage" title="Action Planning">Action Planning</span><span data-v-72037286="" class="pill pill-safety" title="Safety - Value Alignment">Safety - Value Alignment</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: Leila Methnani" aria-label="First author: Leila Methnani">Leila Methnani</span><span data-v-72037286="" class="date-pill" title="2024.04">2024.04</span><a data-v-72037286="" class="link-icon" href="https://dl.acm.org/doi/pdf/10.1145/3645090" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://ieeexplore.ieee.org/abstract/document/10771431" target="_blank" rel="noopener" title="World Models: The Safety Perspective">World Models: The Safety Perspective</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><!----><a data-v-72037286="" class="link-icon" href="https://ieeexplore.ieee.org/abstract/document/10771431" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/pdf/2503.07087" target="_blank" rel="noopener" title="iManip: Skill-Incremental Learning for Robotic Manipulation">iManip: Skill-Incremental Learning for Robotic Manipulation</a><div data-v-72037286="" class="tags-row"><!----></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author:  Zexin Zheng" aria-label="First author:  Zexin Zheng"> Zexin Zheng</span><span data-v-72037286="" class="date-pill" title="2025.03">2025.03</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/pdf/2503.07087" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div><div data-v-0172fc6e="" class="v-col-lg-11 v-col-xl-11 v-col-12"><article data-v-72037286="" data-v-0172fc6e="" class="paper-card" style="--pc-scale: 0.9;"><div data-v-72037286="" class="top-row"><a data-v-72037286="" class="title" href="https://arxiv.org/abs/2310.15605" target="_blank" rel="noopener" title="tagE: Enabling an Embodied Agent to Understand Human Instructions">tagE: Enabling an Embodied Agent to Understand Human Instructions</a><div data-v-72037286="" class="tags-row"><!----><span data-v-72037286="" class="pill pill-stage" title="Instruction Understanding">Instruction Understanding</span><span data-v-72037286="" class="pill pill-trust" title="Trustworthiness - Accuracy">Trustworthiness - Accuracy</span></div></div><div data-v-72037286="" class="foot"><span data-v-72037286="" class="author" title="First author: " aria-label="First author: "></span><span data-v-72037286="" class="date-pill" title="2023.01">2023.01</span><a data-v-72037286="" class="link-icon" href="https://arxiv.org/abs/2310.15605" target="_blank" rel="noopener noreferrer" aria-label="Open link"><svg data-v-72037286="" viewBox="0 0 24 24" width="16" height="16" aria-hidden="true"><path data-v-72037286="" d="M14 3h7v7h-2V6.41l-9.29 9.3-1.42-1.42 9.3-9.29H14V3zM5 5h6v2H7v10h10v-4h2v6H5V5z" fill="currentColor"></path></svg></a></div><!----></article></div></div></div><!----></div><!----><!----><impactmetrics data-v-0172fc6e="" stars-repo="owner/repo" updated-repo="owner/repo" visitors-api="https://old-union-b7eb.3034297530.workers.dev/visitors" collect-api="https://old-union-b7eb.3034297530.workers.dev/collect" top-n="5"></impactmetrics><div data-v-0172fc6e="" class="v-container v-locale--is-ltr py-8"><div data-v-0172fc6e="" class="v-row justify-center"><div data-v-0172fc6e="" class="v-col-sm-11 v-col-md-10 v-col-lg-9 v-col-xl-9 v-col-12"><section data-v-3f888ab6="" data-v-0172fc6e="" class="mt-8"><div data-v-3f888ab6="" class="bibtex-header"><h2 data-v-3f888ab6="" class="bibtex-title">BibTeX</h2><button data-v-3f888ab6="" type="button" class="v-btn v-theme--light v-btn--density-default v-btn--size-small v-btn--variant-text"><span class="v-btn__overlay"></span><span class="v-btn__underlay"></span><!----><span class="v-btn__content" data-no-activator="">Copy</span><!----><!----></button></div><div data-v-3f888ab6="" class="bibtex-box"><pre data-v-3f888ab6="" class="bibtex-pre"><code data-v-3f888ab6="">@article{tan2025safetrustworthyeai,
  title={Towards Safe and Trustworthy Embodied AI: Foundations, Status, and Prospects},
  author={Tan, Xin and Liu, Bangwei and Bao, Yicheng and Tian, Qijian and Gao, Zhenkun and Wu, Xiongbin and Luo, Zhihao and Wang, Sen and Zhang, Yuqi and Wang, Xuhong and Lu, Chaochao and Zhou, Bowen},
  journal={Open Review},
  url={https://openreview.net/pdf?id=Eu6Yt21Alv},
  year={2025}
}</code></pre></div></section></div></div></div></div></div></div><svg id="MJX-SVG-global-cache" style="display: none;"><defs></defs></svg><div data-v-68ceeaf2="" class="spc-fixed" style="position: fixed; z-index: 2140; right: 16px; bottom: 16px; width: 210px;"><button data-v-68ceeaf2="" type="button" class="v-btn v-theme--light text-primary v-btn--density-comfortable elevation-0 rounded-xl v-btn--size-default v-btn--variant-outlined spc-btn" aria-describedby="v-tooltip-v-4" style="height: 36px;"><span class="v-btn__overlay"></span><span class="v-btn__underlay"></span><!----><span class="v-btn__content" data-no-activator="">SHARE YOUR IDEA ✨</span><!----><!----></button><button data-v-68ceeaf2="" type="button" class="v-btn v-theme--light text-primary v-btn--density-comfortable elevation-0 rounded-xl v-btn--size-default v-btn--variant-outlined spc-btn" style="height: 36px;"><span class="v-btn__overlay"></span><span class="v-btn__underlay"></span><!----><span class="v-btn__content" data-no-activator="">MEET SOME TROUBLES</span><!----><!----></button></div><button data-v-5fd8d1ac="" class="metrics-fab" aria-label="Open metrics">ⓘ</button><div class="v-overlay-container"><div class="v-overlay v-overlay--absolute v-theme--light v-locale--is-ltr v-tooltip" id="v-tooltip-v-4" role="tooltip" data-v-68ceeaf2="" style="z-index: 2000;"><!----><div class="v-overlay__content" target="[object HTMLButtonElement]" style="min-width: 0px; display: none;">Think your work fits this topic? Share it with us — we review regularly.</div></div></div></body><div id="immersive-translate-popup" style="all: initial"><template shadowrootmode="open"><style>/** 基础色阶定义 **/
:root,
#mount[data-theme="light"],
#mount:not([data-theme="dark"]) {
  /* 中性灰阶（light） */
  --c-00: #000000;
  --c-22: #222222;
  --c-33: #333333;
  --c-66: #666666;
  --c-83: #838383;
  --c-99: #999999;
  --c-c7: #c7c7c7;
  --c-cc: #cccccc;
  --c-e6: #e6e6e6;
  --c-f5: #f5f5f5;
  --c-ff: #ffffff;
  /* 品牌主色阶（light） */
  --p-main: #ea4c89;
  --p-hover: #ec5e95;
  --p-active: #e34a85;
  --p-special: #ee71a2;
  --p-disabled: #f4a5c4;
  --p-text-disabled: #f9c9dc;
  --p-weak: #fdedf3;
  /* Surface 层级（light，TC 填充-1） */
  --s-1: #f3f5f6;
  --s-1-hover: #f6f8f9;
  --s-1-active: #edf1f2;
  --s-1-weak: #fafbfb;
  /* 输入/边框（light，TC 填充-2） */
  --input-bg-base: #fafbfc;
  --input-border: #ecf0f7;
  --input-border-strong: #e0e0e6;
  --input-bg-strong: #fafdff;
}

:root[data-theme="dark"],
[data-theme="dark"] {
  /* 中性灰阶（dark） */
  --c-00: #ffffff;
  --c-22: #dbdbdb;
  --c-33: #dbdbdb;
  --c-66: #b3b3b3;
  --c-83: #838383;
  --c-99: #707070;
  --c-c7: #666666;
  --c-cc: #5c5c5c;
  --c-e6: #3b3b3b;
  --c-f5: #262626;
  --c-ff: #222222;
  /* 品牌主色阶（dark） */
  --p-main: #e23c7c;
  --p-hover: #ea4c89;
  --p-active: #d5467d;
  --p-special: #a93a65;
  --p-disabled: #7e2f4d;
  --p-text-disabled: #522335;
  --p-weak: #26171d;
  /* Surface 层级（dark，TC 填充-1） */
  --s-1: #2d2e2f;
  --s-1-hover: #323434;
  --s-1-active: #202121;
  --s-1-weak: #262627;
  /* 输入/边框（dark，TC 填充-2） */
  --input-bg-base: #2b2d30;
  --input-border: #3e434b;
  --input-border-strong: #43474b;
  --input-bg-strong: #1f2123;
}

:root,
#mount [data-theme] {
  /* 业务/通用变量引用色阶（全局可见，含 Shadow DOM） */
  --primary: var(--p-main);
  --primary-hover: var(--p-hover);
  --primary-inverse: #fff;
  --modal-background: var(--s-1);
  --modal-border: var(--input-border);
  --modal-text: var(--c-22);
  --modal-text-secondary: var(--c-66);
  --modal-error: var(--p-main);
  --modal-required: #f53f3f;
  --modal-success: #68cd52;
  --modal-button-background: var(--p-main);
  --modal-button-text: var(--c-ff);
  --modal-input-background: var(--input-bg-base);
  --modal-check-color: var(--p-main);
  --background-color: var(--c-ff);
  --background-light-green: var(--s-1-weak, #f5f7f9);
  --text-black-2: var(--c-22);
  --text-gray-2: var(--c-22);
  --text-gray-6: var(--c-66);
  --text-gray-9: var(--c-99);
  --text-gray-c2: var(--c-c7);
  --switch-background-color: var(--c-c7, hsl(205deg, 16%, 77%));
  --float-ball-more-button-border-color: var(--c-f5, #f6f6f6);
  --float-ball-more-button-background-color: var(--c-ff);
  --float-ball-more-button-hover-color: var(--p-weak);
  --float-ball-more-button-svg-color: #6c6f73;
  --service-bg-hover: var(--s-1-hover, #f7faff);
  --service-bg: var(--s-1-weak, #fafbfb);
}

#mount {
  --font-family: system-ui, -apple-system, "Segoe UI", "Roboto", "Ubuntu",
    "Cantarell", "Noto Sans", sans-serif, "Apple Color Emoji", "Segoe UI Emoji",
    "Segoe UI Symbol", "Noto Color Emoji";
  /* PC/H5 兼容的字号、间距、圆角、阴影变量 */
  --f-12: 12px;
  --f-14: 14px;
  --f-15: 15px;
  --f-16: 16px;
  --f-18: 18px;
  --f-20: 20px;
  --space-4: 4px;
  --space-6: 6px;
  --space-8: 8px;
  --space-12: 12px;
  --space-16: 16px;
  --space-18: 18px;
  --space-24: 24px;
  --radius-8: 8px;
  --radius-12: 12px;
  --radius-16: 16px;
  --control-height-lg: 44px;
  --width-28: 28px;
  --width-24: 24px;
  --width-20: 20px;
  --width-18: 18px;
  --width-16: 16px;
  --width-label-md: 56px;
  --shadow-lg: 0 18px 48px rgba(0, 0, 0, 0.12);

  /* 常规变量 */
  --line-height: 1.5;
  --font-weight: 400;
  --font-size: 16px;
  --border-radius: 4px;
  --border-width: 2px;
  --outline-width: 3px;
  --spacing: 16px;
  --typography-spacing-vertical: 24px;
  --block-spacing-vertical: calc(var(--spacing) * 2);
  --block-spacing-horizontal: var(--spacing);
  --grid-spacing-vertical: 0;
  --grid-spacing-horizontal: var(--spacing);
  --form-element-spacing-vertical: 12px;
  --form-element-spacing-horizontal: 16px;
  --nav-element-spacing-vertical: 16px;
  --nav-element-spacing-horizontal: 8px;
  --nav-link-spacing-vertical: 8px;
  --nav-link-spacing-horizontal: 8px;
  --form-label-font-weight: var(--font-weight);
  --transition: 0.2s ease-in-out;
  --modal-overlay-backdrop-filter: blur(4px);
  --switch-color: var(--primary-inverse);
  --switch-checked-background-color: var(--primary);
  --icon-xia: url("data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iMTYiIGhlaWdodD0iMTYiIHZpZXdCb3g9IjAgMCAxNiAxNiIgZmlsbD0ibm9uZSIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj4KPGcgaWQ9IkZyYW1lIj4KPHBhdGggaWQ9IlZlY3RvciIgZD0iTTguMDAyOTEgOS42Nzk4M0wzLjgzMzM5IDUuNTEyMjFMMy4wMjUzOSA2LjMxOTgzTDguMDAzMjkgMTEuMjk1MUwxMi45NzYyIDYuMzE5ODNMMTIuMTY3OSA1LjUxMjIxTDguMDAyOTEgOS42Nzk4M1oiIGZpbGw9IiM4MzgzODMiLz4KPC9nPgo8L3N2Zz4K");
  /* 兼容旧变量：主色直接引用品牌主色阶 */
  --primary: var(--p-main);
  --primary-hover: var(--p-hover);
  --primary-inverse: #fff;
  /* Modal 业务变量引用色阶 */
  --modal-background: var(--s-1);
  --modal-border: var(--input-border);
  --modal-text: var(--c-22);
  --modal-text-secondary: var(--c-66);
  --modal-error: var(--p-main);
  --modal-required: #f53f3f;
  --modal-success: #68cd52;
  --modal-button-background: var(--p-main);
  --modal-button-text: var(--c-ff);
  --modal-input-background: var(--input-bg-base);
  --modal-check-color: var(--p-main);
  --background-color: var(--c-ff);
  --background-light-green: var(--s-1-weak, #f5f7f9);
  --text-black-2: var(--c-22);
  --text-gray-2: var(--c-22);
  --text-gray-6: var(--c-66);
  --text-gray-9: var(--c-99);
  --text-gray-c2: var(--c-c7);
  --switch-background-color: var(--c-c7, hsl(205deg, 16%, 77%));
  --float-ball-more-button-border-color: var(--c-f5, #f6f6f6);
  --float-ball-more-button-background-color: var(--c-ff);
  --float-ball-more-button-hover-color: var(--p-weak);
  --float-ball-more-button-svg-color: #6c6f73;
  --service-bg-hover: var(--s-1-hover, #f7faff);
  --service-bg: var(--s-1-weak, #fafbfb);
  line-height: var(--line-height);
  font-family: var(--font-family);
  font-size: var(--font-size);
}

@media (max-width: 480px) {
  :root,
  #mount {
    --f-12: 10px;
    --f-14: 12px;
    --f-15: 13px;
    --f-16: 14px;
    --f-18: 16px;
    --f-20: 18px;
    --space-4: 4px;
    --space-6: 4px;
    --space-8: 6px;
    --space-12: 8px;
    --space-16: 12px;
    --space-18: 14px;
    --space-24: 18px;
    --radius-8: 6px;
    --radius-12: 10px;
    --radius-16: 12px;
    --control-height-lg: 38px;
    --shadow-lg: 0 12px 32px rgba(0, 0, 0, 0.1);
    --width-28: 24px;
    --width-24: 20px;
    --width-20: 16px;
    --width-18: 14px;
    --width-16: 12px;
    --width-label-md: 52px;
  }
}

#mount * {
  box-sizing: border-box;
}

[hidden] {
  display: none !important;
}

:where(#mount) a,
:where(#mount) [role="link"] {
  --color: var(--primary);
  --background-color: transparent;
  outline: none;
  background-color: var(--background-color);
  color: var(--color);
  -webkit-text-decoration: var(--text-decoration);
  text-decoration: var(--text-decoration);
  transition: background-color var(--transition), color var(--transition),
    box-shadow var(--transition), -webkit-text-decoration var(--transition);
  transition: background-color var(--transition), color var(--transition),
    text-decoration var(--transition), box-shadow var(--transition);
  transition: background-color var(--transition), color var(--transition),
    text-decoration var(--transition), box-shadow var(--transition),
    -webkit-text-decoration var(--transition);
}
:where(#mount) a:is([aria-current], :hover, :active, :focus),
:where(#mount) [role="link"]:is([aria-current], :hover, :active, :focus) {
  --color: var(--primary-hover);
  --text-decoration: underline;
}

:where(#mount) label {
  font-size: 13px;
  line-height: 1.3;
  color: var(--text-gray-2, #222222);
}

:where(#mount) button {
  width: 100%;
  font-family: inherit;
  font-size: 15px;
  line-height: 1.3;
  min-height: 44px;
  border-radius: 12px;
  padding: 0 14px;
  border: none;
  background-color: var(--primary, #ea4c89);
  color: #ffffff;
  cursor: pointer;
  transition: background-color 0.2s ease, box-shadow 0.2s ease, color 0.2s ease;
}

:where(#mount) button:hover {
  background-color: var(--primary-hover, #f082ac);
}

:where(#mount) button:disabled {
  opacity: 0.6;
  cursor: not-allowed;
}

:where(#mount) select,
:where(#mount) input,
:where(#mount) textarea {
  font-family: inherit;
  color: var(--text-gray-2, #222222);
}

:where(#mount) select {
  appearance: none;
  -webkit-appearance: none;
  -moz-appearance: none;
  font-family: inherit;
  color: var(--text-gray-2, inherit);
  font-size: 13px;
  line-height: 1.3;
  outline: none;
  padding: 8px 16px;
  border: none;
  border-radius: 12px;
  background-color: var(--popup-item-background-color, transparent);
  background-image: var(--icon-xia, none);
  background-repeat: no-repeat;
  background-position: center right 12px;
  background-size: 16px auto;
  cursor: pointer;
}

:where(#mount) input[type="checkbox"] {
  accent-color: var(--primary, #ea4c89);
}

[type="checkbox"],
[type="radio"] {
  -webkit-appearance: none;
  -moz-appearance: none;
  appearance: none;
  width: 1.25em;
  height: 1.25em;
  margin-top: -0.125em;
  margin-right: 0.375em;
  margin-left: 0;
  -webkit-margin-start: 0;
  margin-inline-start: 0;
  -webkit-margin-end: 0.375em;
  margin-inline-end: 0.375em;
  border-width: var(--border-width);
  font-size: inherit;
  vertical-align: middle;
  cursor: pointer;
}
[type="checkbox"]::-ms-check,
[type="radio"]::-ms-check {
  display: none;
}
[type="checkbox"]:checked,
[type="checkbox"]:checked:active,
[type="checkbox"]:checked:focus,
[type="radio"]:checked,
[type="radio"]:checked:active,
[type="radio"]:checked:focus {
  --background-color: var(--primary);
  --border-color: var(--primary);
  background-image: var(--icon-checkbox);
  background-position: center;
  background-size: 0.75em auto;
  background-repeat: no-repeat;
}
[type="checkbox"] ~ label,
[type="radio"] ~ label {
  display: inline-block;
  margin-right: 0.375em;
  margin-bottom: 0;
  cursor: pointer;
}

[type="checkbox"]:indeterminate {
  --background-color: var(--primary);
  --border-color: var(--primary);
  background-image: var(--icon-minus);
  background-position: center;
  background-size: 0.75em auto;
  background-repeat: no-repeat;
}

[type="radio"] {
  border-radius: 50%;
}
[type="radio"]:checked,
[type="radio"]:checked:active,
[type="radio"]:checked:focus {
  --background-color: var(--primary-inverse);
  border-width: 0.35em;
  background-image: none;
}

:where(#mount) [type="checkbox"][role="switch"] {
  --background-color: var(--switch-background-color);
  --border-color: var(--switch-background-color);
  --color: var(--switch-color);
  width: 2.25em;
  height: 1.25em;
  border: var(--border-width) solid var(--border-color);
  border-radius: 1.25em;
  background-color: var(--background-color);
  line-height: 1.25em;
}
:where(#mount) [type="checkbox"][role="switch"]:focus {
  --background-color: var(--switch-background-color);
  --border-color: var(--switch-background-color);
}
:where(#mount) [type="checkbox"][role="switch"]:checked {
  --background-color: var(--switch-checked-background-color);
  --border-color: var(--switch-checked-background-color);
}
:where(#mount) [type="checkbox"][role="switch"]:before {
  display: block;
  width: calc(1.25em - (var(--border-width) * 2));
  height: 100%;
  border-radius: 50%;
  background-color: var(--color);
  content: "";
  transition: margin 0.1s ease-in-out;
}
:where(#mount) [type="checkbox"][role="switch"]:checked {
  background-image: none;
}
:where(#mount) [type="checkbox"][role="switch"]:checked::before {
  margin-left: calc(1.125em - var(--border-width));
  -webkit-margin-start: calc(1.125em - var(--border-width));
  margin-inline-start: calc(1.125em - var(--border-width));
}

:where(#mount) [type="checkbox"][aria-invalid="false"],
:where(#mount) [type="checkbox"]:checked[aria-invalid="false"],
:where(#mount) [type="radio"][aria-invalid="false"],
:where(#mount) [type="radio"]:checked[aria-invalid="false"],
:where(#mount) [type="checkbox"][role="switch"][aria-invalid="false"],
:where(#mount) [type="checkbox"][role="switch"]:checked[aria-invalid="false"] {
  --border-color: var(--form-element-valid-border-color);
}
:where(#mount) [type="checkbox"][aria-invalid="true"],
:where(#mount) [type="checkbox"]:checked[aria-invalid="true"],
:where(#mount) [type="radio"][aria-invalid="true"],
:where(#mount) [type="radio"]:checked[aria-invalid="true"],
:where(#mount) [type="checkbox"][role="switch"][aria-invalid="true"],
:where(#mount) [type="checkbox"][role="switch"]:checked[aria-invalid="true"] {
  --border-color: var(--form-element-invalid-border-color);
}

.text-black {
  color: var(--text-black-2);
}

.text-gray-2 {
  color: var(--text-gray-2);
}

.text-gray-6 {
  color: var(--text-gray-6);
}

.text-gray-9 {
  color: var(--text-gray-9);
}

.text-gray-c2 {
  color: var(--text-gray-c2);
}

.pt-4 {
  padding-top: 16px;
}

.p-2 {
  padding: 8px;
}

.pl-5 {
  padding-left: 48px;
}

.p-0 {
  padding: 0;
}

.pl-2 {
  padding-left: 8px;
}

.pl-4 {
  padding-left: 24px;
}

.pt-2 {
  padding-top: 8px;
}

.pb-2 {
  padding-bottom: 8px;
}

.pb-4 {
  padding-bottom: 16px;
}

.pb-5 {
  padding-bottom: 20px;
}

.pr-5 {
  padding-right: 48px;
}

.text-sm {
  font-size: 13px;
}

.text-base {
  font-size: 16px;
}

.w-full {
  width: 100%;
}

.flex {
  display: flex;
}

.flex-row {
  flex-direction: row;
}

.flex-wrap {
  flex-wrap: wrap;
}

.flex-end {
  justify-content: flex-end;
}

.flex-grow {
  flex-grow: 1;
}

.justify-between {
  justify-content: space-between;
}

.mb-0 {
  margin-bottom: 0px;
}

.mb-2 {
  margin-bottom: 8px;
}

.mb-4 {
  margin-bottom: 16px;
}

.mb-3 {
  margin-bottom: 12px;
}

.inline-block {
  display: inline-block;
}

.py-2 {
  padding-top: 8px;
  padding-bottom: 8px;
}

.py-2-5 {
  padding-top: 6px;
  padding-bottom: 6px;
}

.mt-0 {
  margin-top: 0;
}

.mt-2 {
  margin-top: 8px;
}

.mt-3 {
  margin-top: 12px;
}

.mt-4 {
  margin-top: 16px;
}

.mt-5 {
  margin-top: 20px;
}

.mt-6 {
  margin-top: 24px;
}

.mb-1 {
  margin-bottom: 4px;
}

.ml-4 {
  margin-left: 24px;
}

.ml-3 {
  margin-left: 16px;
}

.ml-2 {
  margin-left: 8px;
}

.ml-1 {
  margin-left: 4px;
}

.mr-1 {
  margin-right: 4px;
}

.mr-2 {
  margin-right: 8px;
}

.mr-3 {
  margin-right: 16px;
}

.mx-2 {
  margin-left: 8px;
  margin-right: 8px;
}

.pl-3 {
  padding-left: 12px;
}

.pr-3 {
  padding-right: 12px;
}

.p-3 {
  padding: 12px;
}

.px-1 {
  padding-left: 4px;
  padding-right: 4px;
}

.px-3 {
  padding-left: 12px;
  padding-right: 12px;
}

.pt-3 {
  padding-top: 12px;
}

.px-6 {
  padding-left: 18px;
  padding-right: 18px;
}

.px-4 {
  padding-left: 16px;
  padding-right: 16px;
}

.pt-6 {
  padding-top: 20px;
}

.py-3 {
  padding-top: 12px;
  padding-bottom: 12px;
}

.py-0 {
  padding-top: 0;
  padding-bottom: 0;
}

.left-auto {
  left: auto !important;
}

.max-h-28 {
  max-height: 112px;
}

.max-h-30 {
  max-height: 120px;
}

.overflow-y-scroll {
  overflow-y: scroll;
}

.text-xs {
  font-size: 12px;
}

.inline-flex {
  display: inline-flex;
}

.flex-1 {
  flex: 1;
}

.flex-3 {
  flex: 3;
}

.flex-4 {
  flex: 4;
}

.flex-2 {
  flex: 2;
}

.items-center {
  align-items: center;
}

.max-content {
  width: max-content;
}

.justify-center {
  justify-content: center;
}

.items-end {
  align-items: flex-end;
}

.items-baseline {
  align-items: baseline;
}

.my-5 {
  margin-top: 48px;
  margin-bottom: 48px;
}

.my-4 {
  margin-top: 24px;
  margin-bottom: 24px;
}

.my-3 {
  margin-top: 16px;
  margin-bottom: 16px;
}

.pt-3 {
  padding-top: 12px;
}

.px-3 {
  padding-left: 12px;
  padding-right: 12px;
}

.pt-2 {
  padding-top: 8px;
}

.px-2 {
  padding-left: 8px;
  padding-right: 8px;
}

.pt-1 {
  padding-top: 4px;
}

.px-1 {
  padding-left: 4px;
  padding-right: 4px;
}

.pb-2 {
  padding-bottom: 8px;
}

.justify-end {
  justify-content: flex-end;
}

.w-auto {
  width: auto;
}

.shrink-0 {
  flex-shrink: 0;
}

.text-right {
  text-align: right;
}

.clickable {
  cursor: pointer;
}

.close {
  cursor: pointer;
  width: 16px;
  height: 16px;
  background-image: var(--icon-close);
  background-position: center;
  background-size: auto 1rem;
  background-repeat: no-repeat;
  opacity: 0.5;
  transition: opacity var(--transition);
}

.padding-two-column {
  padding-left: 40px;
  padding-right: 40px;
}

.muted {
  color: #999;
}

.text-label {
  color: #666;
}

.display-none {
  display: none;
}

/* dark use #18232c */
@media (prefers-color-scheme: dark) {
  .text-label {
    color: #9ca3af;
  }
}

.text-decoration-none {
  text-decoration: none;
}

.text-decoration-none:is([aria-current], :hover, :active, :focus),
[role="link"]:is([aria-current], :hover, :active, :focus) {
  --text-decoration: none !important;
  background-color: transparent !important;
}

.text-overflow-ellipsis {
  text-overflow: ellipsis;
  overflow: hidden;
  white-space: nowrap;
}

.max-w-20 {
  max-width: 180px;
  white-space: nowrap;
}

[data-theme="light"],
#mount:not([data-theme="dark"]) {
  --popup-footer-background-color: #e8eaeb;
  --popup-content-background-color: #ffffff;
  --popup-item-background-color: #f3f5f6;
  --popup-item-hover-background-color: #eaeced;
  --popup-trial-pro-background-color: #f9fbfc;
  --service-select-content-shadow: 0px 2px 12px 0px rgba(75, 76, 77, 0.2);
  --service-select-border-color: #fafafa;
  --service-select-selected-background-color: #f3f5f6;
  --download-app-background: #f3f5f6;
}

[data-theme="dark"] {
  --popup-footer-background-color: #0d0d0d;
  --popup-content-background-color: #191919;
  --popup-item-background-color: #272727;
  --popup-item-hover-background-color: #333333;
  --popup-trial-pro-background-color: #222222;
  --service-select-content-shadow: 0px 2px 12px 0px rgba(0, 0, 0, 0.9);
  --service-select-border-color: #2c2c2c;
  --service-select-selected-background-color: #333333;
  --download-app-background: #333;
}

#mount {
  min-width: 268px;
}

body {
  padding: 0;
  margin: 0 auto;
  min-width: 268px;
  border-radius: 10px;
}

.popup-container {
  font-size: 16px;
  --font-size: 16px;
  color: #666;
  background-color: var(--popup-footer-background-color);
  width: 316px;
  min-width: 316px;
}

.popup-content {
  background-color: var(--popup-content-background-color);
  border-radius: 0px 0px 12px 12px;
  padding: 16px 20px;
}

.immersive-translate-popup-overlay {
  position: fixed;
  top: 0;
  left: 0;
  height: 100%;
  width: 100%;
  touch-action: none;
}

.immersive-translate-popup-wrapper {
  background: var(--background-color);
  border-radius: 10px;
  border: 1px solid var(--muted-border-color);
}

.main-button {
  font-size: 15px;
  vertical-align: middle;
  border-radius: 12px;
  padding: unset;
  height: 44px;
  line-height: 44px;
}

select.language-select,
select.translate-service,
select.min-select {
  --form-element-spacing-horizontal: 0;
  margin-bottom: 0px;
  max-width: unset;
  flex: 1;
  overflow: hidden;
  font-size: 13px;
  border: none;
  border-radius: 8px;
  padding-right: 30px;
  padding-left: 0px;
  background-position: center right 12px;
  background-size: 16px auto;
  background-image: var(--icon-xia);
  text-overflow: ellipsis;
  color: var(--text-gray-2);
  background-color: transparent;
  box-shadow: unset !important;
  cursor: pointer;
}

select.more {
  background-position: center right;
  padding-right: 20px;
}

select.translate-service {
  color: var(--text-black-2);
}

.min-select-container.disabled {
  opacity: 0.5;
  pointer-events: none;
}

.popup-footer {
  background-color: var(--popup-footer-background-color);
  height: 40px;
}

.language-select-container {
  position: relative;
  width: 100%;
  background-color: var(--popup-item-background-color);
  height: 55px;
  border-radius: 12px;
}

select.language-select {
  color: var(--text-black-2);
  font-size: 14px;
  padding: 8px 24px 24px 16px;
  position: absolute;
  border-radius: 12px;
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
}

.language-select-container label {
  position: absolute;
  bottom: 10px;
  left: 16px;
  font-size: 12px;
  color: var(--text-gray-9);
  line-height: 12px;
  margin: 0;
}

.translation-service-container {
  background-color: var(--popup-item-background-color);
  border-radius: 12px;
}

.min-select-container {
  display: flex;
  justify-content: space-between;
  align-items: center;
  height: 44px;
  background-color: var(--popup-item-background-color);
  padding-left: 16px;
}

.min-select-container:first-child {
  border-top-left-radius: 10px;
  border-top-right-radius: 10px;
}

.min-select-container:last-child {
  border-bottom-left-radius: 10px;
  border-bottom-right-radius: 10px;
}

.min-select-container:only-child {
  border-radius: 10px;
}

.translate-mode {
  width: 44px;
  height: 44px;
  border-radius: 22px;
  background-color: var(--popup-item-background-color);
  display: flex;
  align-items: center;
  justify-content: center;
  flex-shrink: 0;
  cursor: pointer;
}

.translate-mode svg {
  fill: var(--text-gray-2);
}

.widgets-container {
  display: flex;
  align-items: stretch;
  justify-content: space-between;
  width: 100%;
  gap: 9px;
}

/* 当只有两个小组件时的样式优化 */
.widgets-container.widgets-two-items {
  gap: 16px;
}

.widgets-container.widgets-two-items .widget-item {
  flex: 0 1 auto;
  min-width: 93px;
  max-width: 120px;
}

.widget-item {
  display: flex;
  max-width: 93px;
  flex-direction: row;
  align-items: center;
  justify-content: center;
  background-color: var(--popup-item-background-color);
  font-size: 12px;
  min-height: 44px;
  height: 100%;
  border-radius: 8px;
  cursor: pointer;
  flex: 1;
  padding: 8px 4px;
  text-align: center;
}

.widget-icon-text {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  color: var(--text-gray-2);
}

.widgets-container svg {
  fill: var(--text-gray-2);
  color: var(--text-gray-2);
}

.share-button-container {
  display: flex;
  align-items: center;
  cursor: pointer;
  padding: 2px 3px 0 8px;
}

.share-button-container svg {
  fill: var(--text-gray-9);
}

.min-select-container:hover,
.language-select-container:hover,
.widget-item:hover,
.translate-mode:hover {
  background-color: var(--popup-item-hover-background-color);
}

.main-button:hover {
  background-color: #f5508f;
}

.share-button-container:hover {
  background-color: var(--popup-item-background-color);
  border-radius: 6px;
}

.error-boundary {
  background: #fff2f0;
  border: 1px solid #ffccc7;
  display: flex;
  padding: 12px;
  font-size: 14px;
  color: rgba(0, 0, 0, 0.88);
  word-break: break-all;
  margin: 12px;
  border-radius: 12px;
  flex-direction: column;
}

.upgrade-pro {
  border-radius: 11px;
  background: linear-gradient(57deg, #272727 19.8%, #696969 82.2%);
  padding: 2px 8px;
  transform: scale(0.85);
}

.upgrade-pro span {
  background: linear-gradient(180deg, #ffeab4 17.65%, #f8c235 85.29%);
  background-clip: text;
  -webkit-background-clip: text;
  -webkit-text-fill-color: transparent;
  font-size: 12px;
  margin-left: 4px;
}

.upgrade-pro svg {
  margin-top: -2px;
}

.upgrade-pro:hover {
  background: linear-gradient(57deg, #3d3d3d 19.8%, #949494 82.2%);
}

.border-bottom-radius-0 {
  border-bottom-left-radius: 0 !important;
  border-bottom-right-radius: 0 !important;
}

.trial-pro-container {
  border-radius: 0px 0px 12px 12px;
  background: var(--popup-trial-pro-background-color);
  display: flex;
  align-items: center;
  height: 44px;
  padding-left: 16px;
  padding-right: 12px;
  font-size: 12px;
}

.trial-pro-container label {
  line-height: 13px;
  color: var(--text-black-2);
}

.trial-pro-container img {
  margin-left: 5px;
}

.cursor-pointer {
  cursor: pointer;
}

.upgrade-pro-discount-act {
  height: 25px;
  display: flex;
  padding: 0 4px;
  align-items: center;
  border-radius: 15px;
  background: linear-gradient(
    90deg,
    #cefbfa 11.33%,
    #d7f56f 63.75%,
    #fccd5e 100%
  );
  transform: scale(0.9);
  box-shadow: 0px 1.8px 3.6px 0px rgba(0, 0, 0, 0.1);
  cursor: pointer;
}

.upgrade-pro-discount-act span {
  font-size: 12px;
  font-weight: 700;
  margin-left: 4px;
  color: #222222;
}

.upgrade-pro-discount-act:hover {
  text-decoration: unset;
  background: linear-gradient(
    90deg,
    #e2fffe 11.33%,
    #e6ff91 63.75%,
    #ffdf93 100%
  );
}

.custom-select-container {
  width: 200px;
  position: relative;
  flex: 1;
}

#translation-service-select {
  padding-right: 12px;
  padding-left: 6px;
}

.custom-select-content {
  border-radius: 12px;
  background: var(--popup-content-background-color);
  box-shadow: var(--service-select-content-shadow);
  border: 1px solid var(--service-select-border-color);
  padding: 4px 5px;
  position: absolute;
  left: -10px;
  right: 0;
  z-index: 100;
  overflow-y: auto;
}

.custom-select-item.default {
  width: 100%;
  padding: 0;
}

.custom-select-item {
  font-size: 13px;
  padding: 5px 6px;
  border-radius: 8px;
  display: flex;
  align-items: center;
  cursor: pointer;
  color: var(--text-black-2);
  width: auto;
  overflow: hidden;
  height: 30px;
  line-height: 30px;
}

.custom-select-item-img {
  width: 20px;
  height: 20px;
  margin-right: 4px;
}

@media (prefers-color-scheme: dark) {
  .custom-select-item-img {
    margin-right: 6px;
  }
}

.custom-select-content .custom-select-item.selected,
.custom-select-content .custom-select-item:hover {
  background: var(--service-select-selected-background-color);
}

.custom-select-item > span {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}

.custom-select-item-pro {
  font-size: 12px;
  margin-left: 6px;
  display: flex;
}

.custom-select-item-pro img {
  margin: 0 3px;
  width: 20px;
  flex-shrink: 0;
}

.custom-select-group-header {
  font-size: 12px;
  font-weight: 500;
  color: var(--text-gray-9);
  padding: 6px 8px 4px;
  margin-top: 2px;
  text-transform: uppercase;
  letter-spacing: 0.5px;
}

.more-container {
  position: relative;
}

.new-menu-indicator {
  position: absolute;
  width: 8px;
  height: 8px;
  background-color: #ef3434;
  border-radius: 50%;
  right: 18px;
  top: 4px;
}

.download-app {
  display: inline-flex;
  align-items: center;
  gap: 4px;
  border-radius: 8px;
  background: var(--download-app-background);
  padding: 4px 8px;
  color: var(--text-gray-6);
  font-size: 12px;
  cursor: pointer;
  transition: all 0.2s ease-in-out;
}

/* Popup 动画效果 */
@keyframes popup-fade-in {
  from {
    opacity: 0;
    transform: translateY(10px) scale(0.95);
  }
  to {
    opacity: 1;
    transform: translateY(0) scale(1);
  }
}

@keyframes popup-fade-out {
  from {
    opacity: 1;
    transform: translateY(0) scale(1);
  }
  to {
    opacity: 0;
    transform: translateY(10px) scale(0.95);
  }
}

.popup-generic-content {
  animation: popup-fade-in 0.2s ease-out;
}

.popup-generic-content.hiding {
  animation: popup-fade-out 0.15s ease-in;
}

select.min-select {
  --form-element-spacing-horizontal: 0;
  max-width: 128px;
  overflow: hidden;
  color: var(--primary);
  font-size: 13px;
  border: none;
  padding: 0;
  padding-right: 20px;
  text-overflow: ellipsis;
  color: var(--color);
}
select.min-select-secondary {
  color: var(--color);
}
select.min-select:focus {
  outline: none;
  border: none;
  --box-shadow: none;
}

select.min-select-left {
  padding-right: 0px;
  /* padding-left: 24px; */
  /* background-position: center left 0; */
  text-overflow: ellipsis;
  text-align: left;
}

select.transform-padding-left {
  padding-left: 12px;
  transform: translateX(-12px);
  background-position: center right 0px;
}

select.text-gray-6 {
  color: var(--text-gray-6);
}

/* dark use black, for windows */
@media (prefers-color-scheme: dark) {
  select.language-select option,
  select.translate-service option,
  select.min-select option {
    background-color: #666666;
  }
}

select.min-select-no-arrow {
  background-image: none;
  padding-right: 0;
}



.activity-tips {
  border-radius: 8px;
  padding: 0px 8px;
  min-height: 28px;
  background: linear-gradient(83deg, #FACCDE -0.87%, #FCE7EF 43.13%, #FBD6E4 72.08%, #FFB3D1 96.34%);  gap: 2px;
  color: #333;
  cursor: pointer;
  gap: 4px;
}

.activity-tips-icon {
  width: 18px;
  height: 18px;
  flex-shrink: 0;
}

.countdown-container {
  min-width: 50px;
  text-align: left;
  font-weight: 600;
  font-size: 12px;
  letter-spacing: 0.01em;
}

.activity-tips-text {
  font-weight: 600;
  max-width: 100px;
  overflow: hidden;
  text-overflow: ellipsis;
  white-space: nowrap;
}

html {
  font-size: 17px;
}

@media print {
  .imt-fb-container {
    display: none !important;
  }
}

#mount {
  position: absolute;
  display: none;
  min-width: 250px;
  height: auto;
  --font-size: 17px;
  font-size: 17px;
}

/* float-ball */
.imt-fb-container {
  position: fixed;
  padding: 0;
  top: 335px;
  width: fit-content;
  display: flex;
  flex-direction: column;
  display: none;
  direction: ltr;
  background-color: transparent !important;
}

.imt-fb-container:hover .imt-manga-button {
  opacity: 1;
}

.imt-fb-container.left {
  align-items: flex-start;
  left: 0;
}

.imt-fb-container.right {
  align-items: flex-end;
  right: 0;
}

.imt-fb-btn {
  cursor: pointer;
  background: var(--float-ball-more-button-background-color);
  height: 36px;
  box-shadow: 0 0 10px 0 rgba(0, 0, 0, 0.08);
}

.imt-fb-btn.left {
  border-top-right-radius: 36px;
  border-bottom-right-radius: 36px;
  padding-left: 3px;
}

.imt-fb-btn.right {
  border-top-left-radius: 36px;
  border-bottom-left-radius: 36px;
  padding-right: 3px;
}

.imt-fb-btn.mini.left {
  transform: translateX(-22px);
}

.imt-fb-btn.mini.right {
  transform: translateX(22px);
}

.imt-fb-btn.mini.active {
  transform: translateX(0);
}

.imt-fb-btn div {
  height: 36px;
  display: inline-flex;
  align-items: center;
  position: relative;
}

.imt-fb-btn .imt-fb-hit-area {
  position: absolute;
  top: 0;
  height: 36px;
  bottom: -6px;
  left: -6px;
  right: -6px;
  background: transparent;
}

.imt-fb-btn.mini.right .imt-fb-hit-area {
  left: -10px;
}

.imt-fb-btn.mini.left .imt-fb-hit-area {
  right: -10px;
}

.imt-fb-btn.left div {
  border-top-right-radius: 34px;
  border-bottom-right-radius: 34px;
  justify-content: flex-end;
}

.imt-fb-btn.right div {
  border-top-left-radius: 34px;
  border-bottom-left-radius: 34px;
}

.imt-fb-logo-img {
  width: 20px;
  height: 20px;
  margin: 0 10px;
}

.imt-fb-logo-img-big-bg {
  width: 28px;
  height: 28px;
  margin: 0;
  background-color: #ed6d8f;
  border-radius: 50%;
  margin: 0 5px;
}

.imt-float-ball-translated {
  position: absolute;
  width: 11px;
  height: 11px;
  bottom: 4px;
  right: 4px;
}

.btn-animate {
  -webkit-transform: translate3d(0, 0, 0);
  transform: translate3d(0, 0, 0);
  -webkit-transition: -webkit-transform ease-out 250ms;
  transition: -webkit-transform ease-out 250ms;
  transition: transform ease-out 250ms;
  transition: transform ease-out 250ms, -webkit-transform ease-out 250ms;
}

.imt-fb-setting-btn {
  margin-right: 18px;
  width: 28px;
  height: 28px;
}

.immersive-translate-popup-wrapper {
  background: var(--background-color);
  border-radius: 20px;
  box-shadow: 2px 10px 24px 0px #0e121614;
  border: none;
}

.popup-container {
  border-radius: 20px;
}

.popup-content {
  border-radius: 20px 20px 12px 12px;
}
.popup-footer {
  border-radius: 20px;
}

.imt-fb-close-button {
  cursor: pointer;
  position: absolute;
  left: -10px;
  bottom: -20px;
}
.imt-fb-container.left .imt-fb-close-button {
  left: auto;
  right: -10px;
}

.imt-fb-close-content {
  padding: 22px;
  width: 320px;
  pointer-events: all;
}

.imt-fb-close-title {
  font-weight: 500;
  color: var(--text-black-2);
}

.imt-fb-close-radio-content {
  background-color: var(--background-light-green);
  padding: 8px 20px;
}

.imt-fb-radio-sel,
.imt-fb-radio-nor {
  width: 16px;
  height: 16px;
  border-radius: 8px;
  flex-shrink: 0;
}

.imt-fb-radio-sel {
  border: 2px solid var(--primary);
  display: flex;
  align-items: center;
  justify-content: center;
}

.imt-fb-radio-sel div {
  width: 8px;
  height: 8px;
  border-radius: 4px;
  background-color: var(--primary);
}

.imt-fb-radio-nor {
  border: 2px solid var(--input-border-strong);
}

.imt-fb-primary-btn {
  background-color: var(--primary);
  width: 72px;
  height: 32px;
  color: white;
  border-radius: 8px;
  text-align: center;
  line-height: 32px;
  font-size: 16px;
  cursor: pointer;
}

.imt-fb-default-btn {
  border: 1px solid var(--primary);
  width: 72px;
  height: 32px;
  border-radius: 8px;
  color: var(--primary);
  line-height: 32px;
  text-align: center;
  font-size: 16px;
  cursor: pointer;
}

.imt-fb-guide-container {
  width: 312px;
  transform: translateY(-45%);
}

.imt-fb-guide-bg {
  position: absolute;
  left: 30px;
  right: 0;
  top: 0;
  bottom: 0;
  z-index: -1;
  height: 100%;
  width: 90%;
}

.imt-fb-guide-bg.left {
  transform: scaleX(-1);
}

.imt-fb-guide-content {
  margin: 16px -30px 80px 0px;
  display: flex;
  flex-direction: column;
  align-items: center;
}

.imt-fb-guide-content.left {
  margin: 16px 21px 60px 32px;
}

.imt-fb-guide-img {
  width: 220px;
  height: 112px;
}

.imt-fb-guide-message {
  font-size: 14px;
  line-height: 28px;
  color: #333333;
  white-space: pre-wrap;
  text-align: center;
  font-weight: 700;
  margin-bottom: 20px;
}

.imt-manga-guide-message {
  font-size: 16px;
  line-height: 24px;
  color: #333333;
  text-align: center;
  font-weight: 500;
  margin-bottom: 12px;
}

.imt-fb-guide-button {
  margin-top: 16px;
  line-height: 40px;
  height: 40px;
  padding: 0 20px;
  width: unset;
}

.imt-fb-side {
  border-radius: 50%;
  cursor: pointer;
  pointer-events: all;
  position: relative;
}

.imt-fb-side {
  margin: 10px 0;
}

.imt-fb-side::before,
.imt-manga-button::before,
.imt-fb-round-button::before {
  content: "";
  position: absolute;
  top: 2px;
  right: 2px;
  bottom: 2px;
  left: 2px;
  border-radius: 50%;
  background-color: transparent;
  transition: background-color 0.15s ease;
}

.imt-fb-side:hover::before,
.imt-manga-button:hover::before,
.imt-fb-round-button:hover::before {
  background-color: var(--float-ball-more-button-hover-color);
}

.imt-fb-new-badge {
  width: 26px;
  height: 14px;
  padding: 3px;
  background-color: #f53f3f;
  border-radius: 4px;
  position: absolute;
  top: -5px;
  right: 15px;
  display: flex;
  align-items: center;
  justify-content: center;
}

.imt-fb-side * {
  pointer-events: all;
}

.imt-fb-pin-icon {
  position: absolute;
  left: -16px;
  bottom: -12px;
  opacity: 0;
  cursor: pointer;
}
.imt-fb-container.left .imt-fb-pin-icon {
  left: auto;
  right: -16px;
}

.imt-fb-side:hover .imt-fb-pin-icon {
  opacity: 1;
}

.imt-fb-more-button {
  width: 36px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
}
/* Sheet.css */
.immersive-translate-sheet {
  position: fixed;
  transform: translateY(100%);
  /* Start off screen */
  left: 0;
  right: 0;
  background-color: var(--background-color);
  transition: transform 0.3s ease-out;
  /* Smooth slide transition */
  box-shadow: 0px -2px 10px rgba(0, 0, 0, 0.1);
  /* Ensure it's above other content */
  bottom: 0;
  border-top-left-radius: 16px;
  border-top-right-radius: 16px;
  overflow: hidden;
}

.immersive-translate-sheet.visible {
  transform: translateY(0);
}

.immersive-translate-sheet-backdrop {
  position: fixed;
  top: 0;
  left: 0;
  right: 0;
  bottom: 0;
  background-color: rgba(0, 0, 0, 0.5);
  opacity: 0;
  transition: opacity 0.3s ease-out;
}

.immersive-translate-sheet-backdrop.visible {
  opacity: 1;
}

.popup-container-sheet {
  max-width: 100vw;
  width: 100vw;
}

.imt-no-events svg * {
  pointer-events: none !important;
}

.imt-manga-button {
  width: 36px;
  height: 36px;
  display: flex;
  flex-direction: column;
  position: relative;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  pointer-events: all;
  margin: 0 0 10px 0;
  background-color: var(--float-ball-more-button-background-color);
  border-radius: 50%;
  box-shadow: 0 2px 10px 0 rgba(0, 0, 0, 0.08);
  right: 8px;
}

.imt-manga-feedback {
  cursor: pointer;
  margin-bottom: 10px;
}

.imt-fb-round-button {
  width: 36px;
  height: 36px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  background: var(--float-ball-more-button-background-color);
  border-radius: 50%;
  box-shadow: 0 0 10px 0 rgba(0, 0, 0, 0.08);
  position: relative;
}

.imt-fb-round-button > svg {
  z-index: 1;
}

.imt-fb-upgrade-button {
  cursor: pointer;
  margin-top: 10px;
}

.imt-manga-translated {
  position: absolute;
  left: 24px;
  top: 20px;
}

.imt-float-ball-loading {
  animation: imt-loading-animation 0.6s infinite linear !important;
}

.imt-manga-guide-bg {
  position: absolute;
  left: 0;
  right: 0;
  top: 0;
  bottom: 0;
  z-index: -1;
  width: 100%;
  transform: translateY(-50%);
}
.imt-manga-guide-content {
  position: absolute;
  top: 15px;
  left: 0;
  right: 0;
  margin: 0 40px 0;
}

.img-manga-guide-button {
  width: fit-content;
  margin: 0 auto;
}

.img-manga-close {
  position: absolute;
  bottom: -200px;
  width: 32px;
  height: 32px;
  left: 0;
  right: 0;
  margin: auto;
  cursor: pointer;
}

.imt-fb-container.dragging .imt-manga-button,
.imt-fb-container.dragging .btn-animate:not(.imt-fb-btn) {
  display: none !important;
}

.imt-fb-container.dragging .imt-fb-btn {
  border-radius: 50% !important;
  width: 36px !important;
  height: 36px !important;
  display: flex !important;
  align-items: center !important;
  cursor: move !important;
}

.imt-fb-container.dragging .imt-fb-btn div {
  border-radius: 50% !important;
  width: 36px !important;
  height: 36px !important;
  display: flex !important;
  align-items: center !important;
  justify-content: center !important;
  margin: 0 !important;
}

.imt-fb-container.dragging .imt-fb-btn.left,
.imt-fb-container.dragging .imt-fb-btn.right {
  border-radius: 50% !important;
}

.imt-fb-container.dragging .imt-fb-btn.left div,
.imt-fb-container.dragging .imt-fb-btn.right div {
  border-radius: 50% !important;
}

.imt-fb-container.dragging .imt-fb-logo-img {
  margin: 0 !important;
}

.imt-fb-container.dragging .imt-float-ball-translated {
  right: 2px !important;
  bottom: 2px !important;
}

@-webkit-keyframes imt-loading-animation {
  from {
    -webkit-transform: rotate(0deg);
  }

  to {
    -webkit-transform: rotate(359deg);
  }
}

@keyframes imt-loading-animation {
  from {
    transform: rotate(0deg);
  }

  to {
    transform: rotate(359deg);
  }
}

.imt-fb-icon {
  color: #666666;
}

[data-theme="dark"] .imt-fb-icon {
  color: #b3b3b3;
}

[data-theme="light"] .imt-fb-icon {
  color: #666666;
}

/* 悬浮球设置弹窗样式 */
.imt-fb-setting-wrapper {
  border-radius: 12px;
  box-shadow: 2px 10px 24px 0px rgba(0, 0, 0, 0.06);
}

.imt-fb-setting-content {
  padding: 16px;
  width: 350px;
  pointer-events: all;
}

.imt-fb-setting-title {
  font-weight: 700;
  font-size: 16px;
  line-height: 24px;
  color: var(--text-black-2);
}

.imt-fb-setting-preview {
  display: flex;
  justify-content: center;
  align-items: center;
  margin: 24px 0;
}

.imt-fb-preview-wrapper {
  display: flex;
  border-radius: 12px;
  overflow: hidden;
}

.imt-fb-preview-item {
  width: 99px;
  height: 57px;
  display: flex;
  align-items: center;
  justify-content: flex-end;
  position: relative;
  overflow: hidden;
}

.imt-fb-preview-item.light {
  background-color: #f3f5f6;
  border-top-left-radius: 12px;
  border-bottom-left-radius: 12px;
}

.imt-fb-preview-item.dark {
  background-color: #444;
  border-top-right-radius: 12px;
  border-bottom-right-radius: 12px;
}

.imt-fb-preview-ball {
  height: 36px;
  display: flex;
  align-items: center;
  box-shadow: 0 0 10px 0 rgba(0, 0, 0, 0.08);
  transition: all 0.2s ease;
}

.imt-fb-preview-ball.light {
  background-color: #fff;
  border-top-left-radius: 36px;
  border-bottom-left-radius: 36px;
}

.imt-fb-preview-ball.dark {
  background-color: #111;
  border-top-left-radius: 36px;
  border-bottom-left-radius: 36px;
}

.imt-fb-preview-ball.full {
  width: 41px;
  justify-content: center;
}

.imt-fb-preview-ball.mini {
  width: 41px;
  justify-content: center;
  transform: translateX(21px);
}

.imt-fb-preview-ball .imt-fb-preview-logo {
  flex-shrink: 0;
}

/* 选项区域 */
.imt-fb-setting-options {
  margin-top: 0;
  display: flex;
  flex-direction: column;
  gap: 16px;
}

.imt-fb-setting-option-row {
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 0 4px;
}

.imt-fb-setting-option-label {
  font-size: 14px;
  font-weight: 400;
  line-height: 21px;
  color: var(--text-black-2);
}

/* Toggle 开关样式 */
.imt-fb-toggle {
  width: 30px;
  height: 18px;
  background-color: var(--switch-background-color);
  border-radius: 24px;
  position: relative;
  cursor: pointer;
  transition: background-color 0.2s ease;
  flex-shrink: 0;
}

.imt-fb-toggle.checked {
  background-color: var(--primary);
}

.imt-fb-toggle.disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.imt-fb-toggle-thumb {
  width: 14px;
  height: 14px;
  background-color: var(--switch-color);
  border-radius: 50%;
  position: absolute;
  top: 2px;
  left: 2px;
  transition: transform 0.2s ease;
  box-shadow: 0 0 10px 0 rgba(0, 0, 0, 0.08);
}

.imt-fb-toggle.checked .imt-fb-toggle-thumb {
  transform: translateX(12px);
}

.imt-fb-setting-hide-options {
  background-color: var(--background-light-green);
  padding: 8px 20px;
  border-radius: 8px;
  margin-top: 8px;
}

.imt-fb-setting-save-btn {
  background-color: var(--primary);
  color: white;
  border-radius: 8px;
  text-align: center;
  line-height: 45px;
  height: 45px;
  font-size: 14px;
  font-weight: 500;
  cursor: pointer;
  margin-top: 16px;
  transition: opacity 0.2s ease;
}

.imt-fb-setting-save-btn:hover {
  opacity: 0.9;
}

.imt-fb-setting-save-btn:active {
  opacity: 0.8;
}
</style><div id="mount" style="display: block;"><div class="imt-fb-container right notranslate " data-theme="light" style="z-index: 2147483637; pointer-events: none; right: 0px; top: 562px; display: flex;"><div class="btn-animate" style="transform: translateX(-5px); opacity: 0.7;"><div class="imt-fb-btn imt-fb-more-button imt-fb-side"><div class=" btn-animate" style="position: relative; pointer-events: all; display: inline-block;"><div><svg class="imt-fb-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"><path d="M9.65332 13.3477C10.4097 13.4245 11 14.0632 11 14.8398V18.8398L10.9922 18.9932C10.9205 19.6992 10.3593 20.2604 9.65332 20.332L9.5 20.3398H5.5C4.72334 20.3398 4.08461 19.7496 4.00781 18.9932L4 18.8398V14.8398C4 14.0114 4.67157 13.3398 5.5 13.3398H9.5L9.65332 13.3477ZM18.6533 13.3477C19.4097 13.4245 20 14.0632 20 14.8398V18.8398L19.9922 18.9932C19.9205 19.6992 19.3593 20.2604 18.6533 20.332L18.5 20.3398H14.5C13.7233 20.3398 13.0846 19.7496 13.0078 18.9932L13 18.8398V14.8398C13 14.0114 13.6716 13.3398 14.5 13.3398H18.5L18.6533 13.3477ZM5.5 14.3398C5.22386 14.3398 5 14.5637 5 14.8398V18.8398C5 19.116 5.22386 19.3398 5.5 19.3398H9.5C9.77614 19.3398 10 19.116 10 18.8398V14.8398C10 14.5637 9.77614 14.3398 9.5 14.3398H5.5ZM14.5 14.3398C14.2239 14.3398 14 14.5637 14 14.8398V18.8398C14 19.116 14.2239 19.3398 14.5 19.3398H18.5C18.7761 19.3398 19 19.116 19 18.8398V14.8398C19 14.5637 18.7761 14.3398 18.5 14.3398H14.5ZM9.65332 4.34766C10.4097 4.42445 11 5.06318 11 5.83984V9.83984L10.9922 9.99316C10.9205 10.6992 10.3593 11.2604 9.65332 11.332L9.5 11.3398H5.5C4.72334 11.3398 4.08461 10.7496 4.00781 9.99316L4 9.83984V5.83984C4 5.01142 4.67157 4.33984 5.5 4.33984H9.5L9.65332 4.34766ZM16.1484 3.18848C16.2521 2.83834 16.7479 2.83834 16.8516 3.18848L17.3281 4.79785C17.5079 5.40507 17.9648 5.89104 18.5596 6.1084L20.0576 6.65625C20.3786 6.77361 20.3785 7.22732 20.0576 7.34473L18.5596 7.89258C17.9648 8.10991 17.5079 8.59598 17.3281 9.20312L16.8516 10.8125C16.7479 11.1626 16.2521 11.1626 16.1484 10.8125L15.6719 9.20312C15.4921 8.59598 15.0352 8.10991 14.4404 7.89258L12.9424 7.34473C12.6215 7.22732 12.6214 6.77361 12.9424 6.65625L14.4404 6.1084C15.0352 5.89104 15.4921 5.40507 15.6719 4.79785L16.1484 3.18848ZM5.5 5.33984C5.22386 5.33984 5 5.5637 5 5.83984V9.83984C5 10.116 5.22386 10.3398 5.5 10.3398H9.5C9.77614 10.3398 10 10.116 10 9.83984V5.83984C10 5.5637 9.77614 5.33984 9.5 5.33984H5.5Z" fill="#ED6D8F"></path></svg><svg class="imt-fb-pin-icon" xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 14 14" fill="none"><circle cx="7" cy="7" r="7" fill="#B1B1B166"></circle><g transform="translate(2, 2)"><mask id="mask0_39788_745" maskUnits="userSpaceOnUse" x="0" y="0" width="10" height="10" style="mask-type: alpha;"><rect width="10" height="10" fill="#D9D9D9"></rect></mask><g mask="url(#mask0_39788_745)"><path d="M6.23958 3.76042C6.28819 3.80903 6.35243 3.88194 6.43229 3.97917C6.51215 4.07639 6.59722 4.18056 6.6875 4.29167C6.6875 4.44444 6.63646 4.57292 6.53438 4.67708C6.43222 4.78125 6.30618 4.83333 6.15625 4.83333C6.00056 4.83333 5.86962 4.78125 5.76344 4.67708C5.65726 4.57292 5.60417 4.44444 5.60417 4.29167C5.59722 4.28472 5.59201 4.28646 5.58854 4.29688C5.58507 4.30729 5.58333 4.31076 5.58333 4.30729C5.58333 4.30382 5.59028 4.30382 5.60417 4.30729C5.61806 4.31076 5.62153 4.30903 5.61458 4.30208C5.47569 4.30208 5.3559 4.25104 5.25521 4.14896C5.15451 4.04681 5.10417 3.92771 5.10417 3.79167C5.10417 3.65278 5.15451 3.53299 5.25521 3.43229C5.3559 3.3316 5.47569 3.28125 5.61458 3.28125C5.74653 3.35764 5.86111 3.43403 5.95833 3.51042C6.05556 3.58681 6.14931 3.67014 6.23958 3.76042ZM5 2.75C4.94667 2.75 4.89465 2.75017 4.84396 2.75052C4.79326 2.75087 4.74083 2.75035 4.68667 2.74896C4.54139 2.7566 4.39931 2.72049 4.26042 2.64062C4.12153 2.56076 4.03472 2.44618 4 2.29688C3.96528 2.14757 3.98611 2.01389 4.0625 1.89583C4.13889 1.77778 4.25267 1.71094 4.40385 1.69531C4.50962 1.6901 4.61059 1.68576 4.70677 1.68229C4.80295 1.67882 4.90069 1.67708 5 1.67708C5.97917 1.67708 6.88368 1.93056 7.71354 2.4375C8.5434 2.94444 9.17059 3.64625 9.5951 4.54292C9.62892 4.61847 9.65451 4.69451 9.67188 4.77104C9.68924 4.84757 9.69792 4.92389 9.69792 5C9.69792 5.07611 9.69097 5.15417 9.67708 5.23417C9.66319 5.31417 9.64236 5.39236 9.61458 5.46875C9.49826 5.68972 9.36504 5.89948 9.2149 6.09802C9.06469 6.29656 8.90278 6.49306 8.72917 6.6875C8.63194 6.79861 8.51042 6.85069 8.36458 6.84375C8.21875 6.83681 8.09375 6.78069 7.98958 6.67542C7.88542 6.57007 7.83681 6.44507 7.84375 6.30042C7.85069 6.15569 7.90278 6.02431 8 5.90625C8.13194 5.76736 8.25125 5.62153 8.35792 5.46875C8.46465 5.31597 8.5641 5.15972 8.65625 5C8.30208 4.30556 7.79861 3.75694 7.14583 3.35417C6.49306 2.95139 5.77778 2.75 5 2.75ZM5 8.32292C4.04861 8.32292 3.1684 8.07465 2.35938 7.57812C1.55035 7.0816 0.919896 6.40906 0.468021 5.56052C0.419896 5.46462 0.387153 5.37142 0.369792 5.28094C0.352431 5.19052 0.34375 5.09677 0.34375 4.99969C0.34375 4.90267 0.352431 4.80556 0.369792 4.70833C0.387153 4.61111 0.420139 4.51736 0.46875 4.42708C0.614583 4.13542 0.787361 3.86813 0.987083 3.62521C1.18688 3.38229 1.40646 3.15306 1.64583 2.9375L0.96875 2.27083C0.871528 2.18056 0.821181 2.07812 0.817708 1.96354C0.814236 1.84896 0.861111 1.74306 0.958333 1.64583C1.05556 1.54861 1.16319 1.5 1.28125 1.5C1.39931 1.5 1.50694 1.54861 1.60417 1.64583L8.19792 8.23958C8.29514 8.33681 8.34201 8.44271 8.33854 8.55729C8.33507 8.67188 8.28819 8.77431 8.19792 8.86458C8.10069 8.96181 7.99479 9.01042 7.88021 9.01042C7.76562 9.01042 7.65972 8.96181 7.5625 8.86458L6.70833 8.01042C6.43472 8.12153 6.15382 8.20139 5.86562 8.25C5.57743 8.29861 5.28889 8.32292 5 8.32292ZM2.41667 3.6875C2.20139 3.88194 2.00347 4.08507 1.82292 4.29688C1.64236 4.50868 1.48611 4.74306 1.35417 5C1.69444 5.70139 2.19271 6.25174 2.84896 6.65104C3.50521 7.05035 4.22222 7.25 5 7.25C5.14729 7.25 5.29347 7.24132 5.43854 7.22396C5.58368 7.2066 5.72569 7.18056 5.86458 7.14583L5.39583 6.67708C5.33333 6.70486 5.26736 6.72396 5.19792 6.73438C5.12847 6.74479 5.0625 6.75 5 6.75C4.51389 6.75 4.10069 6.57986 3.76042 6.23958C3.42014 5.89931 3.25 5.48611 3.25 5C3.25 4.9375 3.25694 4.87153 3.27083 4.80208C3.28472 4.73264 3.29514 4.66667 3.30208 4.60417L2.41667 3.6875Z" fill="white"></path></g></g></svg></div></div></div></div><div hidden="" class="imt-no-events " id="manga-button" style="position: relative; opacity: 0.7;"><div class="imt-manga-button btn-animate" style="transform: translateX(2px);"><div style="position: relative;"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 32 32" fill="none" style="vertical-align: middle;"><path d="M14.8853 4.92364C14.8853 4.92364 16.3905 10.4362 22.6668 4C22.6668 4 20.3381 10.8907 25.3364 10.0843C25.3364 10.0843 22.0563 15.6994 29 18.0599C29 18.0599 22.9934 19.306 21.1617 28C21.1617 28 17.7679 24.54 14.8853 27.3549C14.8853 27.3549 13.3233 23.5724 7.33097 26.27C7.33097 26.27 10.1141 20.6549 4.83179 21.0507C4.83179 21.0507 7.16057 18.8955 3 15.9047C3 15.9047 7.50137 16.1833 6.33697 11.7117C6.33697 11.7117 10.0005 12.3421 8.66576 6.82957C8.65156 6.81491 12.4855 9.80574 14.8853 4.92364Z" fill="#ED6D8F"></path><path d="M20.8599 13.7022C20.885 13.1361 20.9543 12.5713 20.9959 12.0052C21.0337 11.568 20.8107 11.2794 20.3876 11.18C20.0759 11.1013 19.7508 11.0867 19.433 11.137C19.1951 11.1945 18.9542 11.2396 18.7113 11.2721C18.2403 11.3028 17.9973 11.5275 17.9796 11.988C17.977 12.0833 17.9596 12.1777 17.928 12.268C17.3034 13.9102 16.6774 15.5499 16.0503 17.1873C16.0301 17.2401 16.0062 17.2904 15.9671 17.3776C15.7291 16.8975 15.4281 16.4898 15.2745 15.9986C14.8073 14.5152 14.3186 13.033 13.8312 11.5594C13.6826 11.1112 13.3489 10.9344 12.8754 11.0216C12.7889 11.0365 12.7008 11.0398 12.6134 11.0314C12.2241 10.9938 11.8311 11.0404 11.4623 11.1677C11.0946 11.2991 10.9498 11.557 11.0152 11.9254C11.0428 12.0371 11.0643 12.1503 11.0795 12.2643C11.1223 13.1902 11.1777 14.1087 11.2054 15.0321C11.257 16.7992 11.2117 18.5651 11.0858 20.3284C11.0644 20.6354 11.0304 20.9424 11.0228 21.2494C11.0115 21.6092 11.1613 21.7811 11.5266 21.8143C11.9976 21.8573 12.4711 21.8708 12.9421 21.9088C13.0309 21.9201 13.121 21.9003 13.1962 21.8528C13.2714 21.8053 13.3268 21.7334 13.3527 21.6497C13.3996 21.5394 13.4252 21.4216 13.4282 21.3022C13.4295 20.8258 13.4207 20.3493 13.4081 19.8741C13.393 19.3264 13.3917 18.7763 13.3438 18.231C13.2857 17.5839 13.266 16.934 13.2847 16.2847C13.2847 16.2466 13.291 16.2073 13.2985 16.1312C13.3338 16.2024 13.3514 16.2356 13.3665 16.2712C13.9017 17.5228 14.3617 18.8037 14.7443 20.1074C14.7928 20.2421 14.7928 20.3889 14.7443 20.5237C14.6322 20.8196 14.7141 21.037 14.9659 21.1377C15.4445 21.3268 15.9331 21.4926 16.4155 21.6731C16.4865 21.7033 16.566 21.7091 16.6408 21.6895C16.7157 21.6698 16.7815 21.6259 16.8273 21.565C16.9085 21.4643 16.9743 21.3526 17.0225 21.2335C17.0537 21.1374 17.0798 21.0399 17.1006 20.9412C17.3185 20.2425 17.5653 19.5499 17.7517 18.8438C17.9785 17.9723 18.2624 17.1158 18.6018 16.2798C18.6201 16.2439 18.6411 16.2094 18.6647 16.1766C18.6761 16.2319 18.6761 16.254 18.6761 16.2761C18.6345 17.59 18.5955 18.8978 18.5501 20.2056C18.5363 20.5949 18.491 20.9829 18.4809 21.3722C18.4721 21.705 18.6207 21.8708 18.9557 21.9002C19.4355 21.9432 19.9191 21.9592 20.4002 21.9973C20.4888 22.0079 20.5784 21.9875 20.653 21.9399C20.7277 21.8922 20.7827 21.8203 20.8082 21.7369C20.8531 21.6305 20.8766 21.5167 20.8775 21.4017C20.88 20.7668 20.8674 20.132 20.8674 19.4971C20.8662 19.2846 20.8687 19.0722 20.8523 18.8622C20.8158 18.3968 20.7264 17.9314 20.7339 17.4685C20.7515 16.2122 20.8044 14.9572 20.8599 13.7022Z" fill="white"></path></svg></div></div></div><div style="display: flex; align-items: center; flex-direction: row; gap: 8px;"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 14 14" fill="none" style="display: block; opacity: 0;"><g clip-path="url(#clip0_38845_9816)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14Z" fill="#B1B1B1" fill-opacity="0.4"></path><path d="M6.11754 7.00052L4.18254 5.06452C4.06786 4.94725 4.00406 4.78948 4.00498 4.62546C4.00589 4.46144 4.07145 4.3044 4.18744 4.18841C4.30342 4.07243 4.46046 4.00687 4.62448 4.00595C4.7885 4.00503 4.94627 4.06884 5.06354 4.18352L6.99954 6.11852L8.93554 4.18352C8.99315 4.1246 9.06188 4.07771 9.13774 4.04554C9.2136 4.01337 9.29508 3.99656 9.37748 3.9961C9.45988 3.99564 9.54155 4.01153 9.61777 4.04285C9.69398 4.07417 9.76323 4.1203 9.82149 4.17857C9.87976 4.23683 9.92589 4.30608 9.95721 4.38229C9.98853 4.45851 10.0044 4.54018 10.004 4.62258C10.0035 4.70497 9.98669 4.78646 9.95452 4.86232C9.92235 4.93818 9.87545 5.00691 9.81654 5.06452L7.88154 7.00052L9.81654 8.93652C9.93122 9.05379 9.99502 9.21155 9.99411 9.37557C9.99319 9.5396 9.92763 9.69664 9.81164 9.81262C9.69566 9.9286 9.53862 9.99417 9.3746 9.99508C9.21058 9.996 9.05281 9.9322 8.93554 9.81752L6.99954 7.88252L5.06354 9.81752C5.00593 9.87643 4.93721 9.92333 4.86135 9.9555C4.78549 9.98767 4.704 10.0045 4.6216 10.0049C4.5392 10.0054 4.45753 9.9895 4.38131 9.95818C4.3051 9.92686 4.23585 9.88073 4.17759 9.82247C4.11932 9.7642 4.0732 9.69496 4.04188 9.61874C4.01056 9.54253 3.99467 9.46086 3.99513 9.37846C3.99559 9.29606 4.01239 9.21457 4.04456 9.13871C4.07673 9.06285 4.12363 8.99413 4.18254 8.93652L6.11754 7.00052Z" fill="white"></path></g><defs><clippath id="clip0_38845_9816"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg><div style="position: relative;"><div class="imt-fb-btn right btn-animate  " dir="ltr" style="opacity: 0.7;"><div class=" " style="position: relative; pointer-events: all; display: inline-block;"><div><div class="imt-fb-btn-content"><svg class="imt-fb-logo-img imt-fb-logo-img-big-bg" width="28" height="28" viewBox="0 0 28 28" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M0 14C0 6.26801 6.26801 0 14 0C21.732 0 28 6.26801 28 14C28 21.732 21.732 28 14 28C6.26801 28 0 21.732 0 14Z" fill="#ED6D8F"></path><path d="M7.30921 15.6381C7.53044 15.6381 7.70979 15.8175 7.70979 16.0387V19.0088C7.70979 19.934 8.45993 20.684 9.38526 20.684H11.7557C11.9769 20.684 12.1562 20.8634 12.1562 21.0846V22.152C12.1562 22.3732 11.9769 22.5526 11.7557 22.5526H9.38526C7.42782 22.5526 5.84099 20.966 5.84099 19.0088V16.0387C5.84099 15.8175 6.02034 15.6381 6.24157 15.6381H7.30921ZM18.9876 12.6944C19.1515 12.6944 19.2989 12.7943 19.3597 12.9466L22.9711 22.0036C23.076 22.2667 22.8822 22.5526 22.599 22.5526H21.4672C21.3016 22.5526 21.153 22.4506 21.0934 22.2961L20.2724 20.1673C20.2128 20.0128 20.0643 19.9108 19.8986 19.9108H16.7508C16.5852 19.9108 16.4366 20.0128 16.377 20.1673L15.556 22.2961C15.4964 22.4506 15.3478 22.5526 15.1822 22.5526H14.0504C13.7672 22.5526 13.5734 22.2667 13.6783 22.0036L17.2897 12.9466C17.3505 12.7943 17.4979 12.6944 17.6618 12.6944H18.9876ZM18.5116 15.6017C18.4456 15.4308 18.2038 15.4308 18.1378 15.6017L17.3016 17.7699C17.251 17.9011 17.3478 18.0423 17.4885 18.0423H19.1609C19.3016 18.0423 19.3984 17.9012 19.3478 17.7699L18.5116 15.6017ZM9.69589 5.30028C9.79943 4.8999 10.368 4.89991 10.4715 5.30028L11.1086 7.76368C11.3244 8.59845 11.9396 9.27221 12.7513 9.56299L14.9019 10.3334C15.2559 10.4602 15.2559 10.9608 14.9019 11.0876L12.7513 11.8581C11.9396 12.1489 11.3244 12.8226 11.1086 13.6574L10.4715 16.1208C10.368 16.5212 9.79943 16.5212 9.69589 16.1208L9.05884 13.6574C8.84297 12.8226 8.22786 12.1489 7.41615 11.8581L5.26548 11.0876C4.91151 10.9608 4.91151 10.4602 5.26548 10.3334L7.41615 9.56299C8.22786 9.27221 8.84297 8.59844 9.05884 7.76368L9.69589 5.30028ZM18.0064 6.20485C19.9639 6.20485 21.5507 7.79145 21.5507 9.74861V11.281C21.5507 11.5022 21.3713 11.6816 21.1501 11.6816H20.0825C19.8612 11.6816 19.6819 11.5022 19.6819 11.281V9.74861C19.6819 8.82341 18.9318 8.07338 18.0064 8.07338H14.4343C14.2131 8.07338 14.0337 7.89403 14.0337 7.6728V6.60543C14.0337 6.3842 14.2131 6.20486 14.4343 6.20485H18.0064Z" fill="white"></path></svg></div></div></div></div><div title="关闭悬浮球" class="imt-fb-close-button" style="opacity: 0; pointer-events: none;"><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 14 14" fill="none"><g clip-path="url(#clip0_38845_9816)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14Z" fill="#B1B1B1" fill-opacity="0.4"></path><path d="M6.11754 7.00052L4.18254 5.06452C4.06786 4.94725 4.00406 4.78948 4.00498 4.62546C4.00589 4.46144 4.07145 4.3044 4.18744 4.18841C4.30342 4.07243 4.46046 4.00687 4.62448 4.00595C4.7885 4.00503 4.94627 4.06884 5.06354 4.18352L6.99954 6.11852L8.93554 4.18352C8.99315 4.1246 9.06188 4.07771 9.13774 4.04554C9.2136 4.01337 9.29508 3.99656 9.37748 3.9961C9.45988 3.99564 9.54155 4.01153 9.61777 4.04285C9.69398 4.07417 9.76323 4.1203 9.82149 4.17857C9.87976 4.23683 9.92589 4.30608 9.95721 4.38229C9.98853 4.45851 10.0044 4.54018 10.004 4.62258C10.0035 4.70497 9.98669 4.78646 9.95452 4.86232C9.92235 4.93818 9.87545 5.00691 9.81654 5.06452L7.88154 7.00052L9.81654 8.93652C9.93122 9.05379 9.99502 9.21155 9.99411 9.37557C9.99319 9.5396 9.92763 9.69664 9.81164 9.81262C9.69566 9.9286 9.53862 9.99417 9.3746 9.99508C9.21058 9.996 9.05281 9.9322 8.93554 9.81752L6.99954 7.88252L5.06354 9.81752C5.00593 9.87643 4.93721 9.92333 4.86135 9.9555C4.78549 9.98767 4.704 10.0045 4.6216 10.0049C4.5392 10.0054 4.45753 9.9895 4.38131 9.95818C4.3051 9.92686 4.23585 9.88073 4.17759 9.82247C4.11932 9.7642 4.0732 9.69496 4.04188 9.61874C4.01056 9.54253 3.99467 9.46086 3.99513 9.37846C3.99559 9.29606 4.01239 9.21457 4.04456 9.13871C4.07673 9.06285 4.12363 8.99413 4.18254 8.93652L6.11754 7.00052Z" fill="white"></path></g><defs><clippath id="clip0_38845_9816"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg></div></div><svg xmlns="http://www.w3.org/2000/svg" width="14" height="14" viewBox="0 0 14 14" fill="none" style="display: none; opacity: 0;"><g clip-path="url(#clip0_38845_9816)"><path d="M7 14C5.14348 14 3.36301 13.2625 2.05025 11.9497C0.737498 10.637 0 8.85652 0 7C0 5.14348 0.737498 3.36301 2.05025 2.05025C3.36301 0.737498 5.14348 0 7 0C8.85652 0 10.637 0.737498 11.9497 2.05025C13.2625 3.36301 14 5.14348 14 7C14 8.85652 13.2625 10.637 11.9497 11.9497C10.637 13.2625 8.85652 14 7 14Z" fill="#B1B1B1" fill-opacity="0.4"></path><path d="M6.11754 7.00052L4.18254 5.06452C4.06786 4.94725 4.00406 4.78948 4.00498 4.62546C4.00589 4.46144 4.07145 4.3044 4.18744 4.18841C4.30342 4.07243 4.46046 4.00687 4.62448 4.00595C4.7885 4.00503 4.94627 4.06884 5.06354 4.18352L6.99954 6.11852L8.93554 4.18352C8.99315 4.1246 9.06188 4.07771 9.13774 4.04554C9.2136 4.01337 9.29508 3.99656 9.37748 3.9961C9.45988 3.99564 9.54155 4.01153 9.61777 4.04285C9.69398 4.07417 9.76323 4.1203 9.82149 4.17857C9.87976 4.23683 9.92589 4.30608 9.95721 4.38229C9.98853 4.45851 10.0044 4.54018 10.004 4.62258C10.0035 4.70497 9.98669 4.78646 9.95452 4.86232C9.92235 4.93818 9.87545 5.00691 9.81654 5.06452L7.88154 7.00052L9.81654 8.93652C9.93122 9.05379 9.99502 9.21155 9.99411 9.37557C9.99319 9.5396 9.92763 9.69664 9.81164 9.81262C9.69566 9.9286 9.53862 9.99417 9.3746 9.99508C9.21058 9.996 9.05281 9.9322 8.93554 9.81752L6.99954 7.88252L5.06354 9.81752C5.00593 9.87643 4.93721 9.92333 4.86135 9.9555C4.78549 9.98767 4.704 10.0045 4.6216 10.0049C4.5392 10.0054 4.45753 9.9895 4.38131 9.95818C4.3051 9.92686 4.23585 9.88073 4.17759 9.82247C4.11932 9.7642 4.0732 9.69496 4.04188 9.61874C4.01056 9.54253 3.99467 9.46086 3.99513 9.37846C3.99559 9.29606 4.01239 9.21457 4.04456 9.13871C4.07673 9.06285 4.12363 8.99413 4.18254 8.93652L6.11754 7.00052Z" fill="white"></path></g><defs><clippath id="clip0_38845_9816"><rect width="14" height="14" fill="white"></rect></clippath></defs></svg></div><div class="btn-animate" style="margin-top: 10px; transform: translateX(60px);"><div class=" btn-animate" style="position: relative; pointer-events: all; display: inline-block;"><div><div class="imt-fb-round-button"><svg class="imt-fb-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" style="width: 24px; height: 24px;"><path d="M17 8.66797H11.375" stroke="#ED6D8F" stroke-linecap="round" stroke-linejoin="round"></path><path d="M12.625 15.332L7 15.332" stroke="#ED6D8F" stroke-linecap="round" stroke-linejoin="round"></path><path d="M15.125 17C16.1605 17 17 16.1605 17 15.125C17 14.0895 16.1605 13.25 15.125 13.25C14.0895 13.25 13.25 14.0895 13.25 15.125C13.25 16.1605 14.0895 17 15.125 17Z" stroke="#ED6D8F" stroke-linecap="round" stroke-linejoin="round"></path><path d="M8.875 10.75C9.91053 10.75 10.75 9.91053 10.75 8.875C10.75 7.83947 9.91053 7 8.875 7C7.83947 7 7 7.83947 7 8.875C7 9.91053 7.83947 10.75 8.875 10.75Z" stroke="#ED6D8F" stroke-linecap="round" stroke-linejoin="round"></path><rect x="3" y="4" width="18" height="16" rx="1.66667" stroke="#ED6D8F"></rect></svg></div></div></div></div><div class="btn-animate" style="margin-top: 10px; transform: translateX(60px);"><div class=" btn-animate" style="position: relative; pointer-events: all; display: inline-block;"><div><div class="imt-fb-round-button"><svg class="imt-fb-icon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none"><mask id="mask0_39772_2885" maskUnits="userSpaceOnUse" x="0" y="0" width="24" height="24" style="mask-type: alpha;"><rect width="24" height="24" fill="#D9D9D9"></rect></mask><g mask="url(#mask0_39772_2885)"><path d="M12 15.423C12.1808 15.423 12.3286 15.3657 12.4433 15.251C12.5581 15.1362 12.6155 14.9884 12.6155 14.8077C12.6155 14.6269 12.5581 14.4792 12.4433 14.3645C12.3286 14.2497 12.1808 14.1923 12 14.1923C11.8192 14.1923 11.6714 14.2497 11.5568 14.3645C11.4419 14.4792 11.3845 14.6269 11.3845 14.8077C11.3845 14.9884 11.4419 15.1362 11.5568 15.251C11.6714 15.3657 11.8192 15.423 12 15.423ZM12 12.4615C12.1423 12.4615 12.2612 12.4138 12.3567 12.3183C12.4522 12.2228 12.5 12.1038 12.5 11.9615V6.8845C12.5 6.74233 12.4522 6.6235 12.3567 6.528C12.2612 6.43233 12.1423 6.3845 12 6.3845C11.8577 6.3845 11.7388 6.43233 11.6433 6.528C11.5478 6.6235 11.5 6.74233 11.5 6.8845V11.9615C11.5 12.1038 11.5478 12.2228 11.6433 12.3183C11.7388 12.4138 11.8577 12.4615 12 12.4615ZM6.077 18L4.373 19.7038C4.1205 19.9564 3.8285 20.0145 3.497 19.878C3.16567 19.7413 3 19.4929 3 19.1328V5.6155C3 5.15517 3.15417 4.77083 3.4625 4.4625C3.77083 4.15417 4.15517 4 4.6155 4H19.3845C19.8448 4 20.2292 4.15417 20.5375 4.4625C20.8458 4.77083 21 5.15517 21 5.6155V16.3845C21 16.8448 20.8458 17.2292 20.5375 17.5375C20.2292 17.8458 19.8448 18 19.3845 18H6.077ZM5.65 17H19.3845C19.5385 17 19.6796 16.9359 19.8078 16.8077C19.9359 16.6796 20 16.5385 20 16.3845V5.6155C20 5.4615 19.9359 5.32042 19.8078 5.19225C19.6796 5.06408 19.5385 5 19.3845 5H4.6155C4.4615 5 4.32042 5.06408 4.19225 5.19225C4.06408 5.32042 4 5.4615 4 5.6155V18.6443L5.65 17Z" fill="#ED6D8F"></path></g></svg></div></div></div></div><div hidden="" id="immersive-translate-popup-overlay" class="immersive-translate-popup-overlay"><div class="immersive-translate-popup-wrapper" style="position: fixed; bottom: 30px; right: 65px;"></div></div></div></div></template></div><grammarly-desktop-integration data-grammarly-shadow-root="true"><template shadowrootmode="open"><style>
      div.grammarly-desktop-integration {
        position: absolute;
        width: 1px;
        height: 1px;
        padding: 0;
        margin: -1px;
        overflow: hidden;
        clip: rect(0, 0, 0, 0);
        white-space: nowrap;
        border: 0;
        -moz-user-select: none;
        -webkit-user-select: none;
        -ms-user-select:none;
        user-select:none;
      }

      div.grammarly-desktop-integration:before {
        content: attr(data-content);
      }
    </style><div aria-label="grammarly-integration" role="group" tabindex="-1" class="grammarly-desktop-integration" data-content="{&quot;mode&quot;:&quot;full&quot;,&quot;isActive&quot;:true,&quot;isUserDisabled&quot;:false}"></div></template></grammarly-desktop-integration></html>